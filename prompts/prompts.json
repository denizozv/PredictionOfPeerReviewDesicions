{
  "zeroShot": "You are an experienced academic editor. Review the following scientific paper and decide whether it would be accepted at a top-tier conference (such as ICML/ICLR/NeurIPS).\n\nEvaluation criteria:\n- Scope relevance (is it relevant to the conference?)\n- Methodological quality (is the method sound and reliable?)\n- Sample size (is the experimental evidence sufficient?)\n- Originality (does it make a novel contribution?)\n\nOutput your answer strictly in the JSON format below; do not write anything else:\n{\"rejection\": true/false, \"confidence\": 0.00-1.0, \"primary_reason\": \"Short Desc\"}",
  "fewShot": "You are an experienced academic editor. Based on the examples below, decide whether the new paper would be accepted at the level of ICML/ICLR/NeurIPS.\nEXAMPLE 1\nTITLE: Frustratingly Short Attention Spans in Neural Language Modeling\nABSTRACT: Neural language models often use RNNs, and recent work adds differentiable memory to capture longer dependencies by attending over recent history. Standard attention uses a single vector per step for both prediction and memory keys/values. This paper proposes key–value attention that separates memory keys, memory values, and the representation for next-word prediction. The approach outperforms prior memory-augmented models on two corpora, but analysis shows it mainly relies on only the last few steps. Surprisingly, a much simpler model that concatenates outputs from the previous three time steps performs comparably to these more complex memory-based models\nMETHOD: An LSTM language model is extended with several context mechanisms. A standard attention model attends over a sliding window of past hidden states to build a context vector. A Key–Value variant splits each output into attention keys and context values, and a Key–Value–Predict version further separates prediction features from memory representations. A simpler baseline removes attention and concatenates outputs from the previous N−1 steps (N-gram RNN). Under the same training setup, attention-based models mainly use very short histories, while the simple concatenation model achieves similar performance\nDECISION: Accept\nPRIMARY REASON: Provides a careful and well-executed empirical study showing that attention-based language models effectively rely on very short contexts, introduces a useful key–value–predict decomposition for attention, and validates the claims with strong comparisons including an insightful n-gram RNN baseline, yielding impactful insights about the limits of long-range dependency modeling.\nEXAMPLE 2\nTITLE: What does it take to generate natural textures?\nABSTRACT: Natural image generation often relies on statistics from deep, supervised networks, but it is unclear which aspects of these features are essential. Focusing on texture synthesis, this work shows that depth, pooling, and supervised training are not required. High-quality natural textures can be generated using networks with a single layer, no pooling, and even random filters.\nMETHOD: Single-layer CNNs with ReLU and no pooling are used to extract feature maps with various filters (Fourier, k-means, PCA, and random). Texture statistics are captured by Gram matrices of feature correlations. Starting from noise, images are optimized via backpropagation and L-BFGS-B to match the reference Gram matrix. Different filter types and multi-scale setups are compared to identify which representation properties are needed for realistic texture synthesis\nDECISION: Accept\nPRIMARY REASON: Shows a surprising and useful result that high-quality natural texture synthesis can be achieved with very shallow CNNs using random (untrained) filters, backed by thorough ablations and convincing qualitative + metric-based evidence, making it a strong empirical contribution and a practical minimal baseline for future texture-generation work.\nEXAMPLE 3\nTITLE: Emergence of foveal image sampling from learning to attend in visual scenes\nABSTRACT: We present a neural attention model with a learnable retinal sampling lattice, trained on a visual search task to classify objects in cluttered scenes using as few fixations as possible. After training, the learned lattice resembles the primate retina’s eccentricity-dependent layout, with a high-resolution fovea and lower-resolution periphery. We also identify conditions that strengthen or weaken these emergent properties, offering insight into their functional role\nMETHOD: A differentiable attention model generates glimpses using a learnable lattice of Gaussian kernels. Kernel centers and variances are partly learned and partly controlled by global translation and optional zoom variables. Each glimpse is processed by an RNN that updates its hidden state and predicts both class labels and the next fixation parameters. Variants include a fixed lattice, a learnable translation-only lattice, and a translation-plus-zoom model. The system is trained end-to-end with backpropagation through time on cluttered MNIST visual search, applying classification loss at each step so a fovea-like sampling structure emerges from the task\nDECISION: Accept\nPRIMARY REASON: Proposes an original attention model where the retinal sampling lattice itself is learnable and differentiable, and demonstrates a compelling emergent result: under translation-only constraints the learned sampling becomes foveated (high-acuity center + low-acuity periphery) with eccentricity-dependent spacing resembling primate retina; includes clear ablations (fixed vs learnable, translation vs zoom) that provide mechanistic insight into when foveation appears, despite the experimental scope being mostly limited to cluttered MNIST\nEXAMPLE 4\nTITLE: PixelCNN++: Improving the PixelCNN with Discretized Logistic Mixture Likelihood and Other Modifications\nABSTRACT: PixelCNNs are a recently proposed class of powerful generative models with tractable likelihood. Here we discuss our implementation of PixelCNNs which we make available at\nMETHOD PixelCNN++ enhances PixelCNN by replacing the 256-way softmax with a discretized mixture of logistics for a smoother, more efficient likelihood, and by predicting whole pixels with simple channel dependencies instead of separate sub-pixel masking. Architecturally, it captures multi-scale structure using strided downsampling and upsampling with U-net–style skip connections, and uses dropout in residual blocks for regularization. These changes improve optimization, likelihood, and sample quality.\nDECISION: Accept\nPRIMARY REASON: Delivers clear, practically important improvements to PixelCNN—most notably the discretized mixture-of-logistics likelihood (better inductive bias + lower output dimensionality), plus architectural changes (downsampling with skip/shortcut connections) and dropout regularization—validated via thorough ablations and state-of-the-art likelihood on CIFAR-10, along with a high-quality public implementation that makes the advances easy to adopt.\nEXAMPLE 5\nTITLE: Surprisal-Driven Feedback in Recurrent Networks\nABSTRACT: Recurrent neural nets are widely used for predicting temporal data. Their inherent deep feedforward structure allows learning complex sequential patterns. It is believed that top-down feedback might be an important missing ingredient which in theory could help disambiguate similar patterns depending on broader context. In this paper, we introduce surprisal-driven recurrent networks, which take into account past error information when making new predictions. This is achieved by continuously monitoring the discrepancy between most recent predictions and the actual observations. Furthermore, we show that it outperforms other stochastic and fully deterministic approaches on enwik8 character level prediction task achieving 1.37 BPC.\nMETHOD: The method extends RNN and LSTM models with a feedback signal based on surprisal—the mismatch between the previous prediction and the true input. This signal is fed into hidden state updates through an additional learnable connection, so activations depend on inputs, past states, and recent prediction error. The network is trained end-to-end with truncated backpropagation through time, allowing it to learn how to use recent mispredictions to improve sequence modeling\nDECISION: Reject\nPRIMARY REASON: The paper’s key idea (feeding last-step prediction error/surprisal back as an input) is seen as too incremental and insufficiently justified, and the empirical support is weak: it is evaluated essentially only on enwik8 with limited/unclear baselines and no solid ablations or analysis. In addition, the claimed performance is not convincingly positioned against stronger existing results, and the presentation/writing is poor with missing critical details (e.g., model size, fair comparisons, and when/why the feedback helps), which undermines credibility.\nEXAMPLE 6\nTITLE: Iterative Refinement for Machine Translation\nABSTRACT: Standard machine translation decoders generate outputs monotonically and cannot fix earlier errors. This paper proposes starting from an initial translation and iteratively improving it by revisiting past decisions. A convolutional neural network predicts word substitutions using attention over the source sentence and the current translation. With fewer than one edit per sentence, the method improves a phrase-based system by up to 0.4 BLEU on WMT15 German–English\nMETHOD: The approach begins with an initial translation and refines it through iterative word substitutions. A convolutional attention model conditions on the source and the full current translation, using both left and right target context to predict replacement distributions. A dual-attention version also uses the initial guess during training to reduce mismatch with testing. At inference, substitutions are ranked by confidence, and a heuristic applies the best edits within a small edit budget, stopping when confidence drops, allowing gradual correction of earlier errors\nDECISION: Reject\nPRIMARY REASON: Despite a plausible motivation, the proposed “iterative refinement” is not convincingly demonstrated to add value beyond simple local post-editing: the method is restricted to token-level substitutions with a limited context window, cannot perform insertions/deletions/reordering, and yields only modest BLEU gains (~+0.4) with very few edits (~0.6 per sentence); reviewers also note insufficient/weak baselines and comparisons to establish that iterative refinement beats straightforward alternatives (e.g., stronger initial systems, reranking, or globally scored decoding), making the overall impact and claimed benefit of “iterative” inference unsubstantiated.\nEXAMPLE 7\nTITLE: Sampling Generative Networks\nABSTRACT: We present techniques for sampling and visualizing latent spaces in generative models. Using spherical linear interpolation instead of linear interpolation keeps samples consistent with the prior and yields sharper results. J-Diagrams and MINE grids visualize manifolds formed by analogies and nearest neighbors. We also introduce bias-corrected and synthetic attribute vectors, and show how attribute vectors can support quantitative latent space analysis via binary classification. The methods are model-agnostic and demonstrated on VAEs and GANs.\nMETHOD: The work proposes model-agnostic tools for latent space analysis. Sampling uses spherical linear interpolation (slerp) to stay near the prior. Structure is visualized with analogy-based J-Diagrams and MINE grids, which mix nearest neighbors with interpolation to follow the data manifold. Attribute vectors are computed from latent means, corrected through dataset balancing or synthetic augmentation to reduce bias. Their effectiveness is measured by using dot products with latent codes as linear classifiers, providing quantitative evaluation of semantic directions.\nDECISION: Reject\nPRIMARY REASON: \":” The paper presents a collection of useful visualization and latent-space manipulation heuristics (e.g., spherical interpolation, J-diagrams, attribute-vector tricks), but the contribution is primarily practical/illustrative rather than methodological: there is no new generative model, learning objective, or theoretical insight, and the central technical claim (that spherical interpolation is better aligned with the prior) was not convincingly justified. The evaluation is largely qualitative, with minimal rigorous or comparative quantitative analysis, so the originality, depth, and scientific contribution were judged below the bar for a core research track.\n\nEXAMPLE 8\nTITLE: Learning a Static Analyzer: A Case Study on a Toy Language\nABSTRACT: Static analyzers examine programs to detect errors or gather information, such as security flaws or compiler checks. This work asks whether a static analyzer can be learned from data using deep learning instead of manual feature engineering. The authors show that LSTMs can learn a basic analyzer for a toy language, while feature-based models, HMMs, and simple RNNs fail. They also make the system practical by using a language model to help locate reported errors.\nMETHOD: The task is framed as supervised learning: given a tokenized program in a toy language, predict whether all variables are defined before use. A balanced dataset of valid and invalid programs is generated and labeled by a compiler oracle, including cases with undefined variables and reordered statements. Baselines (n-grams, HMMs, vanilla RNNs) are compared to sequence models, where LSTMs perform well. Performance improves further by adding a differentiable set-like memory to track defined variables, trained with per-token supervision and pooled to a program-level decision. A separate LSTM language model scores variable uses and flags low-probability ones to indicate likely error locations.\nDECISION: Reject\nPRIMARY REASON: The submission is an interesting proof-of-concept, but it is fundamentally too toy and under-positioned relative to existing “neural algorithmic reasoning” work. The task (defined-before-use) on a deliberately simplified language is essentially a set-membership/ordering problem that can be solved by a trivial symbolic analyzer, and is very close to standard algorithmic benchmarks where memory-augmented models (NTM/stack-RNN/Neural GPU/DNC, etc.) already demonstrate near-perfect performance. Yet the paper compares mainly to weak baselines (n-grams, HMMs, vanilla RNNs) and does not establish that it solves a meaningfully harder analysis problem than prior techniques. The differentiable-set augmentation is a modest architectural tweak whose benefit is demonstrated mostly on synthetic data generated by the authors, with limited evidence of robust generalization (e.g., longer programs, different variable namespaces, distribution shift) or relevance to real languages/analyses (control-flow joins, aliasing, functions, heap). Overall, the idea is promising, but the current scope, baseline coverage, and external validity are insufficient for a top-tier conference track.\nOutput your answer strictly in the JSON format below; do not write anything else.\n{\n\"rejection\": true/false,\n\"confidence\": 0.00-1.0,  \n\"primary_reason\": \"Short Desc\"\n}\n"
}
