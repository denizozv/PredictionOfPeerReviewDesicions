{
  "id": "780",
  "title": "Efficient Communications in Training Large Scale Neural Networks",
  "abstract": "We consider the problem of how to reduce the cost of communication that is re- quired for the parallel training of a neural network. The state-of-the-art method, Bulk Synchronous Parallel Stochastic Gradient Descent (BSP-SGD), requires a many collective communication operations, like broadcasts of parameters or reduc- tions for sub-gradient aggregations, which for large messages quickly dominates overall execution time and limits parallel scalability. To address this problem, we develop a new technique for collective operations, referred to as Linear Pipelining (LP). It is tuned to the message sizes that arise in BSP-SGD, and works effectively on multi-GPU systems. Theoretically, the cost of LP is invariant to P , where P is the number of GPUs, while the cost of more conventional Minimum Spanning Tree (MST) scales like O(log P ). LP also demonstrate up to 2x faster bandwidth than Bidirectional Exchange (BE) techniques that are widely adopted by current MPI implementations. We apply these collectives to BSP-SGD, showing that the proposed implementations reduce communication bottlenecks in practice while preserving the attractive convergence properties of BSP-SGD.",
  "accepted": false,
  "zeroShot": [
    {
      "model": "claude-haiku-4-5",
      "decision": {
        "rejection": false,
        "confidence": 0.72,
        "primary_reason": "Solid systems contribution with practical impact on distributed neural network training. Linear Pipelining collective design is novel and well-motivated for multi-GPU systems. Theoretical analysis is sound with O(log P) speedup over MST. Experimental validation on AlexNet/GoogLeNet shows significant practical speedups (1.6-2.2x). However, limited experimental scope (only 4-6 GPUs, 2 networks), missing convergence proofs, and incremental nature of contribution prevent higher confidence. Suitable for ICML/ICLR systems track but borderline for main conference."
      },
      "token": {
        "prompt_tokens": 5337,
        "completion_tokens": 160,
        "total_tokens": 5497
      },
      "time": "2026-02-05T23:19:16.808567+00:00"
    },
    {
      "model": "gpt-5.2",
      "decision": {
        "rejection": true,
        "confidence": 0.78,
        "primary_reason": "Insufficient methodological rigor and experimental evidence: claims of P-invariant cost rely on unrealistic latency assumptions and limited 2â€“6 GPU PCIe/QPI setups; comparisons are confounded by implementation differences (e.g., OpenMPI host-staging vs GPU P2P) and lack strong baselines/ablation/generalization beyond Caffe on K40m."
      },
      "token": {
        "prompt_tokens": 4588,
        "completion_tokens": 91,
        "total_tokens": 4679
      },
      "time": "2026-02-09T21:47:58.156809+00:00"
    }
  ],
  "article": [
    {
      "heading": "1 INTRODUCTION",
      "text": "Scaling up neural networks with respect to parameter sizes, training sets, or both has drastically improved the state-of-the-art performance in several domains ranging from scene understanding, speech recognition, even to playing Go against professional players. Although training a large network saturated with nonlinearities is extremely time-consuming, the benefits brought forth by large-scale models has sparked a surge of interest in parallelizing training on multi-GPUs. The parallelization of SGD demands synchronizations to exchange gradients and parameters per iteration, and this introduces significant communication overhead. Previous studies have focused on trading the SGD convergence rate for fast gradient updates, such as stale or asynchronous SGD, 1-bit compressed gradient, etc. However, these methods are rarely adopted by Deep Learning frameworks as they depend on the balance between the enhanced iteration throughput and the decelerated convergence rate. Since BSP retains the convergence properties of SGD, its optimization should be of interest. The gradient aggregations and parameter exchanges in BSP SGD are typical operations of communication collectives (Chan et al., 2007). Messages in the large-scale neural networks training are dense, long, and fixed-length, while the performance of collective algorithms is drastically sensitive to these attributes. Besides, the processing speed is several orders of magnitude faster than the network unidirectional transmission rate. These prioritize the utilization of network bandwidth in the collective design. However, we have seen sub-optimal collective algorithms, e.g. MST and BE, widely adopted by the deep learning community (Agarwal et al., 2014) (Jia et al., 2014) (Duchi et al., 2011). MST is only suitable for the latency dominant case such as frequent short message exchanges, while the bandwidth term of BE can be further improved (Thakur et al., 2005). In this paper, we introduce new Linear Pipeline based collectives for multiGPU training. The collectives demonstrate O(log(P )) speedups over MST collectives and up to 2x speedups over BE based ones; the bounds only hold in training large neural networks. In particular, the theoretical analysis and the implementation yield an interesting insight that the cost of our design is invariant to GPU numbers, i.e., the cost of collective operations on 2 GPUs is similar to 20 GPUs. The design explores message granularity to maximize simultaneous bidirectional data exchanges. In specific, it divides a message into fine-grained blocks as the basic communication element. A GPU sends a block (via DMA 1) while receiving (via DMA 2) a new block from a neighbor. The copies are asynchronously launched on two GPU streams, and numerical operations further overlap data copies. As a result, our method yields a highly efficient pipeline over which messages for neural network training may be exchanged. The proposed collective design achieves 2.3x to 360.55x speedups over Open MPI alternatives on 6 GPUs. In training GoogLeNet, we set up the same BSP SGD implementation with different underlying collectives. Our design demonstrates up to 1.7x convergence speedup over MST based Caffe.",
      "exclude": true
    },
    {
      "heading": "2 RELATED WORK",
      "text": "The communication overhead has been widely identified as the major bottleneck in the data-parallel SGD (Shamir (2014), Li et al. (2014)). The data parallelism linearly adds the processing power by concurrent gradient computations with multiple GPUs. But it also requires synchronizations to collect partial gradients or to broadcast parameters. In practice, the communication rate is several orders of magnitude slower than the computation (Coates et al., 2013). Various approaches have been proposed to reduce the overhead. The first group of approaches relaxes synchronous models of SGD to increase the iteration throughput (Dean et al. (2012), Zinkevich et al. (2010)). In this case, the relaxed SGD enables computations on a GPU to partially overlap with communications on others as demonstrated in Fig.1c and Fig.1d. Recht et al. (2011) proposed a lock free Asynchronous SGD (ASGD) that entirely gets rid of the synchronization requirement by allowing free concurrent parameter updates. But the relaxation only works well on sparse learning problems. In response, Ho et al. (2013) introduced the concept of staleness by bounding the fastest and the slowest machine within a few iterations of each other to ensure correctness. These relaxations claim to be effective as the enhanced iteration throughput offsets the disadvantages of degraded convergence rate. However, recent advances in deep learning frameworks (Cui et al. (2016)) have reestablished the advantages of BSP over relaxed ones in training neural networks. This reiterates the importance of studying BSP SGD. The second group of approaches tries to reduce the overall communication volume. Seide et al. (2014) quantized gradients from 32 bits to 1 bit to reduce the message length, but the lost gradient information decelerates the convergence rate. Another approach is to accelerate the convergence with a large batch. Dekel et al. (2012) shows the convergence rate of mini-batch SGD isO(1/ Tb+1/T ) with b being the batch size. This result indicates a large batch needs fewer iterations to find a solution, and thereby fewer overall synchronizations. However, unwieldy increasing the batch size is also unfavorable under limited computing resources demonstrated by Wang et al. (2016b). Please note these methods still need synchronizations, and our work will further improve their performance. The third group of approaches conducts system optimizations to minimize the communication cost (Wang et al., 2016a). Agarwal & Duchi (2011) and Agarwal et al. (2014) presented partial gradients aggregations guided with a MST that takes log(P ) steps to fully synchronize the model. Deep learning frameworks such as Caffe (Jia et al., 2014) also adopt this approach. Unfortunately, MST is only suitable for latency dominant scenarios (i.e. high frequent short messages). Although collective algorithms have been thoroughly discussed in the HPC community (Almasi et al. (2005), Gabriel et al. (2004), Shipman et al. (2006)), few have studied their performances for the deep learning. The performance of collectives varies significantly with different message lengths and network topologies, while messages in deep network training are dense, long and fixed-length. Therefore, it is imperative to address such peculiarities in the collectives. Worringen (2003) proposed a pipeline collective model in shared memory environment for CPU data, but communications of different MPI processes sharing the same CPU memory bus within the same CPU socket. This causes bandwidth competition among different processes, thereby poor performance for the collective communication in shared memory environment for CPU data. In contrast, PCI-E is bi-directional. The latest GPUs also feature two independent DMA engines for simultaneous independent in/out communications. The hardware updates pave the way for LP based GPU communications.",
      "exclude": true
    },
    {
      "heading": "3 LINEAR PIPELINE BASED COLLECTIVE DESIGN DEDICATED FOR NEURAL NETWORK TRAINING ON MULTI-GPUS",
      "text": "This section presents a new LP based MultiGPU collective design ensued by the concrete proof of its performance in training neural networks. The general idea of LP is as follows: a) we dissect a long message into fine-grained blocks. b) a GPU receives a block from the prior GPU via DMA1 while sending a block to the next one via DMA2. Please note each block exchange utilizes an independent physical link, and the entire network is fully utilized once the pipeline is filled. Broadcast tackles the synchronizations of parameters among multiple GPUs. It copies the source vector to every GPU. Fig.2a illustrates the data flow of the broadcast collective on 3 GPUs. GPU0 is the source, and the rest are destinations. Broadcast starts with filling the pipe by copying block a on GPU0 to GPU1 at step 1. Lets focus on GPU1. At each step, GPU1 receives a block from GPU0 via DMA1, while GPU1 is also sending a block to GPU2 via DMA2. The data exchange in either way utilizes an independent link and DMA engine to achieve the maximal unidirectional rate. Hence, the bandwidth is fully exploited. Reduce aggregates the partial gradients to reconstruct the global one. It combines the elements provided in the vector of each GPU, and returns the combined value in the receive vector to a specific GPU. It supports basic arithmetic operations such as summations and multiplications. Fig.2b illustrates the data flow of the reduce collective. GPU2 is the root that aggregates the vectors across all GPUs. Reduce starts with filling the pipe by writing block a0 to a buffer on GPU1. Then, GPU1 reduces the received block a0 with a1 to yield a (within the rectangle of Fig.2b). Please note the computation is much faster than the communication, we assume no latency on it. In practice, computations are further overlapped with communications. In the next step, GPU1 retrieves b0 from GPU0 to reduce to b via DMA 1, while GPU1 is also sending a to GPU2 to reduce to a via DMA 2. b, c, d are reduced at steps 3, 4, 5 in a similar fashion. AllReduce enables us to collect partial gradients and broadcast the latest parameters with only one synchronization point per SGD iteration. It combines vectors from all GPUs and distributes the result back to them. Mathematically, it is equivalent to a reduce followed by a broadcast. However, allreduce is more efficient than two separate calls as it only needs to fill the pipeline once. For example, it takes 9 timesteps to allreduce 4 message blocks, while broadcast + reduce will cost 10. Fig.2c illustrates the data flow of the allreduce collective. It starts with reducing a, after which a is broadcast to GPU1 and GPU2 at step 5, 6 respectively. Please note d0 utilizes the outbound DMA at step 4, therefore a has to wait until step 5. b, c, d are processed in a similar fashion. Our collective is also specifically designed to accommodate GPU features such as asynchronous kernel launches and multi-stream processing. In the rectangle of Fig.2a, it demonstrates the data transfers are asynchronously launched on two separate streams. The copies happening in the red steps are scheduled on one stream while copies in the black steps are scheduled on another stream. This overlaps the overhead of GPU kernel launches, further improving the pipeline. We illustrate the data flow of the collectives on 3 GPUs. If there are k GPUs, GPU n, 0 < n < k 1, duplicates the same communication pattern on GPU 1.",
      "exclude": false
    },
    {
      "heading": "3.1 ARCHITECTURE ANALYSIS",
      "text": "LP is the optimal collective algorithm to fully exploit the network bandwidth of a MultiGPU system. Even though PCI-E supports full-duplex communication between any two endpoints, each PCI-E endpoint device only has one input and output port. This results in bandwidth competition if a GPU is receiving from multiple GPUs. Similarly, each PCI-E switch only contains one input and output port used for inter-switch communication, and inter-switch communications of the same direction also compete for the PCI-E bus. It is known that any delay in data movement between two GPUs interrupts the pipelining in the collectives. In such architecture, the communication from parents to children in MST based collective algorithms will compete for the same PCI-E bus, therefore breaking pipelining. The data exchange of BE also suffers from the inter-switch communication congestion in one direction. In contrast, LP connects all GPUs into a chain, and data always flow in one direction. Hence, data movements between two GPUs exclusively occupy the entire PCI-E bus, ensuring uninterrupted pipelining.",
      "exclude": false
    },
    {
      "heading": "3.2 THEORETICAL ANALYSIS",
      "text": "We adopt a cost model widely used by the MPI community to analyze collective operations (Thakur et al. (2005), Thakur & Gropp (2003)). The model assumes the time taken to send a message between two nodes follows: T = + n+ n (1) where is the latency or startup time of sending a message, and is the transmission rate and reduce rate measured by time per byte, and n is the message size in bytes. We also denote p as the node count, and b as the block size (in bytes) in the pipeline. Proposition 1 If the network latency 0, Linear Pipeline collectives provide anO(log p) speedup over Minimal Spanning Tree collectives and up to a 2 times speedup over Bidirectional Exchange collectives as the message size n. Proof. First, we derive the costs of the three Linear Pipeline collectives. According to Fig.2, the length of pipeline is p 1 + nb blocks assuming each block to be b bytes. A block exchange takes + b+ b (with reduce) or + b (without reduce). Consequently, broadcast essentially costs (+b)(p1+ nb ) = (p1+ n b )+(b(p1)+n), and reduce costs (+b+b)(p1+ n b ) = (p 1 + nb ) + (b(p 1) + n)( + ). allreduce is approximately equivalent with a reduce followed by a broadcast. Therefore, the allreduces cost is broadcasts cost plus reduces cost, i.e. 2(p 1 + nb )+ (bp b+ n)(2 + ). Secondly, we derive the costs of the three Minimal Spanning Tree collectives. MPI adopts MST to broadcast or reduce short messages (Thakur et al. (2005)), the length of which is less than 12 KB. The core concept of MST is to organize p GPUs into a balanced tree of height dlogpe. Then, it takes dlog pe steps to traverse all GPUs in the tree. Each step carries the message of length n, resulting in the cost of broadcast to be the tree height times the cost per step, i.e. log p(+ n) (we omit the ceiling for simplicity). Similarly, MST reduce is log p(+ n + n), and MST allreduce is also a combination of broadcast and reduce. Please note the latency term, log p, is the smallest among algorithms in Table.1, and the bandwidth term, log pn, is the slowest as log pn n. Therefore, MST is widely used for high frequent exchanges of short message. Finally, we present the costs of the three Bidirectional Exchange collectives. MPI broadcast handles long messages with a MST scatter followed by a BE allgather. Please refer to Chan et al. (2007) for the analysis of BE collectives. Basically, scatter costs dlogpe k=1 (+2 kn) = log p+ p1p n, while allgather costs (p 1) + p1p n. The cost of broadcast is the sum of these two. The MPI long message reduce consists of a reducescatter plus a gather, while allreduce consists of a reducescatter and a allgather. The cost for reducescatter is log p + p1p n + p1 p n, and both the costs of gather and allgather are log p + p1p n (also in Chan et al. (2007)). Table 1 summarizes the costs of broadcast, reduce and allreduce for the three different underlying algorithms. The proposition holds under the assumptions of 0 and n , and these assumptions are legitimate for the training of large scale neural networks on multiGPUs. Nowadays, the PCI Express x16 effectively reduces the latency down to 107s. The current two sockets shared memory machine supports up to 8 GPUs indicating limited p in practice. Lets take an appropriate block size b to ensure p nb and n b 0. This enables us to safely ignore the latency term, e.g. log p in MST broadcast. On the other hand, current deep convolutional neural network uses a tremendous number of parameters. For example, AlexNet uses 50 MB parameters. The transmission rate1 109Byte/Seconds. Compared to the trivial latency term, the bandwidth term dominates the entire cost T . This result leads us to simplify the costs of BE, MST, and LP based broadcast (Table. 2) to be 2p1p n, n log p and (b(p 1) + n), obtaining the following equations: Tbroadcast BE Tbroadcast LP 2(1 1p ) 1 + bn (p 1) < 2 (2) Tbroadcast MST Tbroadcast LP log p b(p1) n + 1 < log p (3) Compared with broadcast, reduce has the additional term. Please note the processing speed of GPUs exceeds TFLOPs implying the term n 0. Therefore, it is also legitimate to ignore the term, and it yields the same result Treduce BE/Treduce LP < 2 and Treduce MST /Treduce LP < log p. This completes our proof of the proposition 1. Another interesting point is the cost of Linear Pipeline is invariant to GPU count p regardless of message length n. This implies broadcasting a vector to 8 GPUs should cost the same as broadcasting to 2 GPUs. In practice, we set the block size b around 64 KB, and p is within 101. This suggests the bandwidth term, e.g. the cost of LP broadcast (bp p + n) n. Hence, the cost of LP collectives are less likely to be affected by GPU counts p.",
      "exclude": false
    },
    {
      "heading": "3.3 DEEP LEARNING WITH EFFICIENT BSP SGD",
      "text": "We formulate the neural network training as the following optimization problem. Let be a loss function with weight vector w as function parameters that takes randomly sampled images dt as the 1https://en.wikipedia.org/wiki/InfiniBand Algorithm 1: BSP SGD with communications/computations overlapping. 1 while not converge do 2 broadcast(w0t ) 3 for i [0, 1, ...,max layers] do 4 nonblocking broadcast(wi+1t ) 5 Forward(i) 6 sync broadcast() 7 Backward(max layers) 8 for i [max layers 1, ..., 1, 0] do 9 nonblocking reduce(i+1sub) 10 Backward(i) 11 sync reduce() 12 wt+1 = GradientUpdate() Algorithm 2: BSP SGD uses broadcast + reduce. 1 while not converge do 2 sub = ForwardBackward(dt) 3 = reduce(sub) 4 if root then 5 wt+1 = GradientUpdate() 6 broadcast(wt+1) 7 barrier /* sync new w */ Algorithm 3: BSP SGD uses allreduce. 1 while not converge do 2 sub = ForwardBackward(dt) 3 = allreduce(sub) 4 barrier /* collect sub */ 5 wt+1 = GradientUpdate() 6 if iter%5 = 0 then 7 broadcast(wt+1) input. The objective of training is to find an approximate solution to the following problem: min w Ew(dt) = w(dt)dP (4) A typical neural network training iteration consists of a forward and backward pass. The forward pass yields a loss that measures the discrepancy between the current predictions and the target; The backward pass calculates the gradient, the negative of which points to the steepest descent direction. The gradient descent updates the parameters, w, as follows: wt = wt1 tw(dt) (5) Guided with Data Parallelism, BSP SGD evenly divides dt into p slices d1t ,d 2 t , ...,d p t so that every GPU computes a partial gradient from dit in parallel. The global gradient is equivalent to the average of partial gradients. After finishing the gradient update, wt is synchronized to all GPUs. We integrate the proposed collectives into this process to harness parallel processing capabilities of multiGPU system. In this paper, we discuss two approaches to BSP SGD implementations. fork and join: This approach forks the gradient computations, and joins partial gradients with communications. In this case, communications do not overlap with computations. Alg.2 and Alg.3 demonstrate two collective based implementations using 2 and 1 synchronization points, respectively. In Alg.2, synchronizations rely on broadcast and reduce. Each GPU calculates a partial gradient referred to as sub. The master GPU reconstructs by reducing all sub. Then, the GPUs synchronize the latest weight, w, by broadcasting. In Alg.3, synchronizations only rely on allreduce. The differences between this and Alg.2 are that 1) there is only 1 synchronization point; 2) every GPU computes the gradient update. However, the parameters are not consistent after several iterations due to the precision issues of float multiplications inGradientUpdate. We synchronize w every 5 iterations to enforce consistency while still retaining the benefit of efficient pipelining in allreduce (line 7-8 Alg.3). overlapping communications with computations: Another approach is to overlap communications and computations for each network layer. In the forward pass, GPUs broadcast network parameters of layer t+1 during forward computations at layer t. In the backward pass, GPUs reduce pros and cons of both approaches: The cost of Alg.2 or Alg.3 is comm+ compt, while the cost of Alg.1 is max(comm, compt). If the network has over a few hundred MB of parameters, the overlapping will be significantly better than the fork and join approach. However, Alg.2 and Alg.3 are relatively easy to implement, and the performance on networks < 100 MB is similar to that of Alg.1.",
      "exclude": false
    },
    {
      "heading": "4 EXPERIMENT",
      "text": "",
      "exclude": false
    },
    {
      "heading": "4.1 COLLECTIVES EVALUATION",
      "text": "The MST and BE implementations used in benchmarks are Caffe 2 and OpenMPI. Caffe optimizes the GPU placement in an MST to fully utilize inter-GPU peer to peer (P2P) access. OpenMPI and our implementation, similar to Caffe, also take advantages of P2P. We set up AlexNet and GoogLeNet training using the three BSP SGD algorithms proposed in section 3.3. Fig.3 presents the performance of LP, MST, and BE based collectives at different message sizes on 4 K40m. The LP broadcast demonstrates an average of 29.2x and 2.3x speedup over BE and MST based alternatives in Caffe and OpenMPI; the LP reduce demonstrates an average of 360.55x and 8.7x speedup over BE and MST reduce, and the LP allreduce demonstrates an average of 109.2x and 7.9x speedup over BE and MST allreduce. In theory, LP is approximately 2x faster than both the MST (p = 4 logp = 2) and BE approaches. An extraordinary speedup against Open MPI is observable due to inefficient data movement in Open MPI, which moves data to host RAM to perform reduce operations on the CPU before being copied to the target GPU. Instead, we perform reduce on the GPUs, and data blocks directly flow to the target GPU via P2P access. The overlapped reduce computations with communications enables our reduce and allreduce to be 8x faster than that of MST. At each step of MST, GPUs reduce the incoming data only after all the data is available. In contrast, our fine-grained block design enables communications and computations to overlap by reducing a block while receiving a new one in the pipeline. broadcast only involves data copies, and both we and Caffe use P2P to transmit the data. Therefore, the speedup of MST broadcast (2.3x), conforms to the 2.0x theoretical prediction. The theoretical analysis indicates both the cost of LP and BE collectives are invariant to the GPU count p, while the cost of MST increases with p by a factor of logp. This is also noticeable in the 2Caffe implements an MST based broadcast and reduce for the multiGPU training. scalability experiment demonstrated in Fig.4. Please note there is a cost jump between 4 and 5 GPUs. Communications have to go through QPI after 4 GPUs incurring the additional cost of copying through the host RAM. The cost of the Linear Pipeline method robustly stays the same if GPU counts =[2,3,4] or [5,6], and QPI explains the inconsistency. The communication steps of MST for 2,3,4,5,6 GPUs are 1,2,2,3,3, respectively. The MST experiments verify the logp cost increase w.r.t GPU counts by evident cost jumps at 3 and 5 GPUs. The data flow of OpenMPI between two GPUs follows GPU RAMhost RAMGPU RAM. The inefficient data flow inside Open MPI contributes to the near linear cost increase with GPU counts p.",
      "exclude": false
    },
    {
      "heading": "4.2 IMPACT ON THE NEURAL NETWORK TRAINING",
      "text": "Fig.5 demonstrates LP collectives effectively reduce the total training time without affecting SGDs convergence properties in training large scale neural networks. We use inspurCaffe, Caffe and cuhks Caffe branch to benchmark the performance of BE-Alg.1, MST-Alg.1 and BE-Overlap-Alg.3. We also implement Alg.1,2,3, integrated with LP collectives, in Caffe to ensure consistency. Please note the model size affects the communication time, while the batch size affects the computation time. We carefully set these parameters to cover as many cases as possible. Please refer to the captions of Table.2 and Fig.5 for experiment details. We assume these algorithms have similar convergence speeds in iterations as losses of AlexNet are approximately 1 after 30000 iterations and losses of GoogLeNet are approximately 2 after 67000 iterations. However, the time taken to reach the target loss varies dramatically. For example, the speedups of LP-Overlap-Alg.3 over BE-Alg.1 in training AlexNet and GoogLeNet are 2.12x and 2.19x, respectively. Under Alg.1, but using different underlying collective algorithms, LP-Alg.1 presents 1.91x and 1.74x speedup over BE-Alg.1 and MST-Alg.1 in AlexNet, and 1.6x and 1.1x speedup over BE-Alg.1 and MST-Alg.1 in GoogLeNet. The iteration profiles of these 3 algorithms in Table.2 indicate the communication cost of LP-Alg.1 is only 10% of BE-Alg.1, and 11% of MST-Alg.1 in AlexNet; and 6% of BE-Alg.1, and 43% of MST-Alg.1 in GoogLetNet. The experiments demonstrate that the speed of the three proposed BSP SGD algorithms is Alg.3 > Alg.2 > Alg.1. The result conforms to our expectations as the cost of Alg.3 is max(comm, compt), while the cost of Alg.1 and Alg.2 is comm+ compt. However, the performance gain is quite limited from Alg.2 to Alg.3 as there is little room left for reducing communications from LP Alg.2 to Alg.3 as demonstrated in Table.2. If the model parameters keep increasing, we expect Alg.3 to be more efficient than Alg.2.",
      "exclude": false
    }
  ],
  "fewShot": [
    {
      "model": "gpt-5.2",
      "decision": {
        "rejection": true,
        "confidence": 0.72,
        "primary_reason": "Primarily a systems/collectives engineering paper with unclear novelty vs established ring/pipelined allreduce and weak/dated experimental positioning; the theoretical claims rely on unrealistic latency assumptions and small-P PCIe-specific settings, and the evaluation is limited (few models, few GPUs, questionable baseline implementations like OpenMPI host-staging) without comparison to modern NCCL/ring allreduce or broader hardware, so the contribution/impact is below ICML/ICLR/NeurIPS bar."
      },
      "token": {
        "prompt_tokens": 7222,
        "completion_tokens": 124,
        "total_tokens": 7346
      },
      "time": "2026-02-09T20:53:28.366283+00:00"
    },
    {
      "model": "claude-haiku-4-5",
      "decision": {
        "rejection": false,
        "confidence": 0.72,
        "primary_reason": "Proposes Linear Pipelining (LP), a novel collective communication algorithm for multi-GPU neural network training that achieves O(1) cost invariant to GPU count (vs O(log P) for MST), with strong theoretical analysis and empirical validation showing 2-360x speedups on benchmarks and 1.6-2.2x end-to-end training speedups on AlexNet/GoogLeNet. The work addresses a real systems bottleneck with a well-motivated design, clear architectural insights, and practical implementations, though the experimental scope is limited to 4-6 GPUs and older networks (2012-2014 era)."
      },
      "token": {
        "prompt_tokens": 8233,
        "completion_tokens": 181,
        "total_tokens": 8414
      },
      "time": "2026-02-11T19:30:13.857842+00:00"
    }
  ],
  "test": false
}