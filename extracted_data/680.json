{
  "id": "680",
  "title": "Deep Character-Level Neural Machine Translation By Learning Morphology",
  "abstract": "Neural machine translation aims at building a single large neural network that can be trained to maximize translation performance. The encoder-decoder architecture with an attention mechanism achieves a translation performance comparable to the existing state-of-the-art phrase-based systems. However, the use of large vocabulary becomes the bottleneck in both training and improving the performance. In this paper, we propose a novel architecture which learns morphology by using two recurrent networks and a hierarchical decoder which translates at character level. This gives rise to a deep character-level model consisting of six recurrent networks. Such a deep model has two major advantages. It avoids the large vocabulary issue radically; at the same time, it is more efficient in training than word-based models. Our model obtains a higher BLEU score than the bpe-based model after training for one epoch on En-Fr and En-Cs translation tasks. Further analyses show that our model is able to learn morphology.",
  "accepted": false,
  "zeroShot": [
    {
      "model": "claude-haiku-4-5",
      "decision": {
        "rejection": false,
        "confidence": 0.72,
        "primary_reason": "Novel hierarchical character-level NMT architecture with morphology learning shows solid empirical results and interesting analysis, though incremental improvements and limited experimental scope reduce impact"
      },
      "token": {
        "prompt_tokens": 5802,
        "completion_tokens": 61,
        "total_tokens": 5863
      },
      "time": "2026-02-05T23:11:05.430792+00:00"
    },
    {
      "model": "gpt-5.2",
      "decision": {
        "rejection": true,
        "confidence": 0.78,
        "primary_reason": "Insufficient and outdated experimental validation for a top-tier venue (limited comparisons/ablations, unclear statistical rigor, and claims like efficiency/one-epoch superiority not convincingly supported), with only incremental novelty over prior character/subword NMT."
      },
      "token": {
        "prompt_tokens": 5233,
        "completion_tokens": 67,
        "total_tokens": 5300
      },
      "time": "2026-02-09T21:39:52.142008+00:00"
    }
  ],
  "article": [
    {
      "heading": "1 INTRODUCTION",
      "text": "Neural machine translation (NMT) attempts to build a single large neural network that reads a sentence and outputs a translation (Sutskever et al., 2014). Most of the extant neural machine translations models belong to a family of word-level encoder-decoders (Sutskever et al., 2014; Cho et al., 2014). Recently, Bahdanau et al. (2015) proposed a model with attention mechanism which automatically searches the alignments and greatly improves the performance. However, the use of a large vocabulary seems necessary for the word-level neural machine translation models to improve performance (Sutskever et al., 2014; Cho et al., 2015). Chung et al. (2016a) listed three reasons behind the wide adoption of word-level modeling: (i) word is a basic unit of a language, (ii) data sparsity, (iii) vanishing gradient of character-level modeling. Consider that a language itself is an evolving system. So it is impossible to cover all words in the language. The problem of rare words that are out of vocabulary (OOV) is a critical issue which can effect the performance of neural machine translation. In particular, using larger vocabulary does improve performance (Sutskever et al., 2014; Cho et al., 2015). However, the training becomes much harder and the vocabulary is often filled with many similar words that share a lexeme but have different morphology. There are many approaches to dealing with the out-of-vocabulary issue. For example, Gulcehre et al. (2016); Luong et al. (2015); Cho et al. (2015) proposed to obtain the alignment information of target unknown words, after which simple word dictionary lookup or identity copy can be performed to replace the unknown words in translation. However, these approaches ignore several important properties of languages such as monolinguality and crosslinguality as pointed out by Luong and Manning (2016). Thus, Luong and Manning (2016) proposed a hybrid neural machine translation model which leverages the power of both words and characters to achieve the goal of open vocabulary neural machine translation. Intuitively, it is elegant to directly model pure characters. However, as the length of sequence grows significantly, character-level translation models have failed to produce competitive results compared with word-based models. In addition, they require more memory and computation resource. Especially, it is much difficult to train the attention component. For example, Ling et al. (2015a) proposed a compositional character to word (C2W) model and applied it to machine translation (Ling et al., 2015b). They also used a hierarchical decoder which has been explored before in other context (Serban et al., 2015). However, they found it slow and difficult to train the character-level models, and one has to resort to layer-wise training the neural network and applying supervision for the attention component. In fact, such RNNs often struggle with separating words that have similar morphologies but very different meanings. In order to address the issues mentioned earlier, we introduce a novel architecture by exploiting the structure of words. It is built on two recurrent neural networks: one for learning the representation of preceding characters and another for learning the weight of this representation of the whole word. Unlike subword-level model based on the byte pair encoding (BPE) algorithm (Sennrich et al., 2016), we learn the subword unit automatically. Compared with CNN word encoder (Kim et al., 2016; Lee et al., 2016), our model is able to generate a meaningful representation of the word. To decode at character level, we devise a hierarchical decoder which sets the state of the second-level RNN (character-level decoder) to the output of the first-level RNN (word-level decoder), which will generate a character sequence until generating a delimiter. In this way, our model almost keeps the same encoding length for encoder as word-based models but eliminates the use of a large vocabulary. Furthermore, we are able to efficiently train the deep model which consists of six recurrent networks, achieving higher performance. In summary, we propose a hierarchical architecture (character -> subword -> word -> source sentence -> target word -> target character) to train a deep character-level neural machine translator. We show that the model achieves a high translation performance which is comparable to the state-of-the-art neural machine translation model on the task of En-Fr, En-Cs and Cs-En translation. The experiments and analyses further support the statement that our model is able to learn the morphology.",
      "exclude": true
    },
    {
      "heading": "2 NEURAL MACHINE TRANSLATION",
      "text": "Neural machine translation is often implemented as an encoder-decoder architecture. The encoder usually uses a recurrent neural network (RNN) or a bidirectional recurrent neural network (BiRNN) (Schuster and Paliwal, 1997) to encode the input sentence x = x1, . . . , xTx into a sequence of hidden states h = h1, . . . ,hTx: ht = f1(e(xt),ht1), where e(xt) Rm is an m-dimensional embedding of xt. The decoder, another RNN, is often trained to predict next word yt given previous predicted words y1, . . . , yt1 and the context vector ct; that is, p(yt | y1, . . . , yt1) = g(e(yt1), st, ct), where st = f2(e(yt1), st1, ct) (1) and g is a nonlinear and potentially multi-layered function that computes the probability of yt. The context ct depends on the sequence of h1, . . . ,hTx. Sutskever et al. (2014) encoded all information in the source sentence into a fixed-length vector, i.e., ct = hTx . Bahdanau et al. (2015) computed ct by the alignment model which handles the bottleneck that the former approach meets. The whole model is jointly trained by maximizing the conditional log-probability of the correct translation given a source sentence with respect to the parameters of the model : = argmax Ty t=1 log p(yt | y1, . . . , yt1,x,). For the detailed description of the implementation, we refer the reader to the papers (Sutskever et al., 2014; Bahdanau et al., 2015).",
      "exclude": false
    },
    {
      "heading": "3 DEEP CHARACTER-LEVEL NEURAL MACHINE TRANSLATION",
      "text": "We consider two problems in the word-level neural machine translation models. First, how can we map a word to a vector? It is usually done by a lookup table (embedding matrix) where the size of vocabulary is limited. Second, how do we map a vector to a word when predicting? It is usually done via a softmax function. However, the large vocabulary will make the softmax intractable computationally. We correspondingly devise two novel architectures, a word encoder which utilizes the morphology and a hierarchical decoder which decodes at character level. Accordingly, we propose a deep character-level neural machine translation model (DCNMT).",
      "exclude": false
    },
    {
      "heading": "3.1 LEARNING MORPHOLOGY IN A WORD ENCODER",
      "text": "Many words can be subdivided into smaller meaningful units called morphemes, such as any-one, any-thing and every-one. At the basic level, words are made of morphemes which are recognized as grammatically significant or meaningful. Different combinations of morphemes lead to different meanings. Based on these facts, we introduce a word encoder to learn the morphemes and the rules of how they are combined. Even if the word encoder had never seen everything before, with a understanding of English morphology, the word encoder could gather the meaning easily. Thus learning morphology in a word encoder might speedup training. The word encoder is based on two recurrent neural networks, as illustrated in Figure 1. We compute the representation of the word anyone as ranyone = tanh( 6 t=1 wtrt), where rt is an RNN hidden state at time t, computed by rt = f(e(xt), rt1). Each rt contains information about the preceding characters. The weight wt of each representation rt is computed by wt = exp(aff(ht)), where ht is another RNN hidden state at time t and aff() is an affine function which maps ht to a scalar. Here, we use a BiRNN to compute ht as shown in Figure 1. Instead of normalizing it by t exp(aff(ht)), we use an activation function tanh as it performs best in experiments. We can regard the weight wi as the energy that determines whether ri is a representation of a morpheme and how it contributes to the representation of the word. Compared with an embedding lookup table, the decoupled RNNs learn the representation of morphemes and the rules of how they are combined respectively, which may be viewed as learning distributed representations of words explicitly. For example, we are able to translate convenienter correctly which validates our idea. After obtaining the representation of the word, we could encode the sentence using a bidirectional RNN as RNNsearch (Bahdanau et al., 2015). The detailed architecture is shown in Figure 2.",
      "exclude": false
    },
    {
      "heading": "3.2 HIERARCHICAL DECODER",
      "text": "To decode at the character level, we introduce a hierarchical decoder. The first-level decoder is similar to RNNsearch which contains the information of the target word. Specifically, st in Eqn. (1) contains the information of target word at time t. Instead of using a multi-layer network following a softmax function to compute the probability of each target word using st, we employ a second-level decoder which generates a character sequence based on st. We proposed a variant of the gate recurrent unit (GRU) (Cho et al., 2014; Chung et al., 2014) that used in the second-level decoder and we denote it as HGRU (It is possible to use the LSTM (Hochreiter and Schmidhuber, 1997) units instead of the GRU described here). HGRU has a settable state and generates character sequence based on the given state until generating a delimiter. In our model, the state is initialized by the output of the first-level decoder. Once HGRU generates a delimiter, it will set the state to the next output of the first-level decoder. Given the previous output character sequence y0, y1, . . . , yt1 where y0 is a token representing the start of sentence, and the auxiliary sequence a0, a1, . . . , at1 which only contains 0 and 1 to indicate whether yi is a delimiter (a0 is set to 1), HGRU updates the state as follows: gt1 = (1 at1)gt1 + at1sit , (2) qjt = ([Wqe(yt1)] j + [Uqgt1] j), (3) zjt = ([Wze(yt1)] j + [Uzgt1] j), (4) gjt = ([We(yt1)] j + [U(qt gt1)]j), (5) gjt = z j tg j t1 + (1 z j t )g j t , (6) where sit is the output of the first-level decoder which calculated as Eqn. (8). We can compute the probability of each target character yt based on gt with a softmax function: p(yt | y1, . . . , yt1,x) = softmax(gt). (7) The current problem is that the number of outputs of the first-level decoder is much fewer than the target character sequence. It will be intractable to conditionally pick outputs from the the first-level decoder when training in batch manner (at least intractable for Theano (Bastien et al., 2012) and other symbolic deep learning frameworks to build symbolic expressions). Luong and Manning (2016) uses two forward passes (one for word-level and another for character-level) in batch training which is less efficient. However, in our model, we use a matrix to unfold the outputs of the first-level decoder, which makes the batch training process more efficient. It is a Ty T matrix R, where Ty is the number of delimiter (number of words) in the target character sequence and T is the length of the target character sequence. R[i, j1 + 1] to R[i, j2] are set as 1 if j1 is the index of the (i1)-th delimiter and j2 is the index of the i-th delimiter in the target character sequence. The index of the 0-th delimiter is set as 0. For example, when the target output is g o ! and the output of the first-level decoder is [s1, s2], the unfolding step will be: [s1, s2] [ 1 1 1 0 0 0 0 0 1 1 ] = [s1, s1, s1, s2, s2], therefore si1 , si2 , si3 , si4 , si5 is correspondingly set to s1, s1, s1, s2, s2 in HGRU iterations. After this procedure, we can compute the probability of each target character by the second-level decoder according to Eqns. (2) to (7).",
      "exclude": false
    },
    {
      "heading": "3.3 MODEL ARCHITECTURES",
      "text": "There are totally six recurrent neural networks in our model, which can be divided into four layers as shown in Figure 2. Figure 2 illustrates the training procedure of a basic deep character-level neural machine translation. It is possible to use multi-layer recurrent neural networks to make the model deeper. The first layer is a source word encoder which contains two RNNs as shown in Figure 1. The second layer is a bidirectional RNN sentence encoder which is identical to that of (Bahdanau et al., 2015). The third layer is the first-level decoder. It takes the representation of previous target word as a feedback, which is produced by the target word encoder in our model. As the feedback is less important, we use an ordinary RNN to encode the target word. The feedback rYt1 then combines the previous hidden state ut1 and the context ct from the sentence encoder to generate the vector st: st = W1ct +W2rYt1 +W3ut1 + b. (8) With the state of HGRU in the second-level decoder setting to st and the information of previous generated character, the second-level decoder generates the next character until generating an end of sentence token (denoted as in Figure 2). With such a hierarchical architecture, we can train our character-level neural translation model perfectly well in an end-to-end fashion.",
      "exclude": false
    },
    {
      "heading": "3.4 GENERATION PROCEDURE",
      "text": "We first encode the source sequence as in the training procedure, then we generate the target sequence character by character based on the output st of the first-level decoder. Once we generate a delimiter, we should compute next vector st+1 according to Eqn. (8) by combining feedback rYt from the target word encoder, the context ct+1 from the sentence encoder and the hidden state ut. The generation procedure will terminate once an end of sentence (EOS) token is produced.",
      "exclude": false
    },
    {
      "heading": "4 EXPERIMENTS",
      "text": "We implement the model using Theano (Bergstra et al., 2010; Bastien et al., 2012) and Blocks (van Merrienboer et al., 2015), the source code and the trained models are available at github 1. We train our model on a single GTX Titan X with 12GB RAM. First we evaluate our model on English-toFrench translation task where the languages are morphologically poor. For fair comparison, we use the same dataset as in RNNsearch which is the bilingual, parallel corpora provided by ACL WMT14. In order to show the strengths of our model, we conduct on the English-to-Czech and Czech-to-English translation tasks where Czech is a morphologically rich language. We use the same dataset as (Chung et al., 2016a; Lee et al., 2016) which is provided by ACL WMT152.",
      "exclude": false
    },
    {
      "heading": "4.1 DATASET",
      "text": "We use the parallel corpora for two language pairs from WMT: En-Cs and En-Fr. They consist of 15.8M and 12.1M sentence pairs, respectively. In terms of preprocessing, we only apply the usual tokenization. We choose a list of 120 most frequent characters for each language which coveres nearly 100% of the training data. Those characters not included in the list are mapped to a special token 1https://github.com/SwordYork/DCNMT 2http://www.statmt.org/wmt15/translation-task.html ( ). We use newstest2013(Dev) as the development set and evaluate the models on newstest2015 (Test). We do not use any monolingual corpus.",
      "exclude": false
    },
    {
      "heading": "4.2 TRAINING DETAILS",
      "text": "We follow (Bahdanau et al., 2015) to use similar hyperparameters. The bidirectional RNN sentence encoder and the hierarchical decoder both consists of two-layer RNNs, each has 1024 hidden units; We choose 120 most frequent characters for DCNMT and the character embedding dimensionality is 64. The source word is encoded into a 600-dimensional vector. The other GRUs in our model have 512 hidden units. We use the ADAM optimizer (Kingma and Ba, 2015) with minibatch of 56 sentences to train each model (for En-Fr we use a minibatch of 72 examples). The learning rate is first set to 103 and then annealed to 104. We use a beam search to find a translation that approximately maximizes the conditional logprobability which is a commonly used approach in neural machine translation (Sutskever et al., 2014; Bahdanau et al., 2015). In our DCNMT model, it is reasonable to search directly on character level to generate a translation.",
      "exclude": false
    },
    {
      "heading": "5 RESULT AND ANALYSIS",
      "text": "We conduct comparison of quantitative results on the En-Fr, En-Cs and Cs-En translation tasks in Section 5.1. Apart from measuring translation quality, we analyze the efficiency of our model and effects of character-level modeling in more details.",
      "exclude": false
    },
    {
      "heading": "5.1 QUANTITATIVE RESULTS",
      "text": "We illustrate the efficiency of the deep character-level neural machine translation by comparing with the bpe-based subword model (Sennrich et al., 2016) and other character-level models. We measure the performance by BLEU score (Papineni et al., 2002). In Table 1, Length indicates the maximum sentence length in training (based on the number of words or characters), Size is the total number of parameters in the models. We report the BLEU scores of DCNMT when trained after one epoch in the above line and the final scores in the following line. The results of other models are taken from (1)Firat et al. (2016), (3)Chung et al. (2016a), (4)Lee et al. (2016) and (5)Luong and Manning (2016) respectively, except (2) is trained according to Ling et al. (2015b). The only difference between CNMT and DCNMT is CNMT uses an ordinary RNN to encode source words (takes the last hidden state). The training time for (3) and (4) is calculated based on the training speed in (Lee et al., 2016). For each test set, the best scores among the models per language pair are bold-faced. Obviously, character-level models are better than the subword-level models, and our model is comparable to the start-of-the-art character-level models. Note that, the purely character model of (5)(Luong and Manning, 2016) took 3 months to train and yielded +0.5 BLEU points compared to our result. We have analyzed the efficiency of our decoder in Section 3.2. Besides, our model is the simplest and the smallest one in terms of the model size.",
      "exclude": false
    },
    {
      "heading": "5.2 LEARNING MORPHOLOGY",
      "text": "In this section, we investigate whether our model could learn morphology. First we want to figure out the difference between an ordinary RNN word encoder and our word encoder. We choose some words with similar meaning but different in morphology as shown in Figure 3. We could find in Figure 3(a) that the words ending with ability, which are encoded by the ordinary RNN word encoder, are jammed together. In contrast, the representations produced by our encoder are more reasonable and the words with similar meaning are closer. Then we analyze how our word encoder learns morphemes and the rules of how they are combined. We demonstrate the encoding details on any* and every*. Figure 4(a) shows the energy of each character, more precisely, the energy of preceding characters. We could see that the last character of a morpheme will result a relative large energy (weight) like any and every in these words. Moreover, even the preceding characters are different, it will produce a similar weight for the same morpheme like way in anyway and everyway. The two-dimensional PCA projection in Figure 4(b) further validates our idea. The word encoder may be able to guess the meaning of everything even it had never seen everything before, thus speedup learning. More interestingly, we find that not only the ending letter has high energy, but also the beginning letter is important. It matches the behavior of human perception (White et al., 2008). Moreover, we apply our trained word encoder to Penn Treebank Line 1. Unlike Chung et al. (2016b), we are able to detect the boundary of the subword units. As shown in Figure 5, consumers, monday, football and greatest are segmented into consum-er-s,mon-day, foot-ball and great-est respectively. Since there are no explicit delimiters, it may be more difficult to detect the subword units.",
      "exclude": false
    },
    {
      "heading": "5.3 BENEFITING FROM LEARNING MORPHOLOGY",
      "text": "As analyzed in Section 5.2, learning morphology could speedup learning. This has also been shown in Table 1 (En-Fr and En-Cs task) from which we see that when we train our model just for one epoch, the obtained result even outperforms the final result with bpe baseline. Another advantage of our model is the ability to translate the misspelled words or the nonce words. The character-level model has a much better chance recovering the original word or sentence. In Table 2, we list some examples where the source sentences are taken from newstest2013 but we change some words to misspelled words or nonce words. We also list the translations from Google translate 3 and online demo of neural machine translation by LISA. As listed in Table 2(a), DCNMT is able to translate out the misspelled words correctly. For a word-based translator, it is never possible because the misspelled words are mapped into 3The translations by Google translate were made on Nov 4, 2016. token before translating. Thus, it will produce an token or just take the word from source sentence (Gulcehre et al., 2016; Luong et al., 2015). More interestingly, DCNMT could translate convenienter correctly as shown in Table 2(b). By concatenating convenient and er, we get the comparative adjective form of convenient which never appears in the training set; however, our model guessed it correctly based on the morphemes and the rules.",
      "exclude": false
    },
    {
      "heading": "6 CONCLUSION",
      "text": "In this paper we have proposed an hierarchical architecture to train the deep character-level neural machine translation model by introducing a novel word encoder and a multi-leveled decoder. We have demonstrated the efficiency of the training process and the effectiveness of the model in comparison with the word-level and other character-level models. The BLEU score implies that our deep characterlevel neural machine translation model likely outperforms the word-level models and is competitive with the state-of-the-art character-based models. It is possible to further improve performance by using deeper recurrent networks (Wu et al., 2016), training for more epochs and training with longer sentence pairs. As a result of the character-level modeling, we have solved the out-of-vocabulary (OOV) issue that word-level models suffer from, and we have obtained a new functionality to translate the misspelled or the nonce words. More importantly, the deep character-level is able to learn the similar embedding of the words with similar meanings like the word-level models. Finally, it would be potentially possible that the idea behind our approach could be applied to many other tasks such as speech recognition and text summarization.",
      "exclude": true
    },
    {
      "heading": "A DETAILED DESCRIPTION OF THE MODEL",
      "text": "Here we describe the implementation using Theano, it should be applicable to other symbolic deep learning frameworks. We use f to denote the transition of the recurrent network. A.1 SOURCE WORD ENCODER As illustrated in Section 3.1, the word encoder is based on two recurrent neural networks. We compute the representation of the word anyone as ranyone = tanh( 6 t=1 wtrt), where rt Rn is an RNN hidden state at time t, computed by rt = f(e(xt), rt1). Each rt contains information about the preceding characters. The weight wt of each representation rt is computed by wt = exp(Wwht + bw), where Ww R12l maps the vector ht R2l to a scalar and ht is the state of the BiRNN at time t: ht = [ h t h t ] . (9) h t Rl is the forward state of the BiRNN which is computed by h t = f(e(xt), h t1). (10) The backward state h t Rl is computed similarly, however in a reverse order. A.2 SOURCE SENTENCE ENCODER After encoding the words by the source word encoder, we feed the representations to the source sentence encoder. For example, the source Hello world is encoded into a vector [rHello, rworld, r ], then the BiRNN sentence encoder encodes this vector into [v1,v2,v3]. The computation is the same as Eqn. (9) and Eqn. (10), however the input now changes to the representation of the words. A.3 FIRST-LEVEL DECODER The first-level decoder is similar to Bahdanau et al. (2015) which utilizes the attention mechanism. Given the context vector ct from encoder, the hidden state ut Rm of the GRU is computed by ut = (1 zt) ut1 + zt ut, where ut = tanh(WrYt1 +U[qt ut1] +Cct) zt = (WzrYt1 +Uzut1 +Czct) qt = (WqrYt1 +Uqut1 +Cqct). rYt1 is the representation of the target word which is produced by an ordinary RNN (take the last state). The context vector ct is computed by the attention mechanism at each step: ct = Tx j=1 tjvj , where tj = exp(etj)Tx k=1 exp(etk) etj = E tanh(Weut1 +Hehj). E R1m which maps the vector into a scalar. Then the hidden state ut is further processed as Eqn. (8) before feeding to the second-level decoder: st+1 = W1ct+1 +W2rYt +W3ut + b. A.4 SECOND-LEVEL DECODER As described in Section 3.2, the number of outputs of the first-level decoder is much fewer than the target character sequence. It will be intractable to conditionally pick outputs from the the first-level decoder when training in batch manner (at least intractable for Theano (Bastien et al., 2012) and other symbolic deep learning frameworks to build symbolic expressions). We use a matrix R RTyT to unfold the outputs [s1, . . . , sTy ] of the first-level decoder (Ty is the number of words in the target sentence and T is the number of characters). R is a symbolic matrix in the final loss, it is constructed according the delimiters in the target sentences when training (see Section 3.2 for the detailed construction, note that R is a tensor in batch training. ). After unfolding, the input of HGRU becomes [si1 , . . . , siT ], that is [si1 , . . . , siT ] = [s1, . . . , sTy ]R. According to Eqns.(2) to (7), we can compute the probability of each target character : p(yt | y1, . . . , yt1,x) = softmax(gt). Finally, we could compute the cross-entroy loss and train with SGD algorithm.",
      "exclude": false
    },
    {
      "heading": "B SAMPLE TRANSLATIONS",
      "text": "We show additional sample translations in the following Tables.",
      "exclude": false
    }
  ],
  "fewShot": [
    {
      "model": "claude-haiku-4-5",
      "decision": {
        "rejection": false,
        "confidence": 0.72,
        "primary_reason": "Novel hierarchical character-level architecture with morphology learning for NMT; solid experimental validation on morphologically rich languages; demonstrates practical advantages (faster convergence, handling OOV words) with reasonable BLEU results, though incremental over concurrent work."
      },
      "token": {
        "prompt_tokens": 5964,
        "completion_tokens": 89,
        "total_tokens": 6053
      },
      "time": "2026-02-08T22:41:10.283136+00:00"
    },
    {
      "model": "gpt-5.2",
      "decision": {
        "rejection": true,
        "confidence": 0.72,
        "primary_reason": "While the character-level hierarchical decoder + morphology-motivated word encoder is a reasonable engineering design, the novelty is limited relative to contemporaneous character/subword NMT work, and the empirical evidence is not strong enough for ICML/ICLR/NeurIPS: gains are emphasized after only 1 epoch (not a standard comparison point), final BLEU is only “comparable” to prior character-level models, and key claims (training efficiency, learned morphology) rely on qualitative/PCA-style analyses and selective examples rather than rigorous ablations, controlled compute/throughput reporting, and strong baselines."
      },
      "token": {
        "prompt_tokens": 7867,
        "completion_tokens": 148,
        "total_tokens": 8015
      },
      "time": "2026-02-09T20:43:43.178697+00:00"
    }
  ],
  "test": false
}