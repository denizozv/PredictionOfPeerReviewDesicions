{
  "id": "335",
  "title": "An Information-Theoretic Framework for Fast and Robust Unsupervised Learning via Neural Population Infomax",
  "abstract": "A framework is presented for unsupervised learning of representations based on infomax principle for large-scale neural populations. We use an asymptotic approximation to the Shannon's mutual information for a large neural population to demonstrate that a good initial approximation to the global information-theoretic optimum can be obtained by a hierarchical infomax method. Starting from the initial solution, an efficient algorithm based on gradient descent of the final objective function is proposed to learn representations from the input datasets, and the method works for complete, overcomplete, and undercomplete bases. As confirmed by numerical experiments, our method is robust and highly efficient for extracting salient features from input datasets. Compared with the main existing methods, our algorithm has a distinct advantage in both the training speed and the robustness of unsupervised representation learning. Furthermore, the proposed method is easily extended to the supervised or unsupervised model for training deep structure networks.",
  "accepted": true,
  "zeroShot": [
    {
      "model": "claude-haiku-4-5",
      "decision": {
        "rejection": true,
        "confidence": 0.85,
        "primary_reason": "Limited experimental validation and unclear novelty. Paper presents information-theoretic framework for unsupervised learning but experiments are restricted to small-scale datasets (MNIST, 12x12 image patches). Comparisons are primarily with older methods (ICA, RBM from 2006-2010). No evaluation on modern benchmarks or deep learning tasks. Theoretical contributions, while detailed, lack clear advantages over existing approaches. Missing comparisons with contemporary unsupervised learning methods (VAE, modern contrastive learning). Presentation is dense with heavy mathematical notation that obscures practical contributions."
      },
      "token": {
        "prompt_tokens": 15612,
        "completion_tokens": 153,
        "total_tokens": 15765
      },
      "time": "2026-02-05T22:36:11.199895+00:00"
    },
    {
      "model": "gpt-5.2",
      "decision": {
        "rejection": true,
        "confidence": 0.83,
        "primary_reason": "Insufficient novelty and experimental rigor for a top-tier ML venue (largely re-derives/extends Infomax ICA with limited modern baselines, unclear empirical validation beyond qualitative filters and proxy entropies, and heavy reliance on asymptotic approximations/assumptions without strong ablations or downstream task results)."
      },
      "token": {
        "prompt_tokens": 13884,
        "completion_tokens": 84,
        "total_tokens": 13968
      },
      "time": "2026-02-09T21:09:31.225305+00:00"
    }
  ],
  "article": [
    {
      "heading": "",
      "text": "A framework is presented for unsupervised learning of representations based on infomax principle for large-scale neural populations. We use an asymptotic approximation to the Shannons mutual information for a large neural population to demonstrate that a good initial approximation to the global information-theoretic optimum can be obtained by a hierarchical infomax method. Starting from the initial solution, an efficient algorithm based on gradient descent of the final objective function is proposed to learn representations from the input datasets, and the method works for complete, overcomplete, and undercomplete bases. As confirmed by numerical experiments, our method is robust and highly efficient for extracting salient features from input datasets. Compared with the main existing methods, our algorithm has a distinct advantage in both the training speed and the robustness of unsupervised representation learning. Furthermore, the proposed method is easily extended to the supervised or unsupervised model for training deep structure networks.",
      "exclude": true
    },
    {
      "heading": "1 INTRODUCTION",
      "text": "How to discover the unknown structures in data is a key task for machine learning. Learning good representations from observed data is important because a clearer description may help reveal the underlying structures. Representation learning has drawn considerable attention in recent years (Bengio et al., 2013). One category of algorithms for unsupervised learning of representations is based on probabilistic models (Lewicki & Sejnowski, 2000; Hinton & Salakhutdinov, 2006; Lee et al., 2008), such as maximum likelihood (ML) estimation, maximum a posteriori (MAP) probability estimation, and related methods. Another category of algorithms is based on reconstruction error or generative criterion (Olshausen & Field, 1996; Aharon et al., 2006; Vincent et al., 2010; Mairal et al., 2010; Goodfellow et al., 2014), and the objective functions usually involve squared errors with additional constraints. Sometimes the reconstruction error or generative criterion may also have a probabilistic interpretation (Olshausen & Field, 1997; Vincent et al., 2010). Shannons information theory is a powerful tool for description of stochastic systems and could be utilized to provide a characterization for good representations (Vincent et al., 2010). However, computational difficulties associated with Shannons mutual information (MI) (Shannon, 1948) have hindered its wider applications. The Monte Carlo (MC) sampling (Yarrow et al., 2012) is a convergent method for estimating MI with arbitrary accuracy, but its computational inefficiency makes it unsuitable for difficult optimization problems especially in the cases of high-dimensional input stimuli and large population networks. Bell and Sejnowski (Bell & Sejnowski, 1995; 1997) have directly applied the infomax approach (Linsker, 1988) to independent component analysis (ICA) of data with independent non-Gaussian components assuming additive noise, but their method requires that the number of outputs be equal to the number of inputs. The extensions of ICA to overcomplete or undercomplete bases incur increased algorithm complexity and difficulty in learning of parameters (Lewicki & Sejnowski, 2000; Kreutz-Delgado et al., 2003; Karklin & Simoncelli, 2011). Since Shannon MI is closely related to ML and MAP (Huang & Zhang, 2017), the algorithms of representation learning based on probabilistic models should be amenable to information-theoretic treatment. Representation learning based on reconstruction error could be accommodated also by information theory, because the inverse of Fisher information (FI) is the Cramer-Rao lower bound on the mean square decoding error of any unbiased decoder (Rao, 1945). Hence minimizing the reconstruction error potentially maximizes a lower bound on the MI (Vincent et al., 2010). Related problems arise also in neuroscience. It has long been suggested that the real nervous systems might approach an information-theoretic optimum for neural coding and computation (Barlow, 1961; Atick, 1992; Borst & Theunissen, 1999). However, in the cerebral cortex, the number of neurons is huge, with about 105 neurons under a square millimeter of cortical surface (Carlo & Stevens, 2013). It has often been computationally intractable to precisely characterize information coding and processing in large neural populations. To address all these issues, we present a framework for unsupervised learning of representations in a large-scale nonlinear feedforward model based on infomax principle with realistic biological constraints such as neuron models with Poisson spikes. First we adopt an objective function based on an asymptotic formula in the large population limit for the MI between the stimuli and the neural population responses (Huang & Zhang, 2017). Since the objective function is usually nonconvex, choosing a good initial value is very important for its optimization. Starting from an initial value, we use a hierarchical infomax approach to quickly find a tentative global optimal solution for each layer by analytic methods. Finally, a fast convergence learning rule is used for optimizing the final objective function based on the tentative optimal solution. Our algorithm is robust and can learn complete, overcomplete or undercomplete basis vectors quickly from different datasets. Experimental results showed that the convergence rate of our method was significantly faster than other existing methods, often by an order of magnitude. More importantly, the number of output units processed by our method can be very large, much larger than the number of inputs. As far as we know, no existing model can easily deal with this situation.",
      "exclude": true
    },
    {
      "heading": "2 METHODS",
      "text": "",
      "exclude": false
    },
    {
      "heading": "2.1 APPROXIMATION OF MUTUAL INFORMATION FOR NEURAL POPULATIONS",
      "text": "Suppose the input x is a K-dimensional vector, x = (x1, , xK)T , the outputs of N neurons are denoted by a vector, r = (r1, , rN )T , where we assume N is large, generally N K. We denote random variables by upper case letters, e.g., random variables X and R, in contrast to their vector values x and r. The MI between X and R is defined by I(X;R) = ln p(x|r)p(x) r,x , where r,x denotes the expectation with respect to the probability density function (PDF) p(r,x). Our goal is to maxmize MI I(X;R) by finding the optimal PDF p(r|x) under some constraint conditions, assuming that p(r|x) is characterized by a noise model and activation functions f(x;n) with parameters n for the n-th neuron (n = 1, , N ). In other words, we optimize p(r|x) by solving for the optimal parameters n. Unfortunately, it is intractable in most cases to solve for the optimal parameters that maximizes I(X;R). However, if p(x) and p(r|x) are twice continuously differentiable for almost every x RK , then for large N we can use an asymptotic formula to approximate the true value of I(X;R) with high accuracy (Huang & Zhang, 2017): I(X;R) ' IG = 1 2 ln ( det ( G(x) 2e )) x +H(X), (1) where det () denotes the matrix determinant and H(X) = ln p(x)x is the stimulus entropy, G(x) = J(x) + P (x) , (2) J(x) = 2 ln p (r|x) xxT r|x , (3) P(x) = 2 ln p (x) xxT . (4) Assuming independent noises in neuronal responses, we have p(r|x) = N n=1 p(rn|x;n), and the Fisher information matrix becomes J(x) N K1 k=1 kS(x;k), where S(x;k) = ln p(r|x;k) x ln p(r|x;k) xT r|x and k > 0 (k = 1, ,K1) is the population density of param- eter k, with K1 k=1 k = 1, and 1 K1 N (see Appendix A.1 for details). Since the cerebral cortex usually forms functional column structures and each column is composed of neurons with the same properties (Hubel & Wiesel, 1962), the positive integer K1 can be regarded as the number of distinct classes in the neural population. Therefore, given the activation function f(x;k), our goal becomes to find the optimal population distribution density k of parameter vector k so that the MI between the stimulus x and the response r is maximized. By Eq. (1), our optimization problem can be stated as follows: minimize QG[k] = 1 2 ln (det (G(x)))x , (5) subject to K1 k=1 k = 1, k > 0, k = 1, ,K1. (6) Since QG[k] is a convex function of k (Huang & Zhang, 2017), we can readily find the optimal solution for small K by efficient numerical methods. For large K, however, finding an optimal solution by numerical methods becomes intractable. In the following we will propose an alternative approach to this problem. Instead of directly solving for the density distribution k, we optimize the parameters k and k simultaneously under a hierarchical infomax framework.",
      "exclude": false
    },
    {
      "heading": "2.2 HIERARCHICAL INFOMAX",
      "text": "For clarity, we consider neuron model with Poisson spikes although our method is easily applicable to other noise models. The activation function f(x;n) is generally a nonlinear function, such as sigmoid and rectified linear unit (ReLU) (Nair & Hinton, 2010). We assume that the nonlinear function for the n-th neuron has the following form: f(x;n) = f(yn; n), where yn = w T nx. (7) with wn being aK-dimensional weights vector, f(yn; n) is a nonlinear function, n = (wTn , T n ) T and n are the parameter vectors (n = 1, , N ). In general, it is very difficult to find the optimal parameters, n, n = 1, , N , for the following reasons. First, the number of output neuronsN is very large, usuallyN K. Second, the activation function f(x;n) is a nonlinear function, which usually leads to a nonconvex optimization problem. For nonconvex optimization problems, the selection of initial values often has a great influence on the final optimization results. Our approach meets these challenges by making better use of the large number of neurons and by finding good initial values by a hierarchical infomax method. We divide the nonlinear transformation into two stages, mapping first from x to yn (n = 1, , N ), and then from yn to f(yn; n), where yn can be regarded as the membrane potential of the n-th neuron, and f(yn; n) as its firing rate. As with the real neurons, we assume that the membrane potential is corrupted by noise: Yn = Yn + Zn, (8) where Zn N ( 0,2 ) is a normal distribution with mean 0 and variance 2. Then the mean membrane potential of the k-th class subpopulation with Nk = Nk neurons is given by Yk = 1 Nk Nk n=1 Ykn = Yk + Zk, k = 1, ,K1, (9) Zk N (0, N1k 2). (10) Define vectors y = (y1, , yN )T , y = (y1, , yK1)T and y = (y1, , yK1)T , where yk = wTk x (k = 1, ,K1). Notice that yn (n = 1, , N ) is also divided into K1 classes, the same as for rn. If we assume f(x;k) = f(yk; k), i.e. assuming an additive Gaussian noise for yn (see Eq. 9), then the random variables X , Y , Y , Y and R form a Markov chain, denoted by X Y Y Y R (see Figure 1), and we have the following proposition (see Appendix A.2). Proposition 1. With the random variables X , Y , Y , Y , R and Markov chain X Y Y Y R, the following equations hold, I(X;R) = I(Y ;R) I(Y ;R) I(Y ;R), (11) I(X;R) I(X; Y ) = I(X; Y ) I(X;Y ), (12) and for large Nk (k = 1, ,K1), I(Y ;R) ' I(Y ;R) ' I(Y ;R) = I(X;R), (13) I(X;Y ) ' I(X; Y ) = I(X; Y ). (14) A major advantage of incorporating membrane noise is that it facilitates finding the optimal solution by using the infomax principle. Moreover, the optimal solution obtained this way is more robust; that is, it discourages overfitting and has a strong ability to resist distortion. With vanishing noise 2 0, we have Yk Yk, f(yk; k) ' f(yk; k) = f(x;k), so that Eqs. (13) and (14) hold as in the case of large Nk. To optimize MI I(Y ;R), the probability distribution of random variable Y , p(y), needs to be determined, i.e. maximizing I(Y ;R) about p(y) under some constraints should yield an optimal distribution: p(y) = arg maxp(y) I(Y ;R). Let C = maxp(y) I (Y ;R) be the channel capacity of neural population coding, and we always have I(X;R) C (Huang & Zhang, 2017). To find a suitable linear transformation from X to Y that is compatible with this distribution p(y), a reasonable choice is to maximize I(X; Y ) ( I(X;Y )), where Y is a noise-corrupted version of Y . This implies minimum information loss in the first transformation step. However, there may exist many transformations from X to Y that maximize I(X; Y ) (see Appendix A.3.1). Ideally, if we can find a transformation that maximizes both I(X; Y ) and I(Y ;R) simultaneously, then I(X;R) reaches its maximum value: I(X;R) = maxp(y) I (Y ;R) = C. From the discussion above we see that maximizing I(X;R) can be divided into two steps, namely, maximizing I(X; Y ) and maximizing I(Y ;R). The optimal solutions of max I(X; Y ) and max I(Y ;R) will provide a good initial approximation that tend to be very close to the optimal solution of max I(X;R). Similarly, we can extend this method to multilayer neural population networks. For example, a twolayer network with outputs R(1) and R(2) form a Markov chain, X R(1) R(1) R(1) R(2), where random variable R(1) is similar to Y , random variable R(1) is similar to Y , and R(1) is similar to Y in the above. Then we can show that the optimal solution of max I(X;R(2)) can be approximated by the solutions of max I(X;R(1)) and max I(R(1);R(2)), with I(R(1);R(2)) ' I(R(1);R(2)). More generally, consider a highly nonlinear feedforward neural network that maps the input x to output z, with z = F (x;) = hL h1 (x), where hl (l = 1, , L) is a linear or nonlinear function (Montufar et al., 2014). We aim to find the optimal parameter by maximizing I (X;Z). It is usually difficult to solve the optimization problem when there are many local extrema for F (x;). However, if each function hl is easy to optimize, then we can use the hierarchical infomax method described above to get a good initial approximation to its global optimization solution, and go from there to find the final optimal solution. This information-theoretic consideration from the neural population coding point of view may help explain why deep structure networks with unsupervised pre-training have a powerful ability for learning representations.",
      "exclude": false
    },
    {
      "heading": "2.3 THE OBJECTIVE FUNCTION",
      "text": "The optimization processes for maximizing I(X; Y ) and maximizing I(Y ;R) are discussed in detail in Appendix A.3. First, by maximizing I(X; Y ) (see Appendix A.3.1 for details), we can get the optimal weight parameter wk (k = 1, ,K1, see Eq. 7) and its population density k (see Eq. 6) which satisfy W = [w1, ,wK1 ] = aU0 1/2 0 C, (15) 1 = = K1 = K11 , (16) where a = K1K 1 0 , C = [c1, , cK1 ] RK0K1 , CC T = IK0 , IK0 is a K0 K0 identity matrix with integer K0 [1,K], the diagonal matrix 0 RK0K0 and matrix U0 RKK0 are given in (A.44) and (A.45), with K0 given by Eq. (A.52). Matrices 0 and U0 can be obtained by and U with UT0 U0 = IK0 and U00U T 0 UU T xxT x (see Eq. A.23). The optimal weight parameter wk (15) means that the input variable x must first undergo a whiteninglike transformation x = 1/20 U T 0 x, and then goes through the transformation y = aC T x, with matrix C to be optimized below. Note that weight matrix W satisfies rank(W) = min(K0,K1), which is a low rank matrix, and its low dimensionality helps reduce overfitting during training (see Appendix A.3.1). By maximizing I (Y ;R) (see Appendix A.3.2), we further solve the the optimal parameters k for the nonlinear functions f(yk; k), k = 1, ,K1. Finally, the objective function for our optimization problem (Eqs. 5 and 6) turns into (see Appendix A.3.3 for details): minimize Q [C] = 1 2 ln ( det ( CC T )) x , (17) subject to CCT = IK0 , (18) where = diag ( (y1) 2, , (yK1)2 ) , (yk) = a1 |gk(yk)/yk| (k = 1, ,K1), gk(yk) = 2 f(yk; k), yk = a1yk = cTk x, and x = 1/2 0 U T 0 x. We apply the gradient descent method to optimize the objective function, with the gradient of Q[C] given by: dQ[C] dC = ( CC T )1 C + xT x , (19) where = (1, , K1) T , k = (yk)(yk)cTk ( CC T )1 ck, k = 1, ,K1. When K0 = K1 (or K0 > K1), the objective function Q[C] can be reduced to a simpler form, and its gradient is also easy to compute (see Appendix A.4.1). However, when K0 < K1, it is computationally expensive to update C by applying the gradient of Q[C] directly, since it requires matrix inversion for every x. We use another objective function Q[C] (see Eq. A.118) which is an approximation to Q[C], but its gradient is easier to compute (see Appendix A.4.2). The function Q[C] is the approximation of Q[C], ideally they have the same optimal solution for the parameter C. Usually, for optimizing the objective in Eq. 17, the orthogonality constraint (Eq. 18) is unnecessary. However, this orthogonality constraint can accelerate the convergence rate if we employ it for the initial iteration to update C (see Appendix A.5).",
      "exclude": false
    },
    {
      "heading": "3 EXPERIMENTAL RESULTS",
      "text": "We have applied our methods to the natural images from Olshausens image dataset (Olshausen & Field, 1996) and the images of handwritten digits from MNIST dataset (LeCun et al., 1998) using Matlab 2016a on a computer with 12 Intel CPU cores (2.4 GHz). The gray level of each raw image was normalized to the range of 0 to 1. M image patches with size w w = K for training were randomly sampled from the images. We used the Poisson neuron model with a modified sigmoidal tuning function f(y; ) = 1 4(1+exp(yb))2 , with g(y) = 2 f(y; ) = 11+exp(yb) , where = (, b) T . We obtained the initial values (see Appendix A.3.2): b0 = 0 and 0 1.81 K1K 1 0 . For our experiments, we set = 0.50 for iteration epoch t = 1, , t0 and = 0 for t = t0 + 1, , tmax, where t0 = 50. Firstly, we tested the case of K = K0 = K1 = 144 and randomly sampled M = 105 image patches with size 1212 from the Olshausens natural images, assuming thatN = 106 neurons were divided into K1 = 144 classes and = 1 (see Eq. A.52 in Appendix). The input patches were preprocessed by the ZCA whitening filters (see Eq. A.68). To test our algorithms, we chose the batch size to be equal to the number of training samples M , although we could also choose a smaller batch size. We updated the matrix C from a random start, and set parameters tmax = 300, v1 = 0.4, and = 0.8 for all experiments. In this case, the optimal solution C looked similar to the optimal solution of IICA (Bell & Sejnowski, 1997). We also compared with the fast ICA algorithm (FICA) (Hyvarinen, 1999), which is faster than IICA. We also tested the restricted Boltzmann machine (RBM) (Hinton et al., 2006) for a unsupervised learning of representations, and found that it could not easily learn Gabor-like filters from Olshausens image dataset as trained by contrastive divergence. However, an improved method by adding a sparsity constraint on the output units, e.g., sparse RBM (SRBM) (Lee et al., 2008) or sparse autoencoder (Hinton, 2010), could attain Gabor-like filters from this dataset. Similar results with Gabor-like filters were also reproduced by the denoising autoencoders (Vincent et al., 2010), which method requires a careful choice of parameters, such as noise level, learning rate, and batch size. In order to compare our methods, i.e. Algorithm 1 (Alg.1, see Appendix A.4.1) and Algorithm 2 (Alg.2, see Appendix A.4.2), with other methods, i.e. IICA, FICA and SRBM, we implemented these algorithms using the same initial weights and the same training data set (i.e. 105 image patches preprocessed by the ZCA whitening filters). To get a good result by IICA, we must carefully select the parameters; we set the batch size as 50, the initial learning rate as 0.01, and final learning rate as 0.0001, with an exponential decay with the epoch of iterations. IICA tends to have a faster convergence rate for a bigger batch size but it may become harder to escape local minima. For FICA, we chose the nonlinearity function f(u) = log cosh(u) as contrast function, and for SRBM, we set the sparseness control constant p as 0.01 and 0.03. The number of epoches for iterations was set to 300 for all algorithms. Figure 2 shows the filters learned by our methods and other methods. Each filter in Figure 2(a) corresponds to a column vector of matrix C (see Eq. A.69), where each vector for display is normalized by ck ck/max(|c1,k|, , |cK,k|), k = 1, ,K1. The results in Figures 2(a), 2(b) and 2(c) look very similar to one another, and slightly different from the results in Figure 2(d) and 2(e). There are no Gabor-like filters in Figure 2(f), which corresponds to SRBM with p = 0.03. Figure 3 shows how the coefficient entropy (CFE) (see Eq. A.122) and the conditional entropy (CDE) (see Eq. A.125) varied with training time. We calculated CFE and CDE by sampling once every 10 epoches from a total of 300 epoches. These results show that our algorithms had a fast convergence rate towards stable solutions while having CFE and CDE values similar to the algorithm of IICA, which converged much more slowly. Here the values of CFE and CDE should be as small 100 101 102 1.8 1.85 1.9 1.95 2 co ef fic ie nt e nt ro py ( bi ts ) Alg.1 Alg.2 IICA FICA SRBM (p = 0.01) SRBM (p = 0.03) 100 101 102 -400 -350 -300 -250 -200 -150 co nd iti on al e nt ro py ( bi ts ) Alg.1 Alg.2 IICA 100 101 102 -200 -100 0 100 200 300 co nd iti on al e nt ro py ( bi ts ) SRBM (p = 0.01) SRBM (p = 0.03) SRBM (p = 0.05) SRBM (p = 0.10) as possible for a good representation learned from the same data set. Here we set epoch number t0 = 50 in our algorithms (see Alg.1 and Alg.2), and the start time was set to 1 second. This explains the step seen in Figure 3 (b) for Alg.1 and Alg.2 since the parameter was updated when epoch number t = t0. FICA had a convergence rate close to our algorithms but had a big CFE, which is reflected by the quality of the filter results in Figure 2. The convergence rate and CFE for SRBM were close to IICA, but SRBM had a much bigger CDE than IICA, which implies that the information had a greater loss when passing through the system optimized by SRBM than by IICA or our methods. From Figure 3(c) we see that the CDE (or MI I(X;R), see Eq. A.124 and A.125) decreases (or increases) with the increase of the value of the sparseness control constant p. Note that a smaller p means sparser outputs. Hence, in this sense, increasing sparsity may result in sacrificing some information. On the other hand, a weak sparsity constraint may lead to failure of learning Gaborlike filters (see Figure 2(f)), and increasing sparsity has an advantage in reducing the impact of noise in many practical cases. Similar situation also occurs in sparse coding (Olshausen & Field, 1997), which provides a class of algorithms for learning overcomplete dictionary representations of the input signals. However, its training is time consuming due to its expensive computational cost, although many new training algorithms have emerged (e.g. Aharon et al., 2006; Elad & Aharon, 2006; Lee et al., 2006; Mairal et al., 2010). See Appendix A.5 for additional experimental results.",
      "exclude": false
    },
    {
      "heading": "4 CONCLUSIONS",
      "text": "In this paper, we have presented a framework for unsupervised learning of representations via information maximization for neural populations. Information theory is a powerful tool for machine learning and it also provides a benchmark of optimization principle for neural information processing in nervous systems. Our framework is based on an asymptotic approximation to MI for a large-scale neural population. To optimize the infomax objective, we first use hierarchical infomax to obtain a good approximation to the global optimal solution. Analytical solutions of the hierarchical infomax are further improved by a fast convergence algorithm based on gradient descent. This method allows us to optimize highly nonlinear neural networks via hierarchical optimization using infomax principle. From the viewpoint of information theory, the unsupervised pre-training for deep learning (Hinton & Salakhutdinov, 2006; Bengio et al., 2007) may be reinterpreted as a process of hierarchical infomax, which might help explain why unsupervised pre-training helps deep learning (Erhan et al., 2010). In our framework, a pre-whitening step can emerge naturally by the hierarchical infomax, which might also explain why a pre-whitening step is useful for training in many learning algorithms (Coates et al., 2011; Bengio, 2012). Our model naturally incorporates a considerable degree of biological realism. It allows the optimization of a large-scale neural population with noisy spiking neurons while taking into account of multiple biological constraints, such as membrane noise, limited energy, and bounded connection weights. We employ a technique to attain a low-rank weight matrix for optimization, so as to reduce the influence of noise and discourage overfitting during training. In our model, many parameters are optimized, including the population density of parameters, filter weight vectors, and parameters for nonlinear tuning functions. Optimizing all these model parameters could not be easily done by many other methods. Our experimental results suggest that our method for unsupervised learning of representations has obvious advantages in its training speed and robustness over the main existing methods. Our model has a nonlinear feedforward structure and is convenient for fast learning and inference. This simple and flexible framework for unsupervised learning of presentations should be readily extended to training deep structure networks. In future work, it would interesting to use our method to train deep structure networks with either unsupervised or supervised learning.",
      "exclude": true
    },
    {
      "heading": "ACKNOWLEDGMENTS",
      "text": "We thank Prof. Honglak Lee for sharing Matlab code for algorithm comparison, Prof. Shan Tan for discussions and comments and Kai Liu for helping draw Figure 1. Supported by grant NIH-NIDCD R01 DC013698.",
      "exclude": true
    },
    {
      "heading": "APPENDIX",
      "text": "",
      "exclude": true
    },
    {
      "heading": "A.1 FORMULAS FOR APPROXIMATION OF MUTUAL INFORMATION",
      "text": "It follows from I(X;R) = ln p(x|r)p(x) r,x and Eq. (1) that the conditional entropy should read: H(X|R) = ln p(x|r)r,x ' 1 2 ln ( det ( G(x) 2e )) x . (A.1) The Fisher information matrix J(x) (see Eq. 3), which is symmetric and positive semidefinite, can be written also as J(x) = ln p(r|x) x ln p(r|x) xT r|x . (A.2) If we suppose p(r|x) is conditional independent, namely, p(r|x) = N n=1 p(rn|x;n), then we have (see Huang & Zhang, 2017) J(x) = N p()S(x;)d, (A.3) S(x;) = ln p(r|x;) x ln p(r|x;) xT r|x , (A.4) where p() is the population density function of parameter , p() = 1 N N n=1 ( n), (A.5) and () denotes the Dirac delta function. It can be proved that the approximation function of MI IG[p()] (Eq. 1) is concave about p() (Huang & Zhang, 2017). In Eq. (A.3), we can approximate the continuous integral by a discrete summation for numerical computation, J(x) N K1 k=1 kS(x;k), (A.6) where K1 k=1 k = 1, k > 0, k = 1, ,K1, 1 K1 N . For Poisson neuron model, by Eq. (A.4) we have (see Huang & Zhang, 2017) p(r|x;) = f(x;) r r! exp (f(x;)), (A.7) S(x;) = 1 f(x;) f(x;) x f(x;) xT = g(x;) x g(x;) xT , (A.8) where f(x;) 0 is the activation function (mean response) of neuron and g(x;) = 2 f(x;). (A.9) Similarly, for Gaussian noise model, we have p(r|x;) = 1 2 exp ( (r f(x;)) 2 22 ) , (A.10) S(x;) = 1 2 f(x;) x f(x;) xT , (A.11) where > 0 denotes the standard deviation of noise. Sometimes we do not know the specific form of p(x) and only know M samples, x1, , xM , which are independent and identically distributed (i.i.d.) samples drawn from the distribution p(x). Then we can use the empirical average to approximate the integral in Eq. (1): IG 1 2 M m=1 ln (det (G(xm))) +H (X) . (A.12)",
      "exclude": false
    },
    {
      "heading": "A.2 PROOF OF PROPOSITION 1",
      "text": "Proof. It follows from the data-processing inequality (Cover & Thomas, 2006) that I(X;R) I(Y ;R) I(Y ;R) I(Y ;R), (A.13) I(X;R) I(X; Y ) I(X; Y ) I(X;Y ). (A.14) Since p(yk|x) = p(yk1 , , ykNk |x) = N (w T k x, N 1 k 2), k = 1, ,K1, (A.15) we have p(y|x) = p(y|x), (A.16) p(y) = p(y), (A.17) I(X; Y ) = I(X; Y ). (A.18) Hence, by (A.14) and (A.18), expression (12) holds. On the other hand, when Nk is large, from Eq. (10) we know that the distribution of Zk, namely, N ( 0,N1k 2 ) , approaches a Dirac delta function (zk). Then by (7) and (9) we have p (r|y) ' p(r|y) = p (r|x) and I(X;R) = I (Y ;R) ln p (r|y) p (r|x) r,x = I (Y ;R) , (A.19) I (Y ;R) = I(Y ;R) ln p (r|y) p (r|y) r,y,y ' I(Y ;R), (A.20) I (Y ;R) = I(Y ;R) ln p (r|y) p (r|y) r,y,y ' I(Y ;R), (A.21) I(X;Y ) = I(X; Y ) ln p (x|y) p (x|y) x,y,y ' I(X; Y ). (A.22) It follows from (A.13) and (A.19) that (11) holds. Combining (11), (12) and (A.20)(A.22), we immediately get (13) and (14). This completes the proof of Proposition 1. A.3 HIERARCHICAL OPTIMIZATION FOR MAXIMIZING I(X;R) In the following, we will discuss the optimization procedure for maximizing I(X;R) in two stages: maximizing I(X; Y ) and maximizing I(Y ;R).",
      "exclude": false
    },
    {
      "heading": "A.3.1 THE 1ST STAGE",
      "text": "In the first stage, our goal is to maximize the MI I(X; Y ) and get the optimal parameters wk (k = 1, ,K1). Assume that the stimulus x has zero mean (if not, let x x xx) and covariance matrix x. It follows from eigendecomposition that x = xxT x 1 M 1 XXT = UUT , (A.23) where X = [x1, , xM ], U = [u1, ,uK ] RKK is an unitary orthogonal matrix and = diag ( 21 , , 2K ) is a positive diagonal matrix with 1 K > 0. Define x = 1/2UTx, (A.24) wk = 1/2UTwk, (A.25) yk = w T k x, (A.26) where k = 1, ,K1. The covariance matrix of x is given by x = xxT x IK , (A.27) and IK is a K K identity matrix. From (1) and (A.11) we have I(X; Y ) = I(X; Y ) and I(X; Y ) ' I G = 1 2 ln ( det ( G 2e )) +H(X), (A.28) G N2 K1 k=1 kwkw T k + IK . (A.29) The following approximations are useful (see Huang & Zhang, 2017): p(x) N (0, IK), (A.30) P(x) = 2 ln p (x) xxT IK . (A.31) By the central limit theorem, the distribution of random variable X is closer to a normal distribution than the distribution of the original random variable X . On the other hand, the PCA models assume multivariate gaussian data whereas the ICA models assume multivariate non-gaussian data. Hence by a PCA-like whitening transformation (A.24) we can use the approximation (A.31) with the Laplaces method of asymptotic expansion, which only requires that the peak be close to its mean while random variable X needs not be exactly Gaussian. Without any constraints on the Gaussian channel of neural populations, especially the peak firing rates, the capacity of this channel may grow indefinitely: I(X; Y ) . The most common constraint on the neural populations is an energy or power constraint which can also be regarded as a signal-to-noise ratio (SNR) constraint. The SNR for the output yn of the n-th neuron is given by SNRn = 1 2 ( wTnx )2 x 1 2 wTn wn, n = 1, , N . (A.32) We require that 1 N N n=1 SNRn 1 2 K1 k=1 kw T k wk , (A.33) where is a positive constant. Then by Eq. (A.28), (A.29) and (A.33), we have the following optimization problem: minimize QG[W] = 1 2 ln ( det ( N2WW T + IK )) , (A.34) subject to h = Tr ( WW T ) E 0, (A.35) where Tr () denotes matrix trace and W = WA 1/2 = 1/2UTWA1/2 = [w1, , wK1 ], (A.36) A = diag (1, , K1), (A.37) W = [w1, ,wK1 ], (A.38) W = [w1, , wK1 ], (A.39) E = 2. (A.40) Here E is a constant that does not affect the final optimal solution so we set E = 1. Then we obtain an optimal solution as follows: W = aU0 1/2 0 V T 0 , (A.41) A = K11 IK1 , (A.42) a = EK1K 1 0 = K1K 1 0 , (A.43) 0 = diag ( 21 , , 2K0 ) , (A.44) U0 = U (:, 1:K0) RKK0 , (A.45) V0 = V (:, 1:K0) RK1K0 , (A.46) where V = [v1, ,vK1 ] is an K1 K1 unitary orthogonal matrix, parameter K0 represents the size of the reduced dimension (1 K0 K), and its value will be determined below. Now the optimal parameters wn (n = 1, , N ) are clustered into K1 classes (see Eq. A.6) and obey an uniform discrete distribution (see also Eq. A.60 in Appendix A.3.2). When K = K0 = K1, the optimal solution of W in Eq. (A.41) is a whitening-like filter. When V = IK , the optimal matrix W is the principal component analysis (PCA) whitening filters. In the symmetrical case with V = U, the optimal matrix W becomes a zero component analysis (ZCA) whitening filter. IfK < K1, this case leads to an overcomplete solution, whereas whenK0 K1 < K, the undercomplete solution arises. Since K0 K1 and K0 K, QG achieves its minimum when K0 = K. However, in practice other factors may prevent it from reaching this minimum. For example, consider the average of squared weights, = K1 k=1 k wk2 = Tr ( WAWT ) = E K0 K0 k=1 2k , (A.47) where denotes the Frobenius norm. The value of is extremely large when any k becomes vanishingly small. For real neurons these weights of connection are not allowed to be too large. Hence we impose a limitation on the weights: E1, where E1 is a positive constant. This yields another constraint on the objective function, h = E K0 K0 k=1 2k E1 0. (A.48) From (A.35) and (A.48) we get the optimal K0 = arg maxK0 ( EK10 K0 k=1 2 k ) . By this constraint, small values of 2k will often result in K0 < K and a low-rank matrix W (Eq. A.41). On the other hand, the low-rank matrix W can filter out the noise of stimulus x. Consider the transformation Y = WTX with X = [x1, , xM ] and Y = [y1, , yM ] for M samples. It follows from the singular value decomposition (SVD) of X that X = USV T , (A.49) where U is given in (A.23), V is aMM unitary orthogonal matrix, S is aKM diagonal matrix with non-negative real numbers on the diagonal, Sk,k = M 1k (k = 1, ,K, K M ), and SST = (M 1). Let X = M 1U01/20 VT0 X, (A.50) where V0 = V (:, 1:K0) RMK0 , 0 and U0 are given in (A.44) and (A.45), respectively. Then Y = WTX = aV0 1/2 0 U T 0 USV T = WT X = a M 1V0VT0 , (A.51) where X can be regarded as a denoised version of X. The determination of the effective rank K0 K of the matrix X by using singular values is based on various criteria (Konstantinides & Yao, 1988). Here we choose K0 as follows: K0 = arg min K0 K0k=1 2kK k=1 2 k , (A.52) where is a positive constant (0 < 1). Another advantage of a low-rank matrix W is that it can significantly reduce overfitting for learning neural population parameters. In practice, the constraint (A.47) is equivalent to a weight-decay regularization term used in many other optimization problems (Cortes & Vapnik, 1995; Hinton, 2010), which can reduce overfitting to the training data. To prevent the neural networks from overfitting, Srivastava et al. (2014) presented a technique to randomly drop units from the neural network during training, which may in fact be regarded as an attempt to reduce the rank of the weight matrix because the dropout can result in a sparser weights (lower rank matrix). This means that the update is only concerned with keeping the more important components, which is similar to first performing a denoising process by the SVD low rank approximation. In this stage, we have obtained the optimal parameter W (see A.41). The optimal value of matrix V0 can also be determined, as shown in Appendix A.3.3.",
      "exclude": false
    },
    {
      "heading": "A.3.2 THE 2ND STAGE",
      "text": "For this stage, our goal is to maximize the MI I(Y ;R) and get the optimal parameters k, k = 1, ,K1. Here the input is y = (y1, , yK1)T and the output r = (r1, , rN )T is also clustered into K1 classes. The responses of Nk neurons in the k-th subpopulation obey a Poisson distribution with mean f(eTk y; k), where ek is a unit vector with 1 in the k-th element and yk = e T k y. By (A.24) and (A.26), we have ykyk = 0, (A.53) 2yk = y2k yk = wk2 . (A.54) Then for large N , by (1)(4) and (A.30) we can use the following approximation, I(Y ;R) ' IF = 1 2 ln ( det ( J(y) 2e )) y +H(Y ), (A.55) where J(y) = diag ( N1 |g1(y1)| 2 , , NK1 gK1(yK1)2) , (A.56) gk(yk) = gk(yk) yk , k = 1, ,K1, (A.57) gk(yk) = 2 f(yk; k), k = 1, ,K1. (A.58) It is easy to get that IF = 1 2 K1 k=1 ln ( Nk |gk(yk)| 2 2e ) y +H(Y ) 1 2 K1 k=1 ln ( |gk(yk)| 2 2e ) y K1 2 ln ( K1 N ) +H(Y ), (A.59) where the equality holds if and only if k = 1 K1 , k = 1, ,K1, (A.60) which is consistent with Eq. (A.42). On the other hand, it follows from the Jensens inequality that IF = ln p (y)1 det( J(y) 2e )1/2 y ln det ( J(y) 2e )1/2 dy, (A.61) where the equality holds if and only if p (y)1 det ( J(y) )1/2 is a constant, which implies that p (y) = det ( J(y) )1/2 det ( J(y) )1/2 dy = K1 k=1 |gk(yk)| K1 k=1 |gk(yk)| dy . (A.62) From (A.61) and (A.62), maximizing IF yields p (yk) = |gk(yk)| |gk(yk)| dyk , k = 1, ,K1. (A.63) We assume that (A.63) holds, at least approximately. Hence we can let the peak of gk(yk) be at yk = ykyk = 0 and y2k yk = 2yk = wk 2. Then combining (A.57), (A.61) and (A.63) we find the optimal parameters k for the nonlinear functions f(yk; k), k = 1, ,K1.",
      "exclude": false
    },
    {
      "heading": "A.3.3 THE FINAL OBJECTIVE FUNCTION",
      "text": "In the preceding sections we have obtained the initial optimal solutions by maximizing I ( X; Y ) and I(Y ;R). In this section, we will discuss how to find the final optimal V0 and other parameters by maximizing I(X;R) from the initial optimal solutions. First, we have y = WT x = ay, (A.64) where a is given in (A.43) and y = (y1, , yK1)T = CT x = CT x, (A.65) x = 1/2 0 U T 0 x, (A.66) C = VT0 RK0K1 , (A.67) x = U0 1/2 0 U T 0 x = U0x, (A.68) C = U0C = [c1, , cK1 ]. (A.69) It follows that I(X;R) = I ( X;R ) ' IG = 1 2 ln ( det ( G(x) 2e )) x +H(X), (A.70) G(x) = NWW T + IK , (A.71) W = 1/2UTWA1/2 = a K11 I K K0C = K10 I K K0C, (A.72) where IKK0 is a K K0 diagonal matrix with value 1 on the diagonal and = 2, (A.73) = diag ((y1), , (yK1)) , (A.74) (yk) = a 1 gk(yk)yk , (A.75) gk(yk) = 2 f(yk; k), (A.76) yk = a 1yk = c T k x, k = 1, ,K1. (A.77) Then we have det (G(x)) = det ( NK10 CC T + IK0 ) . (A.78) For large N and K0/N 0, we have det (G(x)) det (J(x)) = det ( NK10 CC T ) , (A.79) IG IF = Q K 2 ln (2e) K0 2 ln () +H(X), (A.80) Q = 1 2 ln ( det ( CC T )) x , (A.81) = K0 N . (A.82) Hence we can state the optimization problem as: minimize Q [C] = 1 2 ln ( det ( CC T )) x , (A.83) subject to CCT = IK0 . (A.84) The gradient from (A.83) is given by: dQ[C] dC = ( CC T )1 C + xT x , (A.85) where C = [c1, , cK1 ], = (1, , K1) T , and k = (yk) (yk)c T k ( CC T )1 ck, k = 1, ,K1. (A.86) In the following we will discuss how to get the optimal solution of C for two specific cases.",
      "exclude": false
    },
    {
      "heading": "A.4 ALGORITHMS FOR OPTIMIZATION OBJECTIVE FUNCTION",
      "text": "A.4.1 ALGORITHM 1: K0 = K1 Now CCT = CTC = IK1 , then by Eq. (A.83) we have Q1[C] = K1 k=1 ln ((yk)) x , (A.87) dQ1[C] dC = xT x , (A.88) k = (yk) (yk) , k = 1, ,K1. (A.89) Under the orthogonality constraints (Eq. A.84), we can use the following update rule for learning C (Edelman et al., 1998; Amari, 1999): Ct+1 = Ct + t dCt dt , (A.90) dCt dt = dQ1[C t] dCt + Ct ( dQ1[C t] dCt )T Ct, (A.91) where the learning rate parameter t changes with the iteration count t, t = 1, , tmax. Here we can use the empirical average to approximate the integral in (A.88) (see Eq. A.12). We can also apply stochastic gradient descent (SGD) method for online updating of Ct+1 in (A.90). The orthogonality constraint (Eq. A.84) can accelerate the convergence rate. In practice, the orthogonal constraint (A.84) for objective function (A.83) is not strictly necessary in this case. We can completely discard this constraint condition and consider minimize Q2 [C] = K1 k=1 ln ( (yk)) x 1 2 ln ( det ( CTC )) , (A.92) where we assume rank (C) = K1 = K0. If we let dC dt = CCT dQ2 [C] dC , (A.93) then Tr ( dQ2 [C] dC dCT dt ) = Tr ( CT dQ2 [C] dC dQ2 [C] dCT C ) 0. (A.94) Therefore we can use an update rule similar to Eq. A.90 for learning C. In fact, the method can also be extended to the case K0 > K1 by using the same objective function (A.92). The learning rate parameter t (see A.90) is updated adaptively, as follows. First, calculate t = vt t , t = 1, , tmax, (A.95) t = 1 K1 K1 k=1 Ct(:, k) Ct(:, k) , (A.96) and Ct+1 by (A.90) and (A.91), then calculate the value Q1 [ Ct+1 ] . If Q1 [ Ct+1 ] 1) epochs for more accurate optimization solution C. In this case, the objective function is given by the Eq. (A.92). We can also further optimize parameter b by gradient descent. When K0 = K1, the objective function Q2 [C] in Eq. (A.92) without constraint is the same as the objective function of infomax ICA (IICA) (Bell & Sejnowski, 1995; 1997), and as a consequence we should get the same optimal solution C. Hence, in this sense, the IICA may be regarded as a special case of our method. Our method has a wider range of applications and can handle more generic situations. Our model is derived by neural populations with a huge number of neurons and it is not restricted to additive noise model. Moreover, our method has a faster convergence rate during training than IICA (see Section 3). A.4.2 ALGORITHM 2: K0 K1 In this case, it is computationally expensive to update C by using the gradient of Q (see Eq. A.85), since it needs to compute the inverse matrix for every x. Here we provide an alternative method for learning the optimal C. First, we consider the following inequalities. Proposition 2. The following inequations hold, 1 2 ln ( det ( CCT )) x 1 2 ln ( det ( C x CT )) , (A.97) ln ( det ( CCT )) x ln ( det ( C x C T )) (A.98) 1 2 ln ( det ( C 2x C T )) (A.99) 1 2 ln ( det ( C x CT )) , (A.100) ln ( det ( CCT )) 1 2 ln ( det ( CCT )) , (A.101) where C RK0K1 , K0 K1, and CCT = IK0 . Proof. Functions ln ( det ( C x CT )) and ln ( det ( C x CT )) are concave functions about p (x) (see the proof of Proposition 5.2. in Huang & Zhang, 2017), which fact establishes inequalities (A.97) and (A.98). Next we will prove the inequality (A.101). By SVD, we have C = UDV T , (A.102) where U is a K0 K0 unitary orthogonal matrix, V = [v1, v2, , vK1 ] is an K1 K1 unitary orthogonal matrix, and D is an K0K1 rectangular diagonal matrix with K0 positive real numbers on the diagonal. By the matrix Hadamards inequality and CauchySchwarz inequality we have det ( CCTCCT ) det ( CC T )1 = det ( DV T CTCVD T ( DD T )1) = det ( VT1 C TCV1 ) = det ( CV1 )2 K0 k=1 ( CV1 )2 k,k K0 k=1 ( CCT )2 k,k ( VT1 V1 )2 k,k = 1, (A.103) where V1 = [v1, v2, , vK0 ] RK1K0 . The last equality holds because of CC T = IK0 and VT1 V1 = IK0 . This establishes inequality (A.101) and the equality holds if and only if K0 = K1 or CV1 = IK0 . Similarly, we get inequality (A.99): ln ( det ( C x C T )) 1 2 ln ( det ( C 2x C T )) . (A.104) By Jensens inequality, we have (yk)2x (yk) 2 x , k = 1, ,K1. (A.105) Then it follows from (A.105) that inequality (A.100) holds: 1 2 ln ( det ( C 2x C T )) 1 2 ln ( det ( C x CT )) . (A.106) This completes the proof of Proposition 2. By Proposition 2, if K0 = K1 then we get 1 2 ln ( det ( )) x 1 2 ln ( det ( x )) , (A.107) ln (det ())x ln (det (x)) (A.108) = 1 2 ln ( det ( 2x )) (A.109) 1 2 ln ( det ( x )) , (A.110) ln (det ()) = 1 2 ln ( det ( )) . (A.111) On the other hand, it follows from (A.81) and Proposition 2 that ln ( det ( CCT )) x Q 1 2 ln ( det ( C x CT )) , (A.112) ln ( det ( CCT )) x Q 1 2 ln ( det ( C x CT )) . (A.113) Hence we can see that Q is close to Q (see A.81). Moreover, it follows from the CauchySchwarz inequality that ()k,k x = (yk)yk ( (yk) 2 dyk p (yk) 2 dyk )1/2 , (A.114) where k = 1, ,K1, the equality holds if and only if the following holds: p (yk) = (yk) (yk) dyk , k = 1, ,K1, (A.115) which is the similar to Eq. (A.63). Since I(X;R) = I(Y ;R) (see Proposition 1), by maximizing I(X;R) we hope the equality in inequality (A.61) and equality (A.63) hold, at least approximatively. On the other hand, let Copt = arg min C Q[C] = arg max C ( ln ( det(CCT ) ) x ) , (A.116) Copt = arg min C Q[C] = arg max C ( ln ( det ( C 2x C T ))) , (A.117) Copt and Copt make (A.63) and (A.115) to hold true, which implies that they are the same optimal solution: Copt = Copt. Therefore, we can use the following objective function Q[C] as a substitute for Q[C] and write the optimization problem as: minimize Q[C] = 1 2 ln ( det ( C 2x C T )) , (A.118) subject to CCT = IK0 . (A.119) The update rule (A.90) may also apply here and a modified algorithm similar to Algorithm 1 may be used for parameter learning.",
      "exclude": false
    },
    {
      "heading": "A.5 SUPPLEMENTARY EXPERIMENTS",
      "text": "",
      "exclude": false
    },
    {
      "heading": "A.5.1 QUANTITATIVE METHODS FOR COMPARISON",
      "text": "To quantify the efficiency of learning representations by the above algorithms, we calculate the coefficient entropy (CFE) for estimating coding cost as follows (Lewicki & Olshausen, 1999; Lewicki & Sejnowski, 2000): yk = w T k x, k = 1, ,K1, (A.120) = K1K1 k=1 wk , (A.121) where x is defined by Eq. (A.68), and wk is the corresponding optimal filter. To estimate the probability density of coefficients qk(yk) (k = 1, ,K1) from the M training samples, we apply the kernel density estimation for qk(yk) and use a normal kernel with an adaptive optimal window width. Then we define the CFE h as h = 1 K1 K1 k=1 Hk(Yk), (A.122) Hk(Yk) = nqk(n) log2 qk(n), (A.123) where qk(yk) is quantized as discrete qk(n) and is the step size. Methods such as IICA and SRBM as well as our methods have feedforward structures in which information is transferred directly through a nonlinear function, e.g., the sigmoid function. We can use the amount of transmitted information to measure the results learned by these methods. Consider a neural population with N neurons, which is a stochastic system with nonlinear transfer functions. We chose a sigmoidal transfer function and Gaussian noise with standard deviation set to 1 as the system noise. In this case, from (1), (A.8) and (A.11), we see that the approximate MI IG is equivalent to the case of the Poisson neuron model. It follows from (A.70)(A.82) that I(X;R) = I ( X;R ) = H(X)H ( X|R ) ' IG = H(X) h1, (A.124) H ( X|R ) ' h1 = 1 2 ln ( det ( 1 2e ( NK10 CC T + IK0 ))) x , (A.125) where we set N = 106. A good representation should make the MI I(X;R) as big as possible. Equivalently, for the same inputs, a good representation should make the conditional entropy (CDE) H ( X|R ) (or h1) as small as possible.",
      "exclude": false
    },
    {
      "heading": "A.5.2 COMPARISON OF BASIS VECTORS",
      "text": "We compared our algorithm with an up-to-date sparse coding algorithm, the mini-batch dictionary learning (MBDL) as given in (Mairal et al., 2009; 2010) and integrated in Python library, i.e. scikitlearn. The input data was the same as the above, i.e. 105 nature image patches preprocessed by the ZCA whitening filters. We denotes the optimal dictionary learned by MBDL as B RKK1 for which each column represents a basis vector. Now we have x U1/2UT By = By, (A.126) B = U1/2UT B, (A.127) where y = (y1, , yK1) T is the coefficient vector. Similarly, we can obtain a dictionary from the filter matrix C. Suppose rank (C) = K0 K1, then it follows from (A.64) that x = ( aCCT )1 Cy. (A.128) By (A.66) and (A.128), we get x By = aBCT1/20 UT0 x, (A.129) B = a1U0 1/2 0 ( CCT )1 C = [b1, ,bK1 ] , (A.130) where y = WTx = aCT1/20 U T 0 x, the vectors b1, ,bK1 can be regarded as the basis vectors and the strict equality holds when K0 = K1 = K. Recall that X = [x1, , xM ] = USV T (see Eq. A.49) and Y = [y1, , yM ] = WTX = a M 1CT VT0 , then we get X = BY = M 1U01/20 VT0 X. Hence, Eq. (A.129) holds. The basis vectors shown in Figure 4(a)4(e) correspond to filters in Figure 2(a)2(e). And Figure 4(f) illustrates the optimal dictionary B learned by MBDL, where we set the regularization parameter as = 1.2/ K, the batch size as 50 and the total number of iterations to perform as 20000, which took about 3 hours for training. From Figure 4 we see that these basis vectors obtained by the above algorithms have local Gabor-like shapes except for those by SRBM. If rank(B) = K = K1, then the matrix BT can be regarded as a filter matrix like matrix C (see Eq. A.69). However, from the column vector of matrix BT we cannot find any local Gabor-like filter that resembles the filters shown in Figure 2. Our algorithm has less computational cost and a much faster convergence rate than the sparse coding algorithm. Moreover, the sparse coding method involves a dynamic generative model that requires relaxation and is therefore unsuitable for fast inference, whereas the feedforward framework of our model is easy for inference because it only requires evaluating the nonlinear tuning functions.",
      "exclude": false
    },
    {
      "heading": "A.5.3 LEARNING OVERCOMPLETE BASES",
      "text": "We have trained our model on the Olshausens nature image patches with a highly overcomplete setup by optimizing the objective (A.118) by Alg.2 and got Gabor-like filters. The results of 400 typical filters chosen from 1024 output filters are displayed in Figure 5(a) and corresponding base (see Eq. A.130) are shown in Figure 5(b). Here the parameters are K1 = 1024, tmax = 100, v1 = 0.4, = 0.8, and = 0.98 (see A.52), from which we got rank (B) = K0 = 82. Compared to the ICA-like results in Figure 2(a)2(c), the average size of Gabor-like filters in Figure 5(a) is bigger, indicating that the small noise-like local structures in the images have been filtered out. We have also trained our model on 60,000 images of handwritten digits from MNIST dataset (LeCun et al., 1998) and the resultant 400 typical optimal filters and bases are shown in Figure 5(c) and Figure 5(d), respectively. All parameters were the same as Figure 5(a) and Figure 5(b): K1 = 1024, tmax = 100, v1 = 0.4, = 0.8 and = 0.98, from which we got rank (B) = K0 = 183. From these figures we can see that the salient features of the input images are reflected in these filters and bases. We could also get the similar overcomplete filters and bases by SRBM and MBDL. However, the results depended sensitively on the choice of parameters and the training took a long time. Figure 6 shows that CFE as a function of training time for Alg.2, where Figure 6(a) corresponds to Figure 5(a)-5(b) for learning nature image patches and Figure 6(b) corresponds to Figure 5(c)-5(d) for learning MNIST dataset. We set parameters tmax = 100 and = 0.8 for all experiments and varied parameter v1 for each experiment, with v1 = 0.2, 0.4, 0.6 or 0.8. These results indicate a fast convergence rate for training on different datasets. Generally, the convergence is insensitive to the change of parameter v1. We have also performed additional tests on other image datasets and got similar results, confirming the speed and robustness of our learning method. Compared with other methods, e.g., IICA, FICA, MBDL, SRBM or sparse autoencoders etc., our method appeared to be more efficient and robust for unsupervised learning of representations. We also found that complete and overovercomplete filters and bases learned by our methods had local Gabor-like shapes while the results by SRBM or MBDL did not have this property. A.5.4 IMAGE DENOISING Similar to the sparse coding method applied to image denoising (Elad & Aharon, 2006), our method (see Eq. A.130) can also be applied to image denoising, as shown by an example in Figure 7. The filters or bases were learned by using 77 image patches sampled from the left half of the image, and subsequently used to reconstruct the right half of the image which was distorted by Gaussian noise. A common practice for evaluating the results of image denoising is by looking at the difference between the reconstruction and the original image. If the reconstruction is perfect the difference should look like Gaussian noise. In Figure 7(c) and 7(d) a dictionary (100 bases) was learned by MBDL and orthogonal matching pursuit was used to estimate the sparse solution. 1 For our method (shown in Figure 7(b)), we first get the optimal filters parameter W, a low rank matrix (K0 < K), then from the distorted image patches xm (m = 1, ,M ) we get filter outputs ym = WTxm and the reconstruction xm = Bym (parameters: = 0.975 and K0 = K1 = 14). As can be seen from Figure 7, our method worked better than dictionary learning, although we only used 14 bases compared with 100 bases used by dictionary learning. Our method is also more efficient. We can get better optimal bases B by a generative model using our infomax approach (details not shown). 1Python source code is available at http://scikit-learn.org/stable/ downloads/plot image denoising.py",
      "exclude": false
    }
  ],
  "fewShot": [
    {
      "model": "claude-haiku-4-5",
      "decision": {
        "rejection": true,
        "confidence": 0.85,
        "primary_reason": "Limited novelty and narrow experimental scope. The hierarchical infomax framework is a relatively incremental extension of existing information-theoretic principles (Bell & Sejnowski's ICA). Experiments are restricted to small-scale image patches (1212) and MNIST; no comparison with modern deep learning methods or evaluation on realistic large-scale tasks. The theoretical contributions, while mathematically detailed, lack significant conceptual innovation. The paper reads as a specialized optimization technique rather than a fundamental advance in unsupervised learning."
      },
      "token": {
        "prompt_tokens": 15774,
        "completion_tokens": 145,
        "total_tokens": 15919
      },
      "time": "2026-02-08T22:05:11.475117+00:00"
    },
    {
      "model": "gpt-5.2",
      "decision": {
        "rejection": true,
        "confidence": 0.78,
        "primary_reason": "Despite a lengthy information-theoretic derivation, the core contribution appears to be a re-derivation/variant of classic Infomax ICA + whitening with a hierarchical initialization and asymptotic MI approximation; the empirical evaluation is narrow (mostly Gabor-filter recovery on Olshausen patches/MNIST), relies on dated/weak baselines and qualitative metrics, and does not convincingly demonstrate state-of-the-art impact, robustness, or scalability relative to modern self-supervised/representation-learning methods."
      },
      "token": {
        "prompt_tokens": 16518,
        "completion_tokens": 127,
        "total_tokens": 16645
      },
      "time": "2026-02-09T20:09:27.408154+00:00"
    }
  ],
  "test": false
}