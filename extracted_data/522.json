{
  "id": "522",
  "title": "Symmetry-Breaking Convergence Analysis of Certain Two-layered Neural Networks with ReLU nonlinearity",
  "abstract": "In this paper, we use dynamical system to analyze the nonlinear weight dynamics of two-layered bias-free networks in the form of $g(x; w) = \\sum_{j=1}^K \\sigma(w_j \\cdot x)$, where $\\sigma(\\cdot)$ is ReLU nonlinearity. We assume that the input $x$ follow Gaussian distribution. The network is trained using gradient descent to mimic the output of a teacher network of the same size with fixed parameters $w*$ using $l_2$ loss. We first show that when $K = 1$, the nonlinear dynamics can be written in close form, and converges to $w*$ with at least $(1-\\epsilon)/2$ probability, if random weight initializations of proper standard derivation ($\\sim 1/\\sqrt{d}$) is used, verifying empirical practice. For networks with many ReLU nodes ($K \\ge 2$), we apply our close form dynamics and prove that when the teacher parameters $\\{w*_j\\}_{j=1}^K$ forms orthonormal bases, (1) a symmetric weight initialization yields a convergence to a saddle point and (2) a certain symmetry-breaking weight initialization yields global convergence to $w*$ without local minima. To our knowledge, this is the first proof that shows global convergence in nonlinear neural network without unrealistic assumptions on the independence of ReLU activations. In addition, we also give a concise gradient update formulation for a multilayer ReLU network when it follows a teacher of the same size with $l_2$ loss. Simulations verify our theoretical analysis.",
  "accepted": false,
  "zeroShot": [
    {
      "model": "claude-haiku-4-5",
      "decision": {
        "rejection": false,
        "confidence": 0.72,
        "primary_reason": "Novel theoretical analysis of ReLU network convergence with symmetry-breaking initialization, though limited to restrictive assumptions (Gaussian inputs, orthonormal teacher weights, two-layer networks). Solid mathematical contributions but narrow scope limits impact."
      },
      "token": {
        "prompt_tokens": 9980,
        "completion_tokens": 77,
        "total_tokens": 10057
      },
      "time": "2026-02-05T22:55:19.479245+00:00"
    }
  ],
  "article": [
    {
      "heading": "",
      "text": "K j=1 (w T j x), where () is ReLU nonlinearity. We assume that the input x follow Gaussian distribution. The network is trained using gradient descent to mimic the output of a teacher network of the same size with fixed parameters w using l2 loss. We first show that when K = 1, the nonlinear dynamics can be written in close form, and converges to w with at least (1 )/2 probability, if random weight initializations of proper standard derivation ( 1/ d) is used, verifying empirical practice [Glorot & Bengio (2010); He et al. (2015); LeCun et al. (2012)]. For networks with many ReLU nodes (K 2), we apply our close form dynamics and prove that when the teacher parameters wjKj=1 forms orthonormal bases, (1) a symmetric weight initialization yields a convergence to a saddle point and (2) a certain symmetry-breaking weight initialization yields global convergence to w without local minima. To our knowledge, this is the first proof that shows global convergence in nonlinear neural network without unrealistic assumptions on the independence of ReLU activations. In addition, we also give a concise gradient update formulation for a multilayer ReLU network when it follows a teacher of the same size with l2 loss. Simulations verify our theoretical analysis.",
      "exclude": true
    },
    {
      "heading": "1 INTRODUCTION",
      "text": "Deep learning has made substantial progress in many applications, including Computer Vision [He et al. (2016); Simonyan & Zisserman (2015); Szegedy et al. (2015); Krizhevsky et al. (2012)], Natural Language Processing [Sutskever et al. (2014)] and Speech Recognition [Hinton et al. (2012)]. However, till now, how and why it works remains elusive due to a lack of theoretical understanding. First, how simple approaches like gradient descent can solve a very complicated non-convex optimization effectively. Second, how the deep models, especially deep convolutional models, achieve generalization power despite massive parameters. In this paper, we focus on the first problem and use dynamical system to analyze the nonlinear gradient descent dynamics of certain two-layered nonlinear network in the following form: g(x;w) = K j=1 (wTj x) (1) where (x) = max(x, 0) is the ReLU nonlinearity. We consider the following setting: a student network learns the parameters that minimize the l2 distance between its prediction and the supervision provided by the teacher network of the same size with a fixed set of parameters w. We assume all inputs x to follow Gaussian distribution and thus the network is bias-free. Eqn. 1 is highly nonconvex and could contain exponential number of symmetrically equivalent solutions. To analyze this, we first derive novel and concise gradient update rules for multilayer ReLU networks (See Lemma 2.1) in the teacher-student setting under l2 loss. Then for K = 1, we prove that the nonlinear gradient dynamics of Eqn. 1 has a close form and converges to w with at least (1 )/2 probability, if initialized randomly with standard derivation on the order of 1/ d, verifying commonly used initialization techniques [Glorot & Bengio (2010); He et al. (2015); LeCun et al. (2012)],. When K 2, we prove that when the teacher parameters wjKj=1 form orthonormal bases, (1) a symmetric initialization of a student network gets stuck at a saddle point and (2) under a certain symmetric breaking weight initialization, the dynamics converges to w, without getting stuck into any local minima. Note that in both cases, the initialization can be arbitrarily close to the origin for a fixed w, showing that such a convergence behavior is beyond the local convex structure at w. To our knowledge, this is the first proof of its kind. Previous works also use dynamical system to analyze deep neural networks. [Saxe et al. (2013)] analyzes the dynamics of multilayer linear network, and [Kawaguchi (2016)] shows every local minima is global for multilinear network. Very little theoretical work has been done to analyze the dynamics of nonlinear networks, especially deep ones. [Mei et al. (2016)] shows the global convergence whenK = 1 with activation function (x) when its derivatives , , are bounded and > 0. Similar to our approach, [Saad & Solla (1996)] also uses the student-teacher setting and analyzes the dynamics of student network when the teachers parameters w forms a orthonomal bases; however, it uses (x) = erf(x) as the nonlinearity and only analyzes the local behaviors of the two critical points (the saddle point in symmetric initializations, and w). In contrast, we prove the global convergence behavior in certain symmetry-breaking cases. Many previous works analyze nonlinear network based on the assumption of independent activations: the activations of ReLU (or other nonlinear) nodes are independent of the input and/or mutually independent. For example, [Choromanska et al. (2015a;b)] relate the nonlinear ReLU network with spin-glass models when several assumptions hold, including the assumption of independent activations (A1p and A5u). [Kawaguchi (2016)] proves that every local minimum in nonlinear network is global based on similar assumptions. [Soudry & Carmon (2016)] shows the global optimality of the local minimum in a two-layered ReLU network, by assuming small sample size and applying independent multiplicative Bernoulli noise on the activations. In practice, the activations are highly dependent due to their common input. Ignoring such dependency also misses important behaviors, and may lead to misleading conclusions. In this paper, no assumption of independent activation is made. For sigmoid activation, [Fukumizu & Amari (2000)] gives quite complicated conditions for a local minimum to be global when adding a new node to a two-layered network. [Janzamin et al. (2015)] gives guarantees on recovering the parameters of a 2-layered neural network learnt with tensor decomposition. In comparison, we analyze ReLU networks trained with gradient descent, which is a more popular setting in practice. The paper is organized as follows. Sec. 2 introduces the basic formulation and some interesting novel properties of ReLU in multilayered ReLU networks. Sec. 3 and Sec. 4 then analyze the twolayered model Eqn. 1 for K = 1 and K 2, respectively. Sec. 5 shows that simulation results are consistent with theoretical analysis. Finally Sec. 7 gives detailed proofs for all theorems.",
      "exclude": true
    },
    {
      "heading": "2 PRELIMINARY",
      "text": "",
      "exclude": false
    },
    {
      "heading": "2.1 NOTATION",
      "text": "Denote X as a N -by-d input data matrix and w is the parameter of the teacher network with desired N -by-1 output u = g(X;w). Now suppose we have an estimator w and the estimated output v = g(X;w). We want to know with l2 loss E(w) = 12u v 2 = 12u g(X;w) 2, whether gradient descent will converge to the desired solution w. The gradient descent update is w(t+1) = w(t) + w(t), where w(t) E(w(t)). If we let 0, then the update rule becomes a first-order differential equation dw/dt = E(w), or more concisely, w = E(w). In this case, E = E(w)Tw = E(w)2 0, i.e., the function value E is nonincreasing over time. The key is to check whether there exist other critical points w 6= w so that E(w) = 0. In our analysis, we assume entries of inputX follow Gaussian distribution. In this situation, the gradient is a random variable and w = E [E(w)]. The expected E [E(w)] is also nonincreasing no matter whether we follow the expected gradient or the gradient itself, because E [ E ] = E [E(w)TE(w)] E [E(w)]T E [E(w)] 0 (2) Therefore, we analyze the behavior of expected gradient E [E(w)] rather thanE(w).",
      "exclude": false
    },
    {
      "heading": "2.2 PROPERTIES OF RELU",
      "text": "In this paper, we discover a few useful properties of ReLU that make our analysis much simpler. Denote D = D(w) = diag(Xw > 0) as a N -by-N diagonal matrix. The l-th diagnonal element of D is a binary variable showing whether the neuron is on for sample l. Using this notation, we could write (Xw) = DXw. Note that D only depends on the direction of w but not its magnitude. Note that for ReLU,D is also tranparent on derivatives. For example, the Jacobian Jw[(Xw)] = (Xw)X = DX at differentiable regions. This gives a very concise rule for gradient descent in ReLU network: suppose we have negative gradient inflow vector g (of dimension N -by-1) on the current ReLU node with weights w, then we can simply write the update w as: w = Jw[(Xw)] Tg = XTDg (3) This can be easily applied to multilayer ReLU network. Denote j [c] if node j is in layer c, dc as the width of layer c, and uj and vj as the output of teacher network and student network, respectively. A simple deduction yields the following lemma: Lemma 2.1 For neural network with ReLU nonlinearity and using l2 loss to match with a teacher network of the same size, the negative gradient inflow gj for node j at layer c has the following form: gj = Lj j (Ljuj Ljvj) (4) where Lj and Lj are N -by-N diagonal matrices. For any k [c+ 1], Lk = j[c] wjkDjLj and similarly for Lk. For the first layer, L = L = I . The intuition here is to start from g = u v (true for l2 loss) at the top layer and use induction. With this formulation, we could write the finite dynamics for wc (all parameters in layer c). Denote the N -by-dc+1dc matrix Rc = [LjDj ]j[c]Xc and Rc = [L jD j ]j[c]X c . Using gradient descent rules: wj = X T cDjgj = X T cDjLj j LjD jX cw j j LjDjXcwj (5) = XTcDjLj (R cw c Rcwc) (6) Therefore we have: wc = R T c (R cw c Rcwc) (7)",
      "exclude": false
    },
    {
      "heading": "3 SINGLE RELU CASE",
      "text": "Lets start with the simplest case where there is only one ReLU node, K = 1. At iteration t, following Eqn. 3, the gradient update rule is: w(t) = XTD(t)g(t) = XTD(t)(DXw D(t)Xw(t)) (8) Note here how the notation ofD(t) comes into play (andD(t)D(t) = D(t)). Indeed, when the neuron is cut off at sample l, then (D(t))ll is zero and will block the corresponding gradient component. Linear case. In this situationD(t) = D = I (no gating in either forward or backward propagation) and: w(t+1) = w(t) + N XTX(w w(t)) (9) where /N is the learning rate. When it is sufficiently small so that the spectral radius (I NX TX) 0) is a binary diagonal matrix. If xi N(0, I) and are i.i.d (and thus bias-free), then: E [F (e,w)] = N 2 [( )w + w sin e] (10) where = (e,w) [0, ] is the angle between e and w. Note that the expectation analysis smooths out the non-differentiable property of ReLU, leaving only one singularity at e = 0. The intuition is that expectation analysis involves an integration over the data distribution. With simple algebraic manipulation, E [w] takes the following closed form: E [w] = N 2 (w w) + N 2 ( sin w w) (11) where = w/w and [0, ] is the angle between w and w. The first term is expected while the last two terms show the nonlinear behavior. Using Lyapunovs method, we show that the dynamics (if treated continuously) converges to w when w(1) = w : w w < w: Lemma 3.2 When w(1) = w : w w < w, following the dynamics of Eqn. 11, the Lyapunov function V (w) = 12w w 2 has V < 0 and the system is asymptotically stable and thus w(t) w when t +. See Appendix for the proof. The intuition is to represent V as a 2-by-2 bilinear form of vector [w, w], and the bilinear coefficient matrix is positive definite. One question arises: will the same approach show the dynamics converges when the initial conditions lie outside the region , in particular for any region that includes the origin? The answer is probably no. Note that w = 0 is a singularity in which w is not continuous (if approaching from different directions towards w = 0, w is different). It is due to the fact that ReLU function is not differentiable at the origin. We could remove this singularity by smoothing out ReLU around the origin. This will yield w 0 when w 0. In this case, V (0) = 0 so Lyapunov method could only tell that the dynamics is stable but not convergent. Note that for ReLU activation, (x) = 0 for certain negative x even after a local smoothing, so the global convergence claim in [Mei et al. (2016)] for l2 loss does not apply. Random Initialization. Then we study how to sample w(1) so that w(1) . We would like to sample within , but we dont know where is w. Sampling around origin with big radius r 2w is inefficient in particular in high-dimensional space. This is because when the sample is uniform, the probability of hitting the ball is proportional to (r/w)d 2d, which is exponentially small. A better idea is to sample around the origin with very small radius (but not at w = 0), so that the convergent hypersphere behaves like a hyperplane near the origin, and thus almost half of the samples is useful (Fig. 2(a)), as shown in the following theorem: Theorem 3.3 The dynamics in Eqn. 11 converges to w with probability at least (1 )/2, if the initial value w(1) is sampled uniformly from Br = w : w r with r 2 d+1w . The intution here is to lower-bound the probability of the shaded area (Fig. 2(b)). From the proof, the conclusion could be made stronger to show r 1/ d, consistent with common initialization techniques [Glorot & Bengio (2010); He et al. (2015); LeCun et al. (2012)]. Fig. 2(c) shows an example in the 2D case, in which there is a singularity at the origin, and sampling towards w yields the convergence. This is consistent with the analysis above.",
      "exclude": false
    },
    {
      "heading": "4 MULTIPLE RELUS CASE",
      "text": "Now we are ready to analyze the network g(x) = K j=1 (w T j x) for K 2 (Fig. 1(c)). Theoretical analysis of such networks is also the main topic in many previous works [Saad & Solla (1996); Soudry & Carmon (2016); Fukumizu & Amari (2000)]. In this case, Lj = Lj = I for 1 j K. Then we have the following nonlinear dynamics from Eqn. 7: wj = K j=1 f(wj ,wj ,w j) (12) where f = F (wj/wj,wj) F (wj/wj,wj). Therefore, using Eqn. 10, its expectation is: 2 N E [ f(wj ,wj ,w j) ] = ( j j )w j ( j j )wj + (wj wj sin j j wj wj sin j j ) wj (13) where j j (wj ,wj) and j j (wj ,wj). Eqn. 12 (and its expected version) gives very complicated nonlinear dynamics and could be hard to solve in general. Unlike K = 1, a similar approach with Lyaponov function does not yield a decisive conclusion. However, if we consider the symmetric case: wj = Pjw and wj = Pjw where Pj is a cyclic permutation matrix that maps index j + 1 to (j + j mod K) + 1 (and P1 is the identity matrix), then RHS of the expected version of Eqn. 12 can be simplified as follows: E [wj ] = j E [ f(wj ,wj ,w j) ] = j E [f(Pjw, Pjw, Pjw)] = j E [f(Pjw, PjPjw, PjPjw)] (PjKj=1 is a group) = Pj j E [f(w, Pjw, Pjw)] (Pw1 = w1, (Pw1, Pw2) = (w1,w2)) = PjE [w1] (14) which means that if all wj and wj are symmetric under the action of cyclic group, so does their expected gradient. Therefore, the trajectory w(t) keeps such cyclic structure. Instead of solving a system of K equations, we only need to solve one: E [w] = K j=1 E [f(w, Pjw, Pjw)] (15) Surprisingly, there is another layer of symmetry in Eqn. 15 when wj forms an orthonomal basis (wj Twj = jj ). In this case, if we start with w (1) = xw + y j 6=1 Pjw then we could show that the trajectory keeps this structure and Eqn. 15 can be further reduced into the following 2D nonlinear dynamics: 2 N E [ x y ] = [( )(x 1 + (K 1)y)] [ 1 1 ] + [ ] + [ x 1 y ] + [(K 1)( sin sin) + sin ] [ x y ] (16) Here the symmetrical factor ( wj/wj, j j , j j , j j ) are defined as follows: = (x2 + (K1)y2)1/2, cos = x, cos = y, cos = 2(2xy+ (K2)y2) (17) For this 2D dynamics, we thus have the following theorem: Theorem 4.1 For any K 2, the 2D dynamics (Eqn. 16) shows the following behaviors: (1) Symmetric case. If the initial condition x(1) = y(1) (0, 1], then the dynamics reduces to 1D and converges to a saddle point x = y = 1K ( K 1 arccos(1/ K) + ). (2) Symmetry-Breaking. If (x(1), y(1)) = x (0, 1], y [0, 1], x > y, then dynamics always converges to (x, y) = (1, 0). From (x(t), y(t)) we could recover w(t)j = x (t)wj + y (t) j 6=j w j . Obviously, a convergence of Eqn. 16 to (1, 0) means Eqn. 12 converges to wj, i.e, the teacher parameters are recovered: Corollary 4.2 For a bias-free two-layered ReLU network g(x;w) = j (w T j x) that takes Gaussian i.i.d inputs (Fig. 1), if the teachers parameters wj form orthogonal bases, then when the student parameters is initialized in the form of w(1)j = x (1)wj + y (1) j 6=j w j where (x(1), y(1)) = x (0, 1], y [0, 1], x > y, then the dynamics (Eqn. 12) converges to wj without being trapped into local minima. When symmetry is broken, since the closure of includes the origin, there exists a path starting at arbitrarily small neighborhood of origin to w, regardless of how large w is. In contrast to traditional convex analysis that only gives the local parameter-dependent convergence basin around wj , here we obtain a convergence basin that is parameter-independent. In comparison, [Saad & Solla (1996)] uses a different activation function ((x) = erf(x)) and only analyzes local behaviors near the two fixed points (the symmetric saddle point and the teachers weights w), leaving symmetry breaking an empirical procedure. Here we show that it is possible to give global convergence analysis on certain symmetry breaking cases for two-layered ReLU network. By symmetry, Corollary 4.1 immediately suggests that when w(1) = y(1) K j=1 w j + (x (1) y(1))wj , then the dynamics will converge to Pjw . Since x > y but can be arbitrarily close, a slighest preturbation on the symmetric solution x = y leads to a different fixed point, which is a permutation of w. This is very similar to Spontaneously Symmetric-Breaking (SSB) procedure in physics, in which a high energy state with full symmetry goes to a low energy state and only retains part of the symmetry. In this case, the energy is the objective function E, the high energy state is the initialization that is almost symmetrical but with small fluctuation, and the low energy state is the fixed point the dynamics converges into. From the simulation shown in Fig. 4, we could see that gradient descent takes a detour to reach the desired solution w, even when the initialization is aligned with w. This is because in the first stage, all ReLU nodes receive the residue and try to explain the data in the same way (both x and y increases); when the obvious component has been explained away, then the residue changes its direction and pushes some ReLU nodes to explain other components as well (x increases but y decreases). Empirically this path also converges to w under noise. We leave it a conjecture that the system converges in the presence of reasonably large noise. If this conjecture is true, then with high probability a random initialization stays in the convergence basin and converges to a permutation of w. The reason is that a random initialization almost never gives ties. Without a tie, there exists one leading component which will dominate the convergence. Conjecture 4.3 When the initialization w(1) = x(1)wj + y(1) j 6=j w j + , where is Gaussian noise and (x(1), y(1)) , then the dynamics Eqn. 12 also converges to w without trapped into local minima.",
      "exclude": false
    },
    {
      "heading": "5 SIMULATION",
      "text": "",
      "exclude": false
    },
    {
      "heading": "5.1 CLOSE FORM SOLUTION FOR ONE RELU NODE",
      "text": "We verify our close form expression of E [F (e,w)] = E [XTD(e)D(w)Xw] (Eqn. 10) with simulation. We randomly pick e and w so that their angle (e,w) is uniformly distributed in [0, ]. We prepare the input data X with standard Gaussian distribution and compare the close form solution E [F (e,w)] with F (e,w), the actual data term in gradient descent without expectation. We use relative RMS error: err = E [F (e,w)] F (e,w)/F (e,w). As shown in Fig. 3(a), The error distribution on angles shows the properties of the close-form solution. For small , D(w) and very large noise is present. Both teacher and student networks use g(x) = K j=1 (w T j x). Each experiment has 8 runs. Bottom row: Convergence when we use g2(x) = K j=1 aj(w T j x). Here the top weights aj is fixed at different numbers (rather than 1). Large positive aj correponds to fast convergence. When aj has positive/negative components, the network does not converge to w. D(e) overlaps sufficiently, giving a reliable estimation for the gradient. When , D(w) and D(e) tend not to overlap, leaving very few data involved in the gradient computation. As a result, the variance grows. Note that all our analysis operate on [0, /2] and is not affected by this behavior. In the following, angles are sampled from [0, /2]. Fig. 3(a) shows that the close form expression becomes more accurate with more samples. We also examine other zero-mean distributions of X , e.g., uniform distribution in [1/2, 1/2]. As shown in Fig. 3(d), the close form expression still works for large d, showing that it could be quite general. Note that the error is computed up to a scaling constant, due to the difference in normalization constants among different distributions. We leave it to the future work to prove its usability for broader distributions.",
      "exclude": false
    },
    {
      "heading": "5.2 CONVERGENCE FOR MULTIPLE RELU NODES",
      "text": "Fig. 4(a) and (b) shows the 2D vector field given by the 2D dynamics (Eqn. 16) and Fig. 4(c) shows the 2D trajectory towards convergence to the teachers parameters w. Interestingly, even when we initialize the weights as (103, 0), aligning with w, the gradient descent takes detours to reach the destination. One explanation is, at the beginning all nodes move similar direction trying to explain the data, once the data have been explained partly, specialization follows (y decreases). Fig. 5 shows empirical convergence for K 2, when the initialization deviates from symmetric initialization in Thm. 4.1. Unless the deviation is large, gradient descent converges to w. We also check the convergence of a more general network g2(x) = K j=1 aj(w T j x). When aj > 0 convergence follows; however, when some aj is negative, the network does not converge to w, even that the student network already knows the ground truth value of ajKj=1.",
      "exclude": false
    },
    {
      "heading": "6 CONCLUSION AND FUTURE WORK",
      "text": "In this paper, we analyze the nonlinear dynamical behavior of certain two-layered bias-free ReLU networks in the form of g(x;w) = K j=1 (w T j x), where = max(x, 0) is the ReLU node. We assume that the input x follows Gaussian distribution and the output is generated by a teacher network with parameters w. In K = 1 we show a close-form nonlinear dynamics can be obtained and its convergence to w can be proven, if we sample the initialization properly. Such initialization is consistent with common practice [Glorot & Bengio (2010); He et al. (2015)] and is independent of the value of w. ForK 2, when the teacher parameters wj form a orthonormal bases, we prove that the trajectory from symmetric initialization is trapped into a saddle point, while certain symmetric breaking initialization converges to w without trapped into any local minima. Future work includes analysis of general cases (or symmetric case plus noise) for K 2, and a generalization to multilayer ReLU (or other nonlinear) networks.",
      "exclude": true
    },
    {
      "heading": "7 APPENDIX",
      "text": "Here we list all detailed proof for all the theorems.",
      "exclude": true
    },
    {
      "heading": "7.1 PROPERTIES OF RELU NETWORKS",
      "text": "Lemma 7.1 For neural network with ReLU nonlinearity and using l2 loss to match with a teacher network of the same size, the negative gradient inflow gj for node j at layer c has the following form: gj = Lj j (Ljuj Ljvj) (18) where Lj and Lj are N -by-N diagonal matrices. For any k [c+ 1], Lk = j[c] wjkDjLj and similarly for Lk. Proof We prove by induction on layer. For the first layer, there is only one node with g = u v, therefore Lj = Lj = I . Suppose the condition holds for all node j [c]. Then for node k [c+1], we have: gk = j wjkDjgj = j wjkDjLj j Ljuj j Ljvj = j wjkDjLj j Lj k Djw jkuk j Lj k Djwjkvk = j wjkDjLj j LjD j k wjkuk j wjkDjLj j LjDj k wjkvk = k j wjkDjLj j LjD jw jk uk k j wjkDjLj j LjDjwjk vk Setting Lk = j wjkDjLj and L k = j w jkD jL j (both are diagonal matrices), we thus have: gk = k LkL kuk LkLkvk = Lk k Lkuk Lkvk (19)",
      "exclude": false
    },
    {
      "heading": "7.2 ONE RELU CASE",
      "text": "Lemma 7.2 Suppose F (e,w) = XTD(e)D(w)Xw where e is a unit vector and X = [x1,x2, ,xN ]T is N -by-d sample matrix. If xi N(0, I) and are i.i.d, then: E [F (e,w)] = N 2 (( )w + w sin e) (20) where [0, ] is the angle between e and w. Proof Note that F can be written in the following form: F (e,w) = i:xTi e0,x T i w0 xix T iw (21) where xi are samples so that X = [x1,x2, ,xn]T. We set up the axes related to e and w as in Fig. 6, while the rest of the axis are prependicular to the plane. In this coordinate system, any vector x = [r sin, r cos, x3, . . . , xd]. We have an orthonomal set of bases: e, e = ew/w cos sin (and any set of bases that span the rest of the space). Under the basis, the representation for e and w is [1,0d1] and [w cos ,w sin ,0d2]. Note that here (, ]. The angle is positive when e chases after w, and is otherwise negative. Now we consider the quality R(0) = E [ 1 N i:i[0,0] xix T i ] . If we take the expectation and use polar coordinate only in the first two dimensions, we have: R(0) = E 1 N i:i[0,0] xix T i = E [xixTi |i [0, 0]]P [i [0, 0]] = + 0 + 0 0 r sinr cos. . . xd [r sin r cos . . . xd] p(r)p() d k=3 p(xk)rdrddx3 . . . dxd where p(r) = er 2/2 and p() = 1/2. Note that R(0) is a d-by-d matrix. The first 2-by-2 block can be computed in close form (note that + 0 r2p(r)rdr = 2). Any off-diagonal element except for the first 2-by-2 block is zero due to symmetric property of i.i.d Gaussian variables. Any diagonal element outside the first 2-by-2 block will be P [i [0, 0]] = 0/2. Finally, we have: R(0) = E 1 N i:i[0,0] xix T i = 1 4 [ 20 sin 20 1 cos 20 0 1 cos 20 20 + sin 20 0 0 0 20Id2 ] (22) = 0 2 Id + 1 4 [ sin 20 1 cos 20 0 1 cos 20 sin 20 0 0 0 0 ] (23) With this equation, we could then compute E [F (e,w)]. When 0, the condition i : xTi e 0,xTiw 0 is equivalent to i : i [, ] (Fig. 6(a)). Using w = [w cos ,w sin ,0d2] and we have: E [F (e,w)] = N (R()R())w (24) = N 4 ( 2( )w w [ sin 2 1 cos 2 0 1 cos 2 sin 2 0 0 0 0 ][ cos sin 0 ]) (25) = N 2 ( ( )w + w [ sin 0 ]) (26) = N 2 (( )w + w sin e) (27) For 0, M22 > 0 and det(M) > 0. The first two are trivial, while the last one is: 4det(M) = 2(sin 2 + 2 2) [(2 ) cos + sin ]2 (31) = 2(sin 2 + 2 2) [ (2 )2 cos2 + (2 ) sin 2 + sin2 ] (32) = (42 1) sin2 4 + 4 cos2 2 cos2 + sin 2 (33) = (42 4 1) sin2 + cos (2 sin cos ) (34) Note that 42 4 1 = 4( ) 1 > 0 for [0, /2], and g() = sin cos 0 for [0, /2] since g(0) = 0 and g() 0 in this region. Therefore, when (0, /2], M is positive definite. When = 0, M() = [1,1;1, 1] and is semi-positive definite, with the null eigenvector being 2 2 [1, 1], i.e., w = w . However, along = 0, the only w that satisfies w = w is w = w. Therefore, V = yTMy 0, it is not straightforward to prove the convergence inH, since the trajectory might go outsideH. On the other hand, is the level set V 0, 0 < s < 1 (41) with s = 1/2 and x = d/2 we have:( d+ 1 2 )1/2 < (d/2 + 1/2) (d/2 + 1) < ( d 2 )1/2 (42) Therefore, it suffices to have r 2 d+ 1 w (43) Note that this upper bound is tight when 0 and d +, since all inequality involved asymptotically becomes equal.",
      "exclude": false
    },
    {
      "heading": "7.3 TWO LAYER CASE",
      "text": "Lemma 7.5 For , and defined in Eqn. 17: (x2 + (K 1)y2)1/2 (44) cos x (45) cos y (46) cos 2(2xy + (K 2)y2) (47) we have the following relations in the triangular region 0 = (x, y) : x 0, y 0, x y + 0 (Fig. 6(c)): (1) , [0, /2] and [0, 0) where 0 = arccos 1K . (2) cos = 1 2(x y)2 and sin = (x y) 2 2(x y)2. (3) (equality holds only when y = 0) and > . Proof Propositions (1) and (2) are computed by direct calculations. In particular, note that since cos = x = 1/ 1 + (K 1)(y/x)2 and x > y 0, we have cos (1/ K, 1] and [0, 0). For Preposition (3), = arccosy > = arccosx because x > y. Finally, for x > y > 0, we have cos cos = 2(2xy + (K 2)y2) y = (2x+ (K 2)y) > (x+ (K 1)y) > 1 (48) The final inequality is because K 2, x, y > 0 and thus (x + (K 1)y)2 > x2 + (K 1)2y2 > x2 + (K 1)y2 = 2. Therefore > . If y = 0 then = . Theorem 7.6 For the dynamics defined in Eqn. 16, there exists 0 > 0 so that the trianglar region 0 = (x, y) : x 0, y 0, x y + 0 (Fig. 6(c)) is a convergent region. That is, the flow goes inwards for all three edges and any trajectory starting in 0 stays. Proof We discuss the three boundaries as follows: Case 1: y = 0, 0 x 1, horizontal line. In this case, = 0, = /2 and = /2. The y component of the dynamics in this line is: f1 2 N y = 2 (x 1) 0 (49) So y points to the interior of . Case 2: x = 1, 0 y 1, vertical line. In this case, 1 and the x component of the dynamics is: f2 2 N x = ( )(K 1)y + (K 1)( sin sin) + sin (50) = (K 1) [( )y ( sin sin)] + ( sin ) (51) Note that since 1, sin sin , so the second term is non-positive. For the first term, we only need to check whether ( )y ( sin sin) is nonnegative. Note that ( )y ( sin sin) (52) = ( )y + (x y) 2 2(x y)2 1 2y2 (53) = y [ 2 2(x y)2 ] + [ x 2 2(x y)2 1 2y2 ] (54) In we have (x y)2 1, combined with 1, we have 1 2 2(x y)2 2 and 1 2y2 1. Since x = 1, the second term is nonnegative. For the first term, since 1, 2 2(x y)2 2 2 > 0 (55)",
      "exclude": false
    },
    {
      "heading": "So (π − φ)y − (α sinφ∗ − sinφ) ≥ 0 and ∆x ≤ 0, pointing inwards.",
      "text": "Case 3: x = y + , 0 y 1, diagonal line. We compute the inner product between (x,y) and (1,1), the inward normal of at the line. Using 2 sin for [0, /2] and = arccosy arccosx 0 when x y, we have: f3(y, ) 2 N [ x y ]T [ 1 1 ] = + [(K 1)( sin sin) + sin ] (56) (K 1) [ sin ( 1 + 2(K 1) ) sin ] = (K 1) [ 1 2y2 ( 1 + 2(K 1) ) 2 2 2 ] Note that for y > 0: y = 1 (x/y)2 + (K 1) = 1 (1 + /y)2 + (K 1) 1 K (57) For y = 0, y = 0 0 andC2 2(1+/2(K1)) > 0. With = 0 > 0 sufficiently small, f3 > 0. Lemma 7.7 (Reparametrization) Denote = x y > 0. The terms x, y and involved in the trigometric functions in Eqn. 16 has the following parameterization: [ y x ] = 1 K [ 2 + (K 1)2 K2 ] (58) where 2 = (K 2)/(K 1). The reverse transformation is given by = K (K 1)2 2. Here [1, K) and 2 (0, 1]. In particular, the critical point (x, y) = (1, 0) corresponds to (, ) = (1, 1). As a result, all trigometric functions in Eqn. 16 only depend on the single variable . In particular, the following relationship is useful: = cos + K 1 sin (59) Proof This transformation can be checked by simple algebraic manipulation. For example: 1 K ( 2) = 1 K ( K 2 (K 1) 2 ) = 1 K ( (Ky + )2 ) = y (60) To prove Eqn. 59, first we notice that K cos = Kx = + (K 1)2. Therefore, we have (K cos )2 (K 1)222 = 0, which gives 2 2 cos + 1 K sin2 = 0. Solving this quadratic equation and notice that 1, [0, /2] and we get: = cos + cos2 +K sin2 1 = cos + K 1 sin (61) Lemma 7.8 After reparametrization (Eqn. 58), f3(, ) 0 for (0, 2/]. Furthermore, the equality is true only if (, ) = (1, 1) or (y, ) = (0, 1). Proof Applying the parametrization (Eqn. 58) to Eqn. 56 and notice that = 2 = 2(), we could write f3 = h1() (+ (K 1) sin) (62) When is fixed, f3 now is a monotonously decreasing function with respect to > 0. Therefore, f3(, ) f3(, ) for 0 and (0, 1], we have the following: f31 = ( ) + (1 )( ) + 2 sin + 2 sin 2 ( sin ) (65) And it reduces to showing whether sin is nonnegative. Using Eqn. 59, we have: f33() = sin = 1 2 sin 2 + K 1 sin2 (66) Note that f 33 = cos 2 + K 1 sin 2 1 = K cos(2 0) 1, where 0 = arccos 1K . By Prepositions 1 in Lemma 7.5, [0, 0). Therefore, f 33 0 and since f33(0) = 0, f33 0. Again the equity holds when = 0, = and = 1, which is the critical point (, ) = (1, 1) or (y, ) = (0, 1). Theorem 7.9 For the dynamics defined in Eqn. 16, the only critical point (x = 0 and y = 0) within is (y, ) = (0, 1). Proof We prove by contradiction. Suppose (, ) is a critical point other than w. A necessary condition for this to hold is f3 = 0 (Eqn. 56). By Lemma 7.8, > = 2/ > 0 and 1 +Ky = 1 (2 + 2) = = 2/ > 2/ = 0 (67) So 1 +Ky is strictly greater than zero. On the other hand, the condition f3 = 0 implies that ((K 1)( sin sin) + sin ) = 1 ( ) + (68) Using [0, /2], and > , we have: 2 N y = ( )( 1 +Ky) ( ) y + ((K 1)( sin sin) + sin ) y = ( )( 1 +Ky) ( ) 1 ( )y < 0 (69) So the current point (, ) cannot be a critical point. Theorem 7.10 Any trajectory in 0 converges to (y, ) = (1, 0), following the dynamics defined in Eqn. 16. Proof We have Lyaponov function V = E [E] so that V = E [wTw] E [w]T E [w] 0. By Thm. 7.9, other than the optimal solution w, there is no other symmetric critical point, w 6= 0 and thus V < 0. On the other hand, by Thm. 7.6, the triangular region 0 is convergent, in which the 2D dynamics isC differentiable. Therefore, any 2D solution curve (t) will stay within. By PoincareBendixson theorem, when there is a unique critical point, the curve either converges to a limit circle or the critical point. However, limit cycle is not possible since V is strictly monotonous decreasing along the curve. Therefore, (t) will converge to the unique critical point, which is (y, ) = (1, 0) and so does the symmetric system (Eqn. 12). Theorem 7.11 When x = y (0, 1], the 2D dynamics (Eqn. 16) reduces to the following 1D case: 2 N x = K(x x) (70) where x = 1K ( K 1 arccos(1/ K) + ). Furthermore, x is a convergent critical point. Proof The 1D system can be computed with simple algebraic manipulations (note that when x = y, = 0 and = = arccos(1/ K)). Note that the 1D system is linear and its close form solution is x(t) = x0 + CeK/2Nt and thus convergent.",
      "exclude": false
    }
  ]
}