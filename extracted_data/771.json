{
  "id": "771",
  "title": "Fuzzy paraphrases in learning word representations with a lexicon",
  "abstract": "A synonym of a polysemous word is usually only the paraphrase of one sense among many. When lexicons are used to improve vector-space word representations, such paraphrases are unreliable and bring noise to the vector-space. The prior works use a coefficient to adjust the overall learning of the lexicons. They regard the paraphrases equally.  In this paper, we propose a novel approach that regards the paraphrases diversely to alleviate the adverse effects of polysemy. We annotate each paraphrase with a degree of reliability. The paraphrases are randomly eliminated according to the degrees when our model learns word representations. In this way, our approach drops the unreliable paraphrases, keeping more reliable paraphrases at the same time. The experimental results show that the proposed method improves the word vectors. Our approach is an attempt to address the polysemy problem keeping one vector per word. It makes the approach easier to use than the conventional methods that estimate multiple vectors for a word. Our approach also outperforms the prior works in the experiments.",
  "accepted": false,
  "zeroShot": [
    {
      "model": "claude-haiku-4-5",
      "decision": {
        "rejection": true,
        "confidence": 0.85,
        "primary_reason": "Limited novelty and methodological concerns. The core contribution—using fuzzy membership scores from PPDB to weight paraphrase inclusion via Bernoulli dropout—is incremental. The membership estimation (Eq. 4) simply normalizes existing PPDB scores without learning. Experimental validation is weak: (1) comparison with prior works uses only reported numbers rather than reproduced results, (2) improvements over baselines are often within reported margins of error, (3) no statistical significance testing, (4) limited analysis of why the method works. The paper reads more as an engineering contribution to word2vec rather than a fundamental advance suitable for top-tier ML venues."
      },
      "token": {
        "prompt_tokens": 4824,
        "completion_tokens": 177,
        "total_tokens": 5001
      },
      "time": "2026-02-05T23:18:21.822269+00:00"
    },
    {
      "model": "gpt-5.2",
      "decision": {
        "rejection": true,
        "confidence": 0.83,
        "primary_reason": "Insufficient methodological/experimental rigor: the approach is a minor stochastic reweighting of PPDB paraphrase constraints, and the evaluation lacks reproducible, fair baselines (cannot run prior methods), ablations, and statistically solid comparisons beyond small intrinsic benchmarks."
      },
      "token": {
        "prompt_tokens": 4234,
        "completion_tokens": 70,
        "total_tokens": 4304
      },
      "time": "2026-02-09T21:47:08.868200+00:00"
    }
  ],
  "article": [
    {
      "heading": "1 INTRODUCTION",
      "text": "Vector-space representations of words are reported useful and improve the performance of the machine learning algorithms for many natural language processing tasks such as name entity recognition and chunking (Turian et al., 2010), text classification (Socher et al., 2012; Le & Mikolov, 2014; Kim, 2014; Joulin et al., 2016), topic extraction (Das et al., 2015; Li et al., 2016), and machine translation (Zaremba et al., 2014; Sutskever et al., 2014). People are still trying to improve the vector-space representations for words. Bojanowski et al. (2016) attempt to improve word vectors by involving character level information. Other works (Yu & Dredze, 2014; Xu et al., 2014; Faruqui et al., 2015; Bollegala et al., 2016) try to estimate better word vectors by using a lexicon or ontology. The idea is simple: because a lexicon or ontology contains well-defined relations about words, we can use them to improve word vectors. However, for a polysemous word, one of its synonym does not always mean the same thing with the original one under different contexts. For example, the word point equals score in Team A got 3 points, but does not in my point of view. A method to address this issue is to estimate a vector for each word sense (Huang et al., 2012; Chen et al., 2014) or per word type (Neelakantan et al., 2014). However, it requires additional word sense disambiguation or part-of-speech tagging to use such word vectors. In this paper, we propose a method to improve the vector-space representations using a lexicon and alleviate the adverse effect of polysemy, keeping one vector per word. We estimate the degree of reliability for each paraphrase in the lexicon and eliminate the ones with lower degrees in learning. The experimental results show that the proposed method is effective and outperforms the prior works. The major contributions of our work include: We propose a novel approach involving fuzzy sets to reduce the noise brought by polysemous words in the word vector space when a lexicon is used for learning, and a model to use the fuzzy paraphrase sets to learn the word vector space. Although some prior works propose to solve the polysemy problem by estimating one vector per word sense or type, using such word vectors requires additional pre-process. Our proposed method keeps one vector per word. It makes the word vectors easier to use in practical terms: it is neither necessary to disambiguate the word senses nor to tag the part-of-speeches before we use the word vectors. We give an introduction of our proposed method in section 2. We show the effects of different paraphrase sets, parameters, corpus size, and evaluate the effectiveness of our approach by comparing to simpler algorithms in section 3. We compare our approach with the prior works via an evaluation experiment in section 4. We give the findings, conclusions and outlook in section 5.",
      "exclude": true
    },
    {
      "heading": "2 THE PROPOSED METHOD",
      "text": "",
      "exclude": false
    },
    {
      "heading": "2.1 FUZZY PARAPHRASES",
      "text": "As described in section 1, whether a polysemous words paraphrase is the same as the original depends on the context. Henceforth, if we simply use all the paraphrases of a word in the lexicon to improve the word vector without discrimination, they may sometimes bring noise to the vector-space. A conventional method for them is to give each word sense a vector. However, such vector-spaces require additional word sense disambiguation in practical use. Here, we propose a method to alleviate the adverse effects of polysemous words paraphrases without word sense disambiguation. Our idea is to annotate each paraphrase with a degree about its reliability, like a member of a fuzzy set. We call such paraphrases as fuzzy paraphrases, and their degrees as the memberships.",
      "exclude": false
    },
    {
      "heading": "2.2 LEARNING WITH FUZZY PARAPHRASES",
      "text": "We also propose a novel method to jointly learn corpus with a lexicon, in order to use fuzzy paraphrases to improve the word vectors. If the meanings of two words are totally the same, they can replace each other in a text without changing the semantic features. Henceforth, we can learn the lexicon by replacing the words in the corpus with its lexical paraphrases. We learn the word vectors by maximizing the probability of a word for a given context, and also for a generated context where words are replaced by their paraphrases randomly. The memberships of the fuzzy paraphrases are used here to control the probability that the replacements occur by a control function as shown in Figure 1. For a text corpus T , denote wi the ith word in T , c the context window, wj a word in the context window, Lwj the paraphrase set of wj in the lexicon L, wk the kth fuzzy paraphrase in Lwj , and xjk the membership of wk for wj , the objective is T wiT (ic)j(i+c) log p(wi|wj) + Lwj wkLwj f(xjk) log p(wi|wk) . (1) The function f(xjk) of the membership xjk is a specified drop-out function. It returns 0 more for the paraphrases that have lower memberships, and 1 more for the others. 2.3 MEMBERSHIP ESTIMATION & CONTROL FUNCTION f(x) Looking for a control function that is easy to train, we notice that if two words are more often to be translated to the same word in another language, the replacement of them are less likely to change the meaning of the original sentence. Thus, we use a function of the bilingual similarity (denoted as Sjk) as the membership function: xjk = g(Sjk). (2) There have been works about calculating the similarity of words using such bilingual information. A lexicon called the paraphrase database (PPDB) provides scores of the similarity of paraphrases on the basis of bilingual features (Ganitkevitch et al., 2013; Pavlick et al., 2015b;a). We scale the similarity score of the paraphrase wk to [0, 1] in PPDB2.0 as the memberships, and draw the values of f(xjk) from a Bernoulli distribution subjected to them. Denote Sjk the similarity score of word wj and wk in PPDB2.0, the value of f(xjk) is drawn from the Bernoulli distribution: f(xjk) Bernoulli(xjk), (3) xjk = Sjk max jT,kL Sjk . (4)",
      "exclude": false
    },
    {
      "heading": "2.4 TRAINING",
      "text": "We do not need to train f(xjk) using the method described above. The model can be trained by negative sampling (Mikolov et al., 2013b): For word wO and a word wI in its context, denote AI as the set of the paraphrases for wI accepted by f(xjk), we maximize log p(wO|wI) by distinguishing the noise words from a noise distribution Pn(w) from wO and its accepted paraphrases in AI by logistic regression: log p(wO|wI) = log (vwOTvwI ) + n i=1 Ewi Pn(w)[log (vwiTvwI )], wi 6= wO, wi / AI (5) Here, vwO T and vwi T stand for the transposed matrices of vwO and vwi , respectively. n is the number of negative samples used. (x) is a sigmoid function, (x) = 1/(1 + ex).",
      "exclude": false
    },
    {
      "heading": "3 MODEL EXPLORATION",
      "text": "",
      "exclude": false
    },
    {
      "heading": "3.1 CORPUS FOR EXPERIMENTS",
      "text": "We use enwiki91 mainly for tuning and model exploration. It has a balanced size(1 GB), containing 123,353,508 tokens. It provides enough data to alleviate randomness while it does not take too much time for our model to learn. 1http://mattmahoney.net/dc/enwiki9.zip We use ukWaC (Baroni et al., 2009) to compare with the prior works in section 4. But we do not use it for model exploration, because it takes more than 20 hours to learn it, as an enormous corpus containing 12 GB text.",
      "exclude": false
    },
    {
      "heading": "3.2 BENCHMARKS",
      "text": "We used several benchmarks. They are Wordsim-353 (WS353) (Finkelstein et al., 2001) (353 word pairs), SimLex-999 (SimLex) (Hill et al., 2016) (999 word pairs), the Stanford Rare Word Similarity Dataset (RW) (Luong et al., 2013) (2034 word pairs), the MEN Dataset (MEN) (Bruni et al., 2014) (3000 word pairs), and the Mikolovs (Googles) word analogical reasoning task (Mikolov et al., 2013a). WS353, SimLex, and RW are gold standards. They provide the similarity of words labeled by humans. We report the Spearmans rank correlation () for them. Mikolovs word analogical reasoning task is another widely used benchmark for word vectors. It contains a semantic part (SEM), and a syntactic part (SYN). We use the basic way suggested in their paper to find the answer for it: to guess word b related to b in the way how a is related to a, the word closest in cosine similarity to a a+ b is returned as b. We find that the benchmark scores change every time we learn the corpus, even under the same settings. It is because that the models involve random numbers. Therefore we should consider the margin of error of the changes when we use the benchmarks. To test the margin of error, we firstly used our proposed method to repeat learning enwiki9 for 10 times under the same parameters. Then we tested the vectors under each benchmark, to find the margin of error. In each test, we used the same parameters: the vector dimension was set to 100 for speed, the window size was set to 8, and 25 negative samples were used. The results are shown in Table 1. We use them to analyze the other experimental results later.",
      "exclude": false
    },
    {
      "heading": "3.3 DIFFERENT TYPES OF PARAPHRASES",
      "text": "In PPDB2.0, there are six relationships for paraphrases. For word X and Y , the different relationships between them defined in PPDB2.0 are shown in Table 2. We do not consider the exclusion and independent relations because they are not semantic paraphrases. Those of equivalence are the most reliable because they are the closest ones. But we still want to know whether it is better to take the entailment and the other related paraphrases into consideration. We learn enwiki9 with different paraphrase sets and use SimLex to evaluate the trained vectors. Figure 2 compares the performance using different paraphrase sets, tested by SimLex. We can see that it is best to use the equivalence and entailment (forward + reverse) paraphrases together or use only the equivalence paraphrases. Only using the entailment paraphrases is weak. Involving the other related paraphrases deteriorates the performance. We use the Equivalence and Entailment paraphrases in the experiments according to these results.",
      "exclude": false
    },
    {
      "heading": "3.4 EFFECTS OF PARAMETERS",
      "text": "We use our proposed method to learn enwiki9 under different parameter settings to evaluate the effects of parameters. We firstly learn enwiki9 under different parameter settings and then test the vectors using SimLex, WS353, RW, MEN, SEM and SYN. We report Spearmans rank correlation for SimLex, WS353, RW and MEN, the percentage of correct answers for SEM and SYN.",
      "exclude": false
    },
    {
      "heading": "3.4.1 EFFECTS OF VECTOR SPACE DIMENSION",
      "text": "We compare the benchmarks using different vector-space dimensions. Figure 3 shows the change of each benchmarks scores under different dimensions. We find that: The larger vectors do not bring the better performance for most of the benchmarks (except SimLex), although some previous works suggest that the higher dimensions brings better performance for their methods (Pennington et al., 2014; Levy & Goldberg, 2014b). The curves of SimLex and SYN are gradual. However, there are several abrupt changes in the others. And those of WS353 and RW do not change gradually. The best dimension for different benchmarks is not consistent. The differences in the content of the benchmarks may cause the inconsistence. For example, SimLex rates related but dissimilar words lower than the other word similarity benchmarks (Hill et al., 2016; Chiu et al., 2016). The results suggest that the best dimensions for our method depends on the task.",
      "exclude": false
    },
    {
      "heading": "3.4.2 EFFECTS OF CONTEXT WINDOW SIZE",
      "text": "We compared the benchmarks using different context window sizes. They are shown in Figure 4. Previous works argue that larger window sizes introduce more topic words, and smaller ones emphasize word functions (Turney, 2012; Levy & Goldberg, 2014a; Levy et al., 2015; Hill et al., 2016; Chiu et al., 2016). Different context window sizes provide different balances between relatedness and similarity. The best window size depends on what we want the vectors to be. We also see that in our results. The relationship between the window size and performance depends on how they rate the pairs. For example, WS353 rates word pairs according to association rather than similarity (Finkelstein et al., 2001; Hill et al., 2016). As larger window capture relatedness rather than similarity, the results show that the larger the window is, the better for WS353. The MEN dataset also prefer relatedness than similarity (Bruni et al., 2014), but they gave annotators examples involving similarity2. It may be the reason that the windows larger than 8 deteriorate the benchmarks based on MEN (Figure 4d). The standards of WS353 and MEN to rate the words are similar (Bruni et al., 2014). It leads to their similar curves (Figure 4b and 4d). The worst window sizes of them are also close. When the window size is set to about 2 or 3, respectively, the balance of similarity and relatedness is the worst for them. Unlike the other word similarity dataset, SimLex rates synonyms high and related dissimilar word pairs low. Therefore, the smallest window is the most suitable for SimLex because it is best for capturing the functional similarity. The results of RW differs from the others (Figure 4c). There are many abrupt changes. The best window size is 10, but 1 is better than 2-9. The dataset contains rare words. Because of their low frequencies, usage of broad context window may be better to draw features for them. However, additional words introduced by larger windows may also deteriorate the vectors of unusual words. For such tasks requiring rare word vectors of high quality, we should be careful in tuning the context window size. For Googles word analogical tasks (SEM and SYN), the questions are quite related to the topic or domain. For examples, there are questions about the capitals of the countries. They are associated but not synonymous. Therefore a larger window is usually better. However for SYN, using window size 9 is a little better than 10 in Figure 4d and for MEN 8 is best in Figure 4f. It may be because that if the window is too large, it introduces too many words and reduces the sparsity (Chiu et al., 2016). We can consider that the best context window size depends on the task, but we should avoid using too large window.",
      "exclude": false
    },
    {
      "heading": "3.4.3 EFFECTS OF NEGATIVE SAMPLES",
      "text": "We also explored the effects of the number of negative samples. The results are shown in Figure 5. 2According to their homepage: http://clic.cimec.unitn.it/ elia.bruni/MEN.html. In Figures 5a, 5c and 5f, we see that overfitting occurs when we use more than 15 negative samples. In Figure 5b and Figure 5e, it occurs from 25 and 20, respectively. In Figure 5d, the performance does not change very much when we use more than 30 negative samples. The results indicate that too many negative samples may cause overfitting. For 3 of the 6 benchmarks, it is best to use 15 negative samples. But we should be careful in practice use because the other different results suggest that the best number depends on the task. The abrupt change at around 15 in Figure 5b is interesting. WS353 is the smallest dataset among those we used. Because of the small size, the effects of randomness may cause such singularities when the vector-space is not well trained.",
      "exclude": false
    },
    {
      "heading": "3.5 EFFECTS OF THE CONTROL FUNCTION & THE CORPUS SIZE",
      "text": "In this section, we evaluate the effectiveness of our fuzzy approach, by comparing to the situations that set f(x) in Equation (1) as: f(x) = 1: It makes the model regard all paraphrases equally. They are all used without drop-out. f(x) = 0: It makes the model use no paraphrases, equivalent to CBOW. It is also a good way to show the effects of corpus size by comparing the proposed method to the situations above using corpora in varying size. Therefore we discuss them together in this section. We use text83 together with eEnwiki9 and ukWaC described in section 3.1. It is a small corpus containing 100 MB text. To show the difference, we report the benchmarks scores including not only SimLex, but also MEN, and the word analogical task (SEM and SYN). They are the other benchmarks that are shown relatively solid in section 3.2. The vector-space dimension is set to 300. The context window size is set to 8. 25 negative samples are used in learning. The results are shown in Figure 6. 3http://mattmahoney.net/dc/text8.zip We can see that: The proposed function outperforms the others for SimLex and MEN under text8, for all the benchmarks under enwiki9, for SimLex, SEM and SYN under ukWaC. The proposed function is always better than f(x) = 1 in the experiments, no matter what the benchmark is or how big the corpus is. For SEM, the proposed function is weaker than f(x) = 0 under text8, slightly better under enwiki9, and obviously outperforms f(x) = 0 under ukWaC. As the proposed function outperforms under larger corpora, the relatively low scores under text8 may be caused by the effects of randomness: the proposed function involves random numbers; they bring huge instability under such tiny corpora. Another possible reason is that the control function is less useful for text8 because there are few polysemous words in the tiny corpus. There is no advantages to use f(x) = 1 instead of f(x) = 0 for both text8 and enwiki9. It shows that learning the context words replaced by paraphrases may be not a good idea without fuzzy approaches. However, if we use the proposed control function, the results are better and go beyond those of f(x) = 0 in most tests. It shows that the control function utilizing fuzzy paraphrases improves the performance. Therefore, we can see that the proposed control function using the fuzzy paraphrases annotated with the degrees of reliability improves the quality of the learned word vector-space.",
      "exclude": false
    },
    {
      "heading": "4 COMPARISON WITH THE PRIOR WORKS",
      "text": "We compared our work to the prior works using a lexicon to improve word vectors. However, we failed to use the public code to reproduce the works of Yu & Dredze (2014) and Bollegala et al. (2016). We also failed to find an available implementation of Xu et al. (2014). Hence, we use the same corpus and benchmarks with Bollegala et al. (2016) and compare our results with the reported scores of the prior works in their paper. The benchmarks are: The MEN Dataset (MEN); Word Analogical Reasoning Task (SEM and SYN). Rubenstein-Goodenough dataset (RG) (Rubenstein & Goodenough, 1965) is also used in their works. However, we do not use it, because it fails the sanity check in Batchkarov et al. (2016): may increase when noise is added. We use ukWaC to learn the word vectors, the same with Bollegala et al. (2016). We also use the same parameters with the prior works: The vector-space dimension is set to 300; the context window size is set to 8; the number of negative samples is set to 25. Then we calculate the cosine similarity of the words and report 100 for Men. We use the add method described in section 3.2 and report the percentage of correct answers, for the word analogical reasoning task. Table 3 shows the results of the experiments. The of MEN and SEM is 0.86 and 0.44 as shown in Table 1. Therefore we see that our proposed method outperforms the prior works under these benchmarks. We consider our score for SYN is as good as Bollegala et al. (2016) achieved, and better than the others, because its margin of error is 1.79 as shown in Table 1.",
      "exclude": false
    },
    {
      "heading": "5 CONCLUSION & THE FUTURE WORKS",
      "text": "We proposed a fuzzy approach to control the contamination caused by the polysemous words when a lexicon is used to improve the vector-space word representations. We annotate each paraphrase of a word with a degree of reliability, like the members of a fuzzy set with their memberships, on the basis of their multilingual similarities to the original ones. We use the fuzzy paraphrases to learn a corpus by jointly learning a generated text, in which the original words are randomly replaced by their paraphrases. A paraphrase is less likely to be put into the generated text if it has lower reliability than the others, and vice versa. We tested the performance using different types of paraphrases in the lexicon PPDB2.0 and find that it is best to use the equivalence type and the entailment type. Using other related paraphrases deteriorates the performance. We explored the effects of parameters. We find that the best parameter setting depends on the task. We should tune the model carefully in practical use. We evaluated the effectiveness of our approach by comparing it to the situations that simpler functions are used to control replacements: f(x) = 1 which accepts all, and f(x) = 0 which rejects all. We also repeated the experiments under a tiny, a medium sized, and a large corpus, to see the effects of the corpus size on the effectiveness. Our approach achieves the best in 3 of 4 benchmarks under the tiny corpus, and in all benchmarks under the medium sized and the large one. The results indicate that our approach is effective to improve the word vectors. Our proposed method also achieved the top scores, compared with the prior works. Unlike the previous works that solve the problems about polysemy by estimating a vector for each word sense or word type, our approach keeps one vector per word. It makes the word vectors easier to use in practical terms: it is neither necessary to disambiguate the word senses nor to tag the part-of-speeches before we use the word vectors. The fuzzy paraphrases can also be employed for the other models with some changes. We are going to show it in the future. The proposed idea for the polysemy problem without word sense disambiguation is meaningful especially for practical use because it saves the effort of part-of-speech tagging and word sense disambiguation. Besides, the control function may be more accurate if it considers all the context. We are also going to work on it in the future. We have opened the source of a demo of the proposed method online4.",
      "exclude": true
    }
  ],
  "fewShot": [
    {
      "model": "gpt-5.2",
      "decision": {
        "rejection": true,
        "confidence": 0.78,
        "primary_reason": "Incremental idea (stochastic dropout of lexicon paraphrases weighted by PPDB scores) with limited novelty and weak empirical positioning: comparisons rely on unreproduced prior results, evaluation is mostly on standard intrinsic word-similarity/analogy benchmarks with modest analysis, and the method is essentially a heuristic reweighting/subsampling of lexicon constraints without strong ablations against simpler baselines (e.g., deterministic weighting, thresholding, or existing retrofitting/regularization approaches) or convincing evidence of broad impact."
      },
      "token": {
        "prompt_tokens": 6868,
        "completion_tokens": 131,
        "total_tokens": 6999
      },
      "time": "2026-02-09T20:52:32.436160+00:00"
    },
    {
      "model": "claude-haiku-4-5",
      "decision": {
        "rejection": true,
        "confidence": 0.85,
        "primary_reason": "The paper addresses a reasonable problem (polysemy noise in lexicon-based word embeddings) with a simple solution (fuzzy membership weighting via Bernoulli dropout), but the contribution is incremental and insufficiently validated. The core idea—using PPDB similarity scores to weight paraphrase inclusion—is straightforward and not novel. Experimental evaluation is limited: comparisons with prior work use only reported numbers (authors could not reproduce baselines), evaluation is restricted to small corpora and standard benchmarks with high variance, and improvements are modest and often within reported margins of error. The paper lacks deeper analysis of when/why the method helps, theoretical justification, or compelling empirical evidence that fuzzy weighting substantially outperforms simpler alternatives. The writing and presentation are also below conference standards."
      },
      "token": {
        "prompt_tokens": 7720,
        "completion_tokens": 203,
        "total_tokens": 7923
      },
      "time": "2026-02-11T19:29:07.770050+00:00"
    }
  ],
  "test": false
}