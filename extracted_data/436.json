{
  "id": "436",
  "title": "Learning a Natural Language Interface with Neural Programmer",
  "abstract": "Learning a natural language interface for database tables is a challenging task that involves deep language understanding and multi-step reasoning. The task is often approached by mapping natural language queries to logical forms or programs that provide the desired response when executed on the database. To our knowledge, this paper presents the first weakly supervised, end-to-end neural network model to induce such programs on a real-world dataset. We enhance the objective function of Neural Programmer, a neural network with built-in discrete operations, and apply it on WikiTableQuestions, a natural language question-answering dataset. The model is trained end-to-end with weak supervision of question-answer pairs, and does not require domain-specific grammars, rules, or annotations that are key elements in previous approaches to program induction. The main experimental result in this paper is that a single Neural Programmer model achieves 34.2% accuracy using only 10,000 examples with weak supervision. An ensemble of 15 models, with a trivial combination technique, achieves 37.7% accuracy, which is competitive to the current state-of-the-art accuracy of 37.1% obtained by a traditional natural language semantic parser.",
  "accepted": true,
  "zeroShot": [
    {
      "model": "claude-haiku-4-5",
      "decision": {
        "rejection": false,
        "confidence": 0.78,
        "primary_reason": "Novel weakly-supervised neural approach to semantic parsing with competitive results on real-world dataset, though limited by small dataset size and modest absolute performance gains over baselines"
      },
      "token": {
        "prompt_tokens": 4473,
        "completion_tokens": 61,
        "total_tokens": 4534
      },
      "time": "2026-02-05T22:46:09.109261+00:00"
    },
    {
      "model": "gpt-5.2",
      "decision": {
        "rejection": true,
        "confidence": 0.72,
        "primary_reason": "Incremental/dated contribution with limited methodological novelty and insufficiently convincing experimental evidence (heavy heuristics, filtering, and reliance on large ensembles) for a current top-tier ML conference."
      },
      "token": {
        "prompt_tokens": 4103,
        "completion_tokens": 55,
        "total_tokens": 4158
      },
      "time": "2026-02-09T21:17:57.756982+00:00"
    }
  ],
  "article": [
    {
      "heading": "1 BACKGROUND AND INTRODUCTION",
      "text": "Databases are a pervasive way to store and access knowledge. However, it is not straightforward for users to interact with databases since it often requires programming skills and knowledge about database schemas. Overcoming this difficulty by allowing users to communicate with databases via natural language is an active research area. The common approach to this task is by semantic parsing, which is the process of mapping natural language to symbolic representations of meaning. In this context, semantic parsing yields logical forms or programs that provide the desired response when executed on the databases (Zelle & Mooney, 1996). Semantic parsing is a challenging problem that involves deep language understanding and reasoning with discrete operations such as counting and row selection (Liang, 2016). The first learning methods for semantic parsing require expensive annotation of question-program pairs (Zelle & Mooney, 1996; Zettlemoyer & Collins, 2005). This annotation process is no longer necessary in the current state-of-the-art semantic parsers that are trained using only question-answer pairs (Liang et al., 2011; Kwiatkowski et al., 2013; Krishnamurthy & Kollar, 2013; Pasupat & Liang, 2015). However, the performance of these methods still heavily depends on domain-specific grammar or pruning strategies to ease program search. For example, in a recent work on building semantic parsers for various domains, the authors hand-engineer a separate grammar for each domain (Wang et al., 2015). Recently, many neural network models have been developed for program induction (Andreas et al., 2016; Jia & Liang, 2016; Reed & Freitas, 2016; Zaremba et al., 2016; Yin et al., 2015), despite Work done at Google Brain. the notorious difficulty of handling discrete operations in neural networks (Joulin & Mikolov, 2015; Kaiser & Sutskever, 2016). Most of these approaches rely on complete programs as supervision (Jia & Liang, 2016; Reed & Freitas, 2016) while others (Zaremba et al., 2016; Yin et al., 2015) have been tried only on synthetic tasks. The work that is most similar to ours is that of Andreas et al. (2016) on the dynamic neural module network. However, in their method, the neural network is employed only to search over a small set of candidate layouts provided by the syntactic parse of the question, and is trained using the REINFORCE algorithm (Williams, 1992). Hence, their method cannot recover from parser errors, and it is not trivial to adapt the parser to the task at hand. Additionally, all their modules or operations are parametrized by a neural network, so it is difficult to apply their method on tasks that require discrete arithmetic operations. Finally, their experiments concern a simpler dataset that requires fewer operations, and therefore a smaller search space, than WikiTableQuestions which we consider in our work. We discuss other related work in Section 4. Neural Programmer (Neelakantan et al., 2016) is a neural network augmented with a set of discrete operations. It produces both a program, made up of those operations, and the result of running the program against a given table. The operations make use of three variables: row selector, scalar answer, and lookup answer, which are updated at every timestep. lookup answer and scalar answer store answers while row selector is used to propagate information across time steps. As input, a model receives a question along with a table (Figure 1). The model runs for a fixed number of time steps, selecting an operation and a column from the table as the argument to the operation at each time step. During training, soft selection (Bahdanau et al., 2014) is performed so that the model can be trained end-to-end using backpropagation. This approach allows Neural Programmer to explore the search space with better sample complexity than hard selection with the REINFORCE algorithm (Williams, 1992) would provide. All the parameters of the model are learned from a weak supervision signal that consists of only the final answer; the underlying program, which consists of a sequence of operations and of selected columns, is latent. In this work, we develop an approach to semantic parsing based on Neural Programmer. We show how to learn a natural language interface for answering questions using database tables, thus integrating differentiable operations that are typical of neural networks with the declarative knowledge contained in the tables and with discrete operations on tables and entries. For this purpose, we make several improvements and adjustments to Neural Programmer, in particular adapting its objective function to make it more broadly applicable. In earlier work, Neural Programmer is applied only on a synthetic dataset. In that dataset, when the expected answer is an entry in the given table, its position is explicitly marked in the table. However, real-world datasets certainly do not include those markers, and lead to many ambiguities (e.g., (Pasupat & Liang, 2015)). In particular, when the answer is a number that occurs literally in the table, it is not known, a priori, whether the answer should be generated by an operation or selected from the table. Similarly, when the answer is a natural language phrase that occurs in multiple positions in the table, it is not known which entry (or entries) in the table is actually responsible for the answer. We extend Neural Programmer to handle the weaker supervision signal by backpropagating through decisions that concern how the answer is generated when there is an ambiguity. Our main experimental results concern WikiTableQuestions (Pasupat & Liang, 2015), a real-world question-answering dataset on database tables, with only 10,000 examples for weak supervision. This dataset is particularly challenging because of its small size and the lack of strong supervision, and also because the tables provided at test time are never seen during training, so learning requires adaptation at test time to unseen column names. A state-of-the-art, traditional semantic parser that relies on pruning strategies to ease program search achieves 37.1% accuracy. Standard neural network models like sequence-to-sequence and pointer networks do not appear to be promising for this dataset, as confirmed in our experiments below, which yield single-digit accuracies. In comparison, a single Neural Programmer model using minimal text pre-processing, and trained end-to-end, achieves 34.2% accuracy. This surprising result is enabled primarily by the sample efficiency of Neural Programmer, by the enhanced objective function, and by reducing overfitting via strong regularization with dropout (Srivastava et al., 2014; Iyyer et al., 2015; Gal & Ghahramani, 2016) and weight decay. An ensemble of 15 models, even with a trivial combination technique, achieves 37.7% accuracy.",
      "exclude": true
    },
    {
      "heading": "2 NEURAL PROGRAMMER",
      "text": "In this section we describe in greater detail the Neural Programmer model and the modifications we made to the model. Neural Programmer is a neural network augmented with a set of discrete operations. The model consists of four modules: Question RNN that processes the question and converts the tokens to a distributed representation. We use an LSTM network (Hochreiter & Schmidhuber, 1997) as the question RNN. A list of discrete operations such as counting and entry selection that are manually defined. Each operation is parameterized by a real-valued vector that is learned during training. A selector module that induces two probability distributions at every time step, one over the set of operations and another over the set of columns. The input to the selector is obtained by concatenating the last hidden state of the question RNN, the hidden state of the history RNN from the current timestep, and the attention vector obtained by performing soft attention (Bahdanau et al., 2014) on the question using the history vector. Following Neelakantan et al. (2016), we employ hard selection at test time. History RNN modeled by a simple RNN (Werbos, 1990) with tanh activations which remembers the previous operations and columns selected by the model. The input to the history RNN at each timestep is the result of concatenating the weighted representations of operations and columns with their corresponding probability distributions produced by the selector at the previous timestep. A more detailed description of the basic model can be found in Neelakantan et al. (2016). The model runs for fixed total of T timesteps. The parameters of the operations, selector module, question and history RNNs are all learned with backpropagation using a weak supervision signal that consists of the final answer. Below, we discuss several modifications to the model to make it more broadly applicable, and easier to train.",
      "exclude": false
    },
    {
      "heading": "2.1 OPERATIONS",
      "text": "We use 15 operations in the model that were chosen to closely match the set of operations used in the baseline model (Pasupat & Liang, 2015). All the operations except select and most frequent entry operate only on the set of selected rows which is given by the row selector variable. Before the first timestep, all the rows in the table are set to be selected. The built-in operations are: count returns the number of selected rows in row selector. select and most frequent entry are operations which are computed only once for every question and output a boolean tensor with size same as the size of the input table. An entry in the output of the select operation is set to 1 if the entry matches some phrase in the question. The matched phrases in the question are anonymized to prevent overfitting. Similarly, for most frequent entry, it is set to 1 if the entry is the most frequently occurring one in its column. argmax, argmin, greater than, less than, greater than or equal to, less than or equal to are all operations that output a tensor with size same as the size of the input table. first, last, previous and next modify the row selector. print operation assigns row selector on the selected column of lookup answer. reset resets row selector to its initial value. This operation also serves as no-op when the model needs to induce programs whose complexity is less than T . All the operations are defined to work with soft selection so that the model can be trained with backpropagation. The operations along with their definitions are discussed in the Appendix.",
      "exclude": false
    },
    {
      "heading": "2.2 OUTPUT AND ROW SELECTOR",
      "text": "Neural programmer makes use of three variables: row selector, scalar answer and lookup answer which are updated at every timestep. The variable lookup answer stores answers that are selected from the table while scalar answer stores numeric answers that are not provided in the table.1 The induced program transfers information across timesteps using the row selector variable which contains rows that are selected by the model. Given an input table , containing M rows and C columns (M and C can vary across examples), the output variables at timestep t are given by: scalar answert = op t (count)outputt(count), lookup answert [i][j] = col t (j) op t (print)row selectt1 [i],(i, j)i = 1, 2, . . . ,M, j = 1, 2, . . . , C where opt (op) and col t (j) are the probabilities assigned by the selector to operation op and column j at timestep t respectively and outputt(count) is the output of the count operation at timestep t. The row selector variable at timestep t is obtained by taking the weighted average of the outputs of the remaining operations and is discussed in the Appendix. lookup answerT [i][j] is the probability that the element (i, j) in the input table is in the final answer predicted by the model.",
      "exclude": false
    },
    {
      "heading": "2.3 TRAINING OBJECTIVE",
      "text": "We modify the training objective of Neural Programmer to handle the supervision signal available in real-world settings. In previous work, the position of the answers are explicitly marked in the table when the answer is an entry from the table. However, as discussed in Section 1, in real-world datasets (e.g., (Pasupat & Liang, 2015)) the answer is simply written down introducing two kinds of ambiguities. First, when the answer is a number and if the number is in the table, it is not known 1It is possible to extend the model to generate natural language responses using an RNN decoder but it is not the focus of this paper and we leave it for further work. whether the loss should be computed using the scalar answer variable or the lookup answer variable. Second, when the answer is a natural language phrase and if the phrase occurs in multiple positions in the table, we again do not know which entry (or entries) in the table is actually responsible for generating the answer. We extend Neural Programmer to handle this weaker supervision signal during training by computing the loss only on the prediction that is closest to the desired response. For scalar answers we compute the square loss: Lscalar(scalar answerT , y) = 1 2 (scalar answerT y)2 where y is the ground truth answer. We divide Lscalar by the number of rows in the input table and do not backpropagate on examples for which the loss is greater than a threshold since it leads to instabilities in training. When the answer is a list of items y = (a1, a2, . . . , aN ), for each element in the list (ai, i = 1, 2, . . . , N ) we compute all the entries in the table that match that element, given by Si = (r, c), (r, c) [r][c] = ai. We tackle the ambiguity introduced when an answer item occurs at multiple entries in the table by computing the loss only on the entry which is assigned the highest probability by the model. We construct g 0, 1MC , where g[i, j] indicates whether the element (i, j) in the input table is part of the output. We compute log-loss for each entry and the final loss is given by: Llookup(lookup answerT , y) = N i=1 min(r,c)Si( log(lookup answerT [r, c])) 1 MC M i=1 C j=1 [g[i, j] == 0] log(1 lookup answerT [i, j]) where [cond] is 1 when cond is True, and 0 otherwise. We deal with the ambiguity that occurs when the ground truth is a number and if the number also occurs in the table, by computing the final loss as the soft minimum of Lscalar and Llookup. Otherwise, the loss for an example is Lscalar when the ground truth is a number and Llookup when the ground truth matches some entries in the table. The two loss functions Lscalar and Llookup are in different scales, so we multiply Llookup by a constant factor which we set to 50.0 after a small exploration in our experiments. Since we employ hard selection at test time, only one among scalar answer and lookup answer is modified at the last timestep. We use the variable that is set at the last timestep as the final output of the model.",
      "exclude": false
    },
    {
      "heading": "3 EXPERIMENTS",
      "text": "We apply Neural Programmer on the WikiTableQuestions dataset (Pasupat & Liang, 2015) and compare it to different non-neural baselines including a natural language semantic parser developed by Pasupat & Liang (2015). Further, we also report results from training the sequence-tosequence model (Sutskever et al., 2014) and a modified version of the pointer networks (Vinyals et al., 2015). Our model is implemented in TensorFlow (Abadi et al., 2016) and the model takes approximately a day to train on a single Tesla K80 GPU. We use double-precision format to store the model parameters since the gradients become undefined values in single-precision format. Our code is available at https://github.com/tensorflow/models/tree/master/neural_ programmer.",
      "exclude": false
    },
    {
      "heading": "3.1 DATA",
      "text": "We use the train, development, and test split given by Pasupat & Liang (2015). The dataset contains 11321, 2831, and 4344 examples for training, development, and testing respectively. We use their tokenization, number and date pre-processing. There are examples with answers that are neither number answers nor phrases selected from the table. We ignore these questions during training but the model is penalized during evaluation following Pasupat & Liang (2015). The tables provided in the test set are unseen at training, hence requiring the model to adapt to unseen column names at test time. We train only on examples for which the provided table has less than 100 rows since we run out of GPU memory otherwise, but consider all examples at test time.",
      "exclude": false
    },
    {
      "heading": "3.2 TRAINING DETAILS",
      "text": "We use T = 4 timesteps in our experiments. Words and operations are represented as 256 dimensional vectors, and the hidden vectors of the question and the history RNN are also 256 dimensional. The parameters are initialized uniformly randomly within the range [-0.1, 0.1]. We train the model using the Adam optimizer (Kingma & Ba, 2014) with mini-batches of size 20. The hyperparameter in Adam is set to 1e-6 while others are set to the default values. Since the training set is small compared to other datasets in which neural network models are usually applied, we rely on strong regularization: We clip the gradients to norm 1 and employ early-stopping. The occurrences of words that appear less than 10 times in the training set are replaced by a single unknown word token. We add a weight decay penalty with strength 0.0001. We use dropout with a keep probability of 0.8 on input and output vectors of the RNN, and selector, operation and column name representations (Srivastava et al., 2014). We use dropout with keep probability of 0.9 on the recurrent connections of the question RNN and history RNN using the technique from Gal & Ghahramani (2016). We use word-dropout (Iyyer et al., 2015) with keep probability of 0.9. Here, words in the question are randomly replaced with the unknown word token while training. We tune the dropout rates, regularization strength, and the hyperparameter using grid search on the development data, we fix the other hyperparameters after a small exploration during initial experiments.",
      "exclude": false
    },
    {
      "heading": "3.3 RESULTS",
      "text": "Table 1 shows the performance of our model in comparison to baselines from Pasupat & Liang (2015). The best result from Neural Programmer is achieved by an ensemble of 15 models. The only difference among these models is that the parameters of each model is initialized with a different random seed. We combine the models by averaging the predicted softmax distributions of the models at every timestep. While it is generally believed that neural network models require a large number of training examples compared to simpler linear models to get good performance, our model achieves competitive performance on this small dataset containing only 10,000 examples with weak supervision. We did not get better results either by using pre-trained word vectors (Mikolov et al., 2013) or by pre-training the question RNN with a language modeling objective (Dai & Le, 2015). A possible explanation is that the word vectors obtained from unsupervised learning may not be suitable to the task under consideration. For example, the learned representations of words like maximum and minimum from unsupervised learning are usually close to each other but for our task it is counterproductive. We consider replacing soft selection with hard selection and training the model with the REINFORCE algorithm (Williams, 1992). The model fails to learn in this experiment, probably because the model has to search over millions of symbolic programs for every input question making it highly unlikely to find a program that gives a reward. Hence, the parameters of the model are not updated frequently enough.",
      "exclude": false
    },
    {
      "heading": "3.3.1 NEURAL NETWORK BASELINES",
      "text": "To understand the difficulty of the task for neural network models, we experiment with two neural network baselines: the sequence-to-sequence model (Sutskever et al., 2014) and a modified version of the pointer networks (Vinyals et al., 2015). The input to the sequence-to-sequence model is a concatenation of the table and the question, and the decoder produces the output one token at a time. We consider only examples whose input length is less than 400 to make the running time reasonable. The resulting dataset has 8, 857 and 1, 623 training and development examples respectively. The accuracy of the best model on this development set after hyperparameter tuning is only 8.9%. Next, we experiment with pointer networks to select entries in the table as the final answer. We modify pointer networks to have two-attention heads: one to select the column and the other to select entries within a column. Additionally, the model performs multiple pondering steps on the table before returning the final answer. We train this model only on lookup questions, since the model does not have a decoder to generate answers. We consider only examples whose tables have less than 100 rows resulting in training and development set consisting of 7, 534 and 1, 829 examples respectively. The accuracy of the best model on this development set after hyperparameter tuning is only 4.0%. These results confirm our intuition that discrete operations are hard to learn for neural networks particularly with small datasets in real-world settings.",
      "exclude": false
    },
    {
      "heading": "3.4 ANALYSIS",
      "text": "",
      "exclude": false
    },
    {
      "heading": "3.4.1 MODEL ABLATION",
      "text": "Table 2 shows the impact of different model design choices on the final performance. While anonymizing phrases in the question that match some table entry seems to have a small positive effect, regularization has a much larger effect on the performance. Column selection is performed in Neelakantan et al. (2016) using only the name of a column; however, this selection procedure is insufficient in real-world settings. For example the column selected in question 3 in Table 3 does not have a corresponding phrase in the question. Hence, to select a column we additionally use a boolean feature that indicates whether an entry in that column matches some phrase in the question. Table 2 shows that the addition of this boolean feature has a significant effect on performance.",
      "exclude": false
    },
    {
      "heading": "3.4.2 INDUCED PROGRAMS",
      "text": "Table 3 shows few examples of programs induced by Neural Programmer that yield the correct answer in the development set. The programs given in Table 3 show a few characteristics of the learned model. First, our analysis indicates that the model can adapt to unseen column names at test time. For example in Question 3, the word outcome occurs only 8 times in the training set and is replaced with the unknown word token. Second, the model does not always induce the most efficient (with respect to number of operations other than the reset operation that are picked) program to solve a task. The last 3 questions in the table can be solved using simpler programs. Finally, the model does not always induce the correct program to get the ground truth answer. For example, the last 2 programs will not result in the correct response for all input database tables. The programs would produce the correct response only when the select operation matches one entry in the table.",
      "exclude": false
    },
    {
      "heading": "3.4.3 CONTRIBUTION OF DIFFERENT OPERATIONS",
      "text": "Table 4 shows the contribution of the different operations. The model induces a program that results in a scalar answer 30.7% of the time while the induced program is a table lookup for the remaining questions. The two most commonly used operations by the model are print and select.",
      "exclude": false
    },
    {
      "heading": "3.4.4 ERROR ANALYSIS",
      "text": "To conclude this section, we suggest ideas to potentially improve the performance of the model. First, the oracle performance with 15 Neural Programmer models is 50.5% on the development set while averaging achieves only 37.5% implying that there is still room for improvement. Next, the accuracy of a single model on the training set is 53% which is about 20% higher than the accuracy in both the development set and the test set. This difference in performance indicates that the model suffers from significant overfitting even after employing strong regularization. It also suggests that the performance of the model could be greatly improved by obtaining more training data. Nevertheless, there are limits to the performance improvements we may reasonably expect: in particular, as shown in previous work (Pasupat & Liang, 2015), 21% of questions on a random set of 200 examples in the considered dataset are not answerable because of various issues such as annotation errors and tables requiring advanced normalization.",
      "exclude": false
    },
    {
      "heading": "4 OTHER RELATED WORK",
      "text": "While we discuss in detail various semantic parsing and neural program induction techniques in Section 1, here we briefly describe other relevant work. Recently, Kocisky et al. (2016) develop a semi-supervised semantic parsing method that uses question-program pairs as supervision. Concurrently to our work, Liang et al. (2016) propose neural symbolic machine, a model very similar to Neural Programmer but trained using the REINFORCE algorithm (Williams, 1992). They use only 2 discrete operations and run for a total of 3 timesteps, hence inducing programs that are much simpler than ours. Neural networks have also been applied on question-answering datasets that do not require much arithmetic reasoning (Bordes et al., 2014; Iyyer et al., 2014; Sukhbaatar et al., 2015; Peng et al., 2015; Hermann et al., 2015; Kumar et al., 2016). Wang & Jiang (2016) use a neural network model to get state-of-the-art results on a reading comprehension task (Rajpurkar et al., 2016).",
      "exclude": true
    },
    {
      "heading": "5 CONCLUSION",
      "text": "In this paper, we enhance Neural Programmer to work with weaker supervision signals to make it more broadly applicable. Soft selection during training enables the model to actively explore the space of programs by backpropagation with superior sample complexity. In our experiments, we show that the model achieves performance comparable to a state-of-the-art traditional semantic parser even though the training set contains only 10,000 examples. To our knowledge, this is the first instance of a weakly supervised, end-to-end neural network model that induces programs on a real-world dataset. Acknowledgements We are grateful to Panupong Pasupat for answering numerous questions about the dataset, and providing pre-processed version of the dataset and the output of the semantic parser. We thank David Belanger, Samy Bengio, Greg Corrado, Andrew Dai, Jeff Dean, Nando de Freitas, Shixiang Gu, Navdeep Jaitly, Rafal Jozefowicz, Ashish Vaswani, Luke Vilnis, Yuan Yu and Barret Zoph for their suggestions and the Google Brain team for the support. Arvind Neelakantan is supported by a Google PhD fellowship in machine learning.",
      "exclude": true
    }
  ],
  "fewShot": [
    {
      "model": "claude-haiku-4-5",
      "decision": {
        "rejection": false,
        "confidence": 0.78,
        "primary_reason": "Novel weak supervision approach for program induction with practical modifications to Neural Programmer architecture. Strong experimental validation on real-world dataset with competitive results and thorough ablation studies. Clear methodological contribution despite modest absolute performance."
      },
      "token": {
        "prompt_tokens": 4635,
        "completion_tokens": 80,
        "total_tokens": 4715
      },
      "time": "2026-02-08T22:16:02.434260+00:00"
    },
    {
      "model": "gpt-5.2",
      "decision": {
        "rejection": false,
        "confidence": 0.72,
        "primary_reason": "Strong, timely contribution: an end-to-end weakly supervised neural program-induction approach for real-world table QA (WikiTableQuestions) that is competitive with a hand-engineered semantic parser, with clear objective modifications to handle answer ambiguity, solid baselines/ablations, and meaningful analysis; main weaknesses (hand-designed ops, heavy ensembling, training heuristics) are outweighed by novelty and empirical impact for ICML/ICLR/NeurIPS of its era."
      },
      "token": {
        "prompt_tokens": 6737,
        "completion_tokens": 122,
        "total_tokens": 6859
      },
      "time": "2026-02-09T20:18:58.708855+00:00"
    }
  ],
  "test": false
}