{
  "id": "640",
  "title": "Beyond Bilingual: Multi-sense Word Embeddings using Multilingual Context",
  "abstract": "Word embeddings, which represent a word as a point in a vector space, have become ubiquitous to several NLP tasks. A recent line of work uses bilingual (two languages) corpora to learn a different vector for each sense of a word, by exploiting crosslingual signals to aid sense identification. We present a multi-view Bayesian non-parametric algorithm which improves multi-sense word embeddings by (a) using multilingual (i.e., more than two languages) corpora to significantly improve sense embeddings beyond what one achieves with bilingual information, and (b) uses a principled approach to learn a variable number of senses per word, in a data-driven manner. Ours is the first approach with the ability to leverage multilingual corpora efficiently for multi-sense representation learning. Experiments show that multilingual training significantly improves performance over monolingual and bilingual training, by allowing us to combine different parallel corpora to leverage multilingual context. Multilingual training yields com- parable performance to a state of the art monolingual model trained on five times more training data.",
  "accepted": false,
  "zeroShot": [
    {
      "model": "claude-haiku-4-5",
      "decision": {
        "rejection": false,
        "confidence": 0.72,
        "primary_reason": "Novel multilingual extension of multi-sense embeddings with principled Bayesian non-parametric approach and solid experimental validation, though limited evaluation scope and some methodological concerns prevent higher confidence."
      },
      "token": {
        "prompt_tokens": 4997,
        "completion_tokens": 75,
        "total_tokens": 5072
      },
      "time": "2026-02-05T23:07:12.673347+00:00"
    }
  ],
  "article": [
    {
      "heading": "1 INTRODUCTION",
      "text": "Word embeddings (Turian, Ratinov, and Bengio, 2010; Mikolov, Yih, and Zweig, 2013, inter alia) represent a word as a point in a vector space. This space is able to capture semantic relationships: vectors of words with similar meanings have high cosine similarity. Use of embeddings as features has been shown to benefit several NLP tasks and serve as good initializations for deep architectures ranging from dependency parsing (Bansal, Gimpel, and Livescu, 2014) to named entity recognition (Guo et al., 2014b). Although these representations are now ubiquitous in NLP, most algorithms for learning wordembeddings do not allow a word to have different meanings in different contexts, a phenomenon known as polysemy. For example, the word bank assumes different meanings in financial (eg. bank pays interest) and geographical contexts (eg. river bank) and which cannot be represented adequately with a single embedding vector. Unfortunately, there are no large sense-tagged corpora available and such polysemy must be inferred from the data during the embedding process. Several attempts (Reisinger and Mooney, 2010; Neelakantan et al., 2014; Li and Jurafsky, 2015) have been made to infer multi-sense word representations by modeling the sense as a latent variable in a Bayesian non-parametric framework. These approaches rely on the one-sense per collocation heuristic (Yarowsky, 1995), which assumes that presence of nearby words correlate with the sense of the word of interest. This heuristic provides only a weak signal for sense identification, and such algorithms require large amount of training data to achieve competitive performance. Recently, several approaches (Guo et al., 2014a; Suster, Titov, and van Noord, 2016) propose to learn multi-sense embeddings by exploiting the fact that different senses of the same word may be translated into different words in a foreign language (Dagan and Itai, 1994; Resnik and Yarowsky, 1999; Diab and Resnik, 2002; Ng, Wang, and Chan, 2003). For example, bank in English may be translated to banc or banque in French, depending on whether the sense is financial or geographical. Such bilingual distributional information allows the model to identify which sense of a word is being used during training. However, bilingual distributional signals often do not suffice. It is common that polysemy for a word survives translation. Fig. 1 shows an illustrative example both senses of interest get translated to interet in French. However, this becomes much less likely as the number of languages under consideration grows. By looking at Chinese translation in Fig. 1, we can observe that the senses translate to different surface forms. Note that the opposite can also happen (i.e. same surface forms in Chinese, but different in French). Existing crosslingual approaches are inherently bilingual and cannot naturally extend to include additional languages due to several limitations (details in Section4). Furthermore, works like (Suster, Titov, and van Noord, 2016) sets a fixed number of senses for each word, leading to inefficient use of parameters, and unnecessary model complexity.1 This paper addresses these limitations by proposing a multi-view Bayesian non-parametric word representation learning algorithm which leverages multilingual distributional information. Our representation learning framework is the first multilingual (not bilingual) approach, allowing us to utilize arbitrarily many languages to disambiguate words in English. To move to multilingual system, it is necessary to ensure that the embeddings of each foreign language are relatable to each other (i.e., they live in the same space). We solve this by proposing an algorithm in which word representations are learned jointly across languages, using English as a bridge. While large parallel corpora between two languages are scarce, using our approach we can concatenate multiple parallel corpora to obtain a large multilingual corpus. The parameters are estimated in a Bayesian nonparametric framework that allows our algorithm to only associate a word with a new sense vector when evidence (from either same or foreign language context) requires it. As a result, the model infers different number of senses for each word in a data-driven manner, avoiding wasting parameters. Together, these two ideas multilingual distributional information and nonparametric sense modeling allow us to disambiguate multiple senses using far less data than is necessary for previous methods. We experimentally demonstrate that our algorithm can achieve competitive performance after training on a small multilingual corpus, comparable to a model trained monolingually on a much larger corpus. We present an analysis discussing the effect of various parameters choice of language family for deriving the multilingual signal, crosslingual window size etc. and also show qualitative improvement in the embedding space.",
      "exclude": true
    },
    {
      "heading": "2 RELATED WORK",
      "text": "Work on inducing multi-sense embeddings can be divided in two broad categories two-staged approaches and joint learning approaches. Two-staged approaches (Reisinger and Mooney, 2010; Huang et al., 2012) induce multi-sense embeddings by first clustering the contexts and then using the clustering to obtain the sense vectors. The contexts can be topics induced using latent topic models(Liu, Qiu, and Huang, 2015; Liu et al., 2015), or Wikipedia (Wu and Giles, 2015) or coarse part-of-speech tags (Qiu et al., 2014). A more recent line of work in the two-staged category is that of retrofitting (Faruqui et al., 2015; Jauhar, Dyer, and Hovy, 2015), which aims to infuse semantic ontologies from resources like WordNet (Miller, 1995) and Framenet (Baker, Fillmore, and Lowe, 1998) into embeddings during a post-processing step. Such resources list (albeit not exhaustively) the senses of a word, and by retro-fitting it is possible to tease apart the different senses of a word. While some resources like WordNet (Miller, 1995) are available for many languages, they are not exhaustive in listing all possible senses. Indeed, the number senses of a word is highly dependent on the task and cannot be pre-determined using a lexicon (Kilgarriff, 1997). Ideally, the senses should be inferred in a data-driven manner, so that new senses not listed in such lexicons can be discovered. While recent work has attempted to remedy this by using parallel text for retrofitting sense-specific embeddings (Ettinger, Resnik, and Carpuat, 2016), their procedure requires creation of sense graphs, which introduces additional tuning parameters. On the other hand, our approach only requires two tuning parameters (prior and maximum number of senses T ). 1Most words in conventional English are monosemous, i.e. single sense (eg. the word monosemous) In contrast, joint learning approaches (Neelakantan et al., 2014; Li and Jurafsky, 2015) jointly learn the sense clusters and embeddings by using non-parametrics. Our approach belongs to this category. The closest non-parametric approach to ours is that of (Bartunov et al., 2016), who proposed a multisense variant of the skip-gram model which learns the different number of sense vectors for all words from a large monolingual corpus (eg. English Wikipedia). Our work can be viewed as the multi-view extension of their model which leverages both monolingual and crosslingual distributional signals for learning the embeddings. In our experiments, we compare our model to monolingually trained version of their model. Incorporating crosslingual distributional information is a popular technique for learning word embeddings, and improves performance on several downstream tasks (Faruqui and Dyer, 2014; Guo et al., 2016; Upadhyay et al., 2016). However, there has been little work on learning multi-sense embeddings using crosslingual signals (Bansal, DeNero, and Lin, 2012; Guo et al., 2014a; Suster, Titov, and van Noord, 2016) with only (Suster, Titov, and van Noord, 2016) being a joint approach. (Kawakami and Dyer, 2015) also used bilingual distributional signals in a deep neural architecture to learn context dependent representations for words, though they do not learn separate sense vectors.",
      "exclude": true
    },
    {
      "heading": "3 MODEL DESCRIPTION",
      "text": "Let E = xe1, .., xei , .., xeNe denote the words of the English side and F = x f 1 , .., x f i , .., x f Nf denote the words of the foreign side of the parallel corpus. We assume that we have access to word alignments Aef and Afe mapping words in English sentence to their translation in foreign sentence (and vice-versa), so that xe and xf are aligned if Aef (xe) = xf . We define Nbr(x, L, d) as the neighborhood in language L of size d (on either side) around word x in its sentence. The English and foreign neighboring words are denoted by ye and yf , respectively. Note that ye and yf need not be translations of each other. Each word xf in the foreign vocabulary is associated with a dense vector xf in Rm, and each word xe in English vocabulary admits at most T sense vectors, with the kth sense vector denoted as xek. 2 As our main goal is to model multiple senses for words in English, we do not model polysemy in the foreign language and use a single vector to represent each word in the foreign vocabulary. We model the joint conditional distribution of the context words ye, yf given an English word xe and its corresponding translation xf on the parallel corpus: P (ye, yf | xe, xf ;, ), (1) where are model parameters (i.e. all embeddings) and governs the hyper-prior on latent senses. Assume xe has multiple senses, which are indexed by the random variable z, Eq. (1) can be rewritten, z P (ye, yfz, | xe, xf , ; )d where are the parameters determining the model probability on each sense for xe (i.e., the weight on each possible value for z). We place a Dirichlet process (Ferguson, 1973) prior on sense assignment for each word. Thus, adding the word-x subscript to emphasize that these are word-specific senses, P (zx = k | x) = xk k1 r=1 (1 xr), xk | ind Beta(xk | 1, ), k = 1, . . . . (2) That is, the potentially infinite number of senses for each word x have probability determined by the sequence of independent stick-breaking weights, xk, in the constructive definition of the DP (Sethuraman, 1994). The hyper-prior concentration provides information on the number of senses we expect to observe in our corpus. After conditioning upon word sense, we decompose the context probability P (ye, yf | z, xe, xf ; ) into two terms, P (ye | xe, xf , z; )P (yf | xe, xf , z; ). Both the first and the second terms are sense-dependent, and each factors as, P (y |xe, xf , z=k; )(xe, z=k, y)(xf , y) = exp(yTxek) exp(yTxf ) = exp(yT (xek+xf )), 2We also maintain a context vector for each word in the English and Foreign vocabularies. The context vector is used as the representation of the word when it appears as the context for another word. The bank paid me [interest] on my savings. (interest,2,savings) where xek is the embedding corresponding to the k th sense of the word xe, and y is either ye or yf . The factor (xe, z = k, y) use the corresponding sense vector in a skip-gram-like formulation. This results in total of 4 factors, P (ye, yf | z, xe, xf ; ) (xe, z, ye)(xf , yf )(xe, z, yf )(xf , ye) (3) See Figure 2 for illustration of each factor. This modeling approach is reminiscent of (Luong, Pham, and Manning, 2015), who jointly learned embeddings for two languages l1 and l2 by optimizing a joint objective containing 4 skip-gram terms using the aligned pair (xe,xf ) two predicting monolingual contexts l1 l1, l2 l2 , and two predicting crosslingual contexts l1 l2, l2 l1. Learning. Learning involves maximizing the log-likelihood, P (ye, yf | xe, xf ;, ) = z P (ye, yf , z, | xe, xf , ; )d Let q(z, ) = q(z)q() where q(z) = i q(zi) and q() = V w=1 T k=1 wk be the fully factorized variational approximation of the true posterior P (z, | ye, yf , xe, xf , ), where V is the size of english vocabulary, and T is the maximum number of senses for any word. The optimization problem solves for ,q(z) and q() using the stochastic variational inference technique (Hoffman et al., 2013) similar to (Bartunov et al., 2016) (refer for details). The resulting learning algorithm is shown as Algorithm 1. The first for-loop (line 1) updates the English sense vectors using the crosslingual and monolingual contexts. First, the expected sense distribution for the current English word w is computed using the current estimate of q() (line 4). The sense distribution is updated (line 7) using the combined monolingual and crosslingual contexts (line 5) and re-normalized (line 8). Using the updated sense distribution q()s sufficient statistics is re-computed (line 9) and the global parameter is updated (line 10) as follows, + t k|zik> yyc zik log p(y|xi, k, ) (4) Note that in the above sum, a sense participates in a update only if its probability exceeds a threshold (=0.001). The final model retains sense vectors whose sense probability exceeds the same threshold. The last for-loop (line 11) jointly optimizes the foreign embeddings using English context with the standard skip-gram updates. Disambiguation. Similar to (Bartunov et al., 2016), we can disambiguate the sense for the word xe given a monolingual context ye as follows, P (z | xe, ye) P (ye | xe, z; ) P (z | xe, )q() (5) Although the model trains embeddings using both monolingual and crosslingual context, we only use monolingual context at test time. We found that so long as the model has been trained with multilingual context, it performs well in sense disambiguation on new data even if it contains only monolingual context. A similar observation was made by (Suster, Titov, and van Noord, 2016).",
      "exclude": false
    },
    {
      "heading": "4 MULTILINGUAL EXTENSION",
      "text": "Bilingual distributional signal alone may not be sufficient as polysemy may survive translation in the second language. Unlike existing approaches, we can easily incorporate multilingual distributional Algorithm 1 Psuedocode of Learning Algorithm Input: parallel corpus E = xe1, .., xei , .., xeNe and F = x f 1 , .., x f i , .., x f Nf and alignments Aef and Afe, Hyper-parameters and T , window sizes d, d . Output: , q(), q(z) 1: for i = 1 to Ne do . update english vectors 2: w xei 3: for k = 1 to T do 4: zik Eq(w)[log p(zi = k|, x e i )] 5: yc Nbr(xei ,E,d) Nbr(xfi ,F ,d ) xfi where x f i = Aef (x e i ) 6: for y in yc do 7: SENSE-UPDATE(xei , y, zi) 8: Renormalize zi using softmax 9: Update suff. stats. for q() like (Bartunov et al., 2016) 10: Update using eq. (4) 11: for i = 1 to Nf do . jointly update foreign vectors 12: yc Nbr(xfi ,F ,d) Nbr(x e i ,E,d ) xei where xei = Afe(xfi ) 13: for y in yc do 14: SKIP-GRAM-UPDATE(xfi , y) 15: procedure SENSE-UPDATE(xi, y, zi) 16: zik zik + log p(y|xi, k, ) signals in our model. For using languages l1 and l2 to learn multi-sense embeddings for English, we train on a concatenation of En-l1 parallel corpus with an En-l2 parallel corpus. This technique can easily be generalized to more than two foreign languages to obtain a large multilingual corpus. Value of (ye, xf ). The factor modeling the dependence of the english context word ye on foreign word xf is crucial to performance when using multiple languages. Consider the case of using French and Spanish contexts to disambiguate the financial sense of the english word bank. In this case, the (financial) sense vector of bank will be used to predict vector of banco (Spanish context) and banque (French context). If vectors for banco and banque do not reside in the same space or are not close, the model will incorrectly assume they are different contexts to introduce a new sense for bank. This is precisely why the bilingual models, like that of (Suster, Titov, and van Noord, 2016), cannot be extended to multilingual setting, as they pre-train the embeddings of second language before running the multi-sense embedding process. As a result of naive pre-training, the French and Spanish vectors of semantically similar pairs like (banco,banque) will lie in different spaces and need not be close. A similar reason holds for (Guo et al., 2014a), as they use a two step approach instead of joint learning. To avoid this, the vector for pairs like banco and banque should lie in the same space and close to each other and the sense vector for bank. The (ye, xf ) term attempts to ensure this by using the vector for banco and banque to predict the vector of bank. This way, the model brings the embedding space for Spanish and French closer by using English as a bridge language during joint training. A similar idea of using English as a bridging language was used in the models proposed in (Hermann and Blunsom, 2014) and (Coulmance et al., 2015). Beside the benefit in the multilingual case, the (ye, xf ) term improves performance in the bilingual case as well, as it forces the English and second language embeddings to remain close in space. To show the value of (ye, xf ) factor in our experiments, we ran a variant of Algorithm 1 without the (ye, xf ) factor, by only using monolingual neighborhood Nbr(xfi , F ) in line 12 of Algorithm 1. We call this variant ONE-SIDED model and the model in Algorithm 1 the FULL model.",
      "exclude": false
    },
    {
      "heading": "5 EXPERIMENTAL SETUP",
      "text": "Parallel Corpora. We use parallel corpora in English (En), French (Fr), Spanish (Es), Russian (Ru) and Chinese (Zh) in our experiments. Corpus statistics for all datasets used in our experiments are shown in Table 1. For En-Zh, we use the FBIS parallel corpus (LDC2003E14). For En-Fr, we use the first 10M lines from the Giga-EnFr corpus released as part of the WMT shared task (CallisonBurch et al., 2011). Note that the domain from which parallel corpus has been derived can affect the final result. To understand what choice of languages provide suitable disambiguation signal, it is necessary to control for domain in all parallel corpora. To this end, we also used the En-Fr, En-Es, En-Zh and En-Ru sections of the MultiUN parallel corpus (Eisele and Chen, 2010). Word alignments were generated using fast_align tool (Dyer, Chahuneau, and Smith, 2013) in the symmetric intersection mode. Tokenization and other preprocessing were performed using cdec 3 toolkit. Stanford Segmenter (Tseng et al., 2005) was used to preprocess the chinese corpora. Word Sense Induction (WSI). We evaluate our approach on word sense induction task. In this task, we are given several sentences showing usages of the same word, and are required to cluster all sentences which use the same sense (Nasiruddin, 2013). The predicted clustering is then compared against a provided gold clustering. Note that WSI is a harder task than Word Sense Disambiguation (WSD)(Navigli, 2009), as unlike WSD, this task does not involve any supervision or explicit human knowledge about senses of words. We use the disambiguation approach in eq. (5) to predict the sense given the word and four context words. To allow for fair comparison with earlier work, we use the same benchmark datasets as (Bartunov et al., 2016) Semeval-2007, 2010 and Wikipedia Word Sense Induction (WWSI). We report Adjusted Rand Index (ARI) (Hubert and Arabie, 1985) in the experiments, as ARI is a more strict and precise metric than F-score and V-measure. Parameter Tuning. For fairness, we used five context words on either side to update each English word-vectors in all the experiments. In the monolingual setting, all five words are English; in the multilingual settings, we used four neighboring English words plus the one foreign word aligned to the word being updated (d = 4, d = 0 in Algorithm 1). We also analyze effect of varying d. We tune the parameters and T by maximizing the log-likelihood of a held out English text.4 The parameters were chosen from the following values = 0.05, 0.1, .., 0.25, T = 5, 10, .., 30. All models were trained for 10 iteration with a decaying learning rate of 0.025, decayed to 0. Unless otherwise stated, all embeddings are 100 dimensional. Under various choice of and T , we identify only about 10-20% polysemous words in the vocabulary using monolingual training and 20-25% polysemous using multilingual training. It is evident using the non-parametric prior has led to substantially more efficient representation compared to previous methods with fixed number of senses per word.",
      "exclude": false
    },
    {
      "heading": "6 EXPERIMENTAL RESULTS",
      "text": "We performed extensive experiments to evaluate the benefit of leveraging bilingual and multilingual information during training. We also analyze how the different choices of language family (i.e. using more distant vs more similar languages) affect performance of the embeddings. Word Sense Induction Results. The results for WSI are shown in Table 2. MONO refers to the AdaGram model of (Bartunov et al., 2016) trained on the English side of the parallel corpus. In all cases, the MONO model is outperformed by ONE-SIDED and FULL models, showing the benefit of using crosslingual signal in training. Best performance is attained by the multilingual model (EnFrZh), showing value of multilingual signal. The value of (ye, xf ) term is also verified by the fact that the ONE-SIDED model performs worse than the FULL model. We can also compare (unfairly to FULL model) to the best results described in (Bartunov et al., 2016), which achieved ARI scores of 0.069, 0.097 and 0.286 on the three datasets respectively after 3github.com/redpony/cdec 4first 100k lines from the En-Fr Europarl (Koehn, 2005) training 300 dimensional embeddings on English Wikipedia ( 100M lines). Note that, as WWSI was derived from Wikipedia, training on Wikipedia gives AdaGram model an undue advantage, resulting in high ARI score on WWSI. Nevertheless, even in the unfair comparison, it noteworthy that on S-2007 and S-2010, we can achieve comparable performance (0.067 and 0.094) with multilingual training to a model trained on almost 5 times more data and higher (300) dimensional embeddings. Contextual Word Similarity Results. For completeness, we report correlation scores on Stanford contextual word similarity dataset (SCWS) (Huang et al., 2012) in Table 2. The task requires computing similarity between two words given their contexts. While the bilingually trained model outperforms the monolingually trained model, surprisingly the multilingually trained model does not perform well on SCWS. We believe this may be due to our parameter tuning strategy.5 Effect of Language Family Distance. Intuitively, choice of language can affect the result from crosslingual training as some languages may provide better disambiguation signals than others. We performed a systematic set of experiment to evaluate whether we should choose languages from a closer family (Indo-European languages) or farther family (Non-Indo European Languages) as training data alongside English.6 To control for domain here we use the MultiUN corpus. We use En paired with Fr and Es as Indo-European languages, and English paired with Ru and Zh for representing Non-Indo-European languages. From Table 3, we see that using Non-Indo European languages yield a slightly higher average improvement in WSI task than using Indo-European languages. This suggests that using languages from a distance family aids better disambiguation. Our findings echo those of (Resnik and Yarowsky, 1999), who found that the tendency to lexicalize different senses of an English word differently in a second language correlated with language distance. Effect of Window Size. Figure 3d shows the effect of increasing the crosslingual window (d) on the average ARI on the WSI task for the En-Fr and En-Zh models. While increasing the window size improves the average score for En-Zh model, the score for the En-Fr model goes down. This suggests that it might be beneficial to have a separate window parameter per language. This also 5Most works tune directly on the test dataset for Word Similarity tasks (Faruqui et al., 2016) 6 (Suster, Titov, and van Noord, 2016) compared different languages but did not control for domain. aligns with the observation earlier that different language families have different suitability (bigger crosslingual context from a distant family helped) and requirements for optimal performance. Qualitative Illustration. As an illustration for the effects of multilingual training, Figure 3 shows PCA plots for 11 sense vectors for 9 words using monolingual, bilingual and multilingual models. From Fig 3a, we note that with monolingual training the senses are poorly separated. Although the model infers two senses for bank, the two senses of bank are close to financial terms, suggesting their distinction was not recognized. The same can be observed about apple. In Fig 3b, with bilingual training, the model infers two senses of bank correctly, and two sense of apple become more distant. The model can still improve eg. pulling interest towards the financial sense of bank, and pulling itunes towards apple 2. Finally, in Fig 3c, all senses of the words are more clearly clustered, improving over the clustering of Fig 3b. The senses of apple, interest, and bank are well separated, and are close to sense-specific words, showing the benefit of multilingual training.",
      "exclude": false
    },
    {
      "heading": "7 CONCLUSION",
      "text": "We presented a multi-view, non-parametric word representation learning algorithm which can leverage multilingual distributional information. Our approach effectively combines the benefits of crosslingual training and Bayesian non-parametrics. Ours is the first multi-sense representation learning algorithm capable of using multilingual distributional information efficiently, by combining several parallel corpora to obtained a large multilingual corpus. Our experiments show how this multi-view approach learns high-quality embeddings using substantially less data and parameters than prior state-of-the-art. While we focused on improving the embedding of English words here, the same algorithm could learn better multi-sense embedding for Chinese, for instance. Exciting avenues for future research include extending our approach to model polysemy in foreign language. The sense vectors can then be aligned across languages (thanks to our joint training paradigm), to generate a multilingual Wordnet like resource, in a completely unsupervised manner.",
      "exclude": true
    }
  ],
  "fewShot": [
    {
      "model": "claude-haiku-4-5",
      "decision": {
        "rejection": false,
        "confidence": 0.72,
        "primary_reason": "Novel multilingual extension of multi-sense embeddings with principled Bayesian non-parametric approach; solid experimental validation on WSI tasks; clear methodological contribution beyond prior bilingual work."
      },
      "token": {
        "prompt_tokens": 5159,
        "completion_tokens": 76,
        "total_tokens": 5235
      },
      "time": "2026-02-08T22:37:15.373266+00:00"
    },
    {
      "model": "gpt-5.2",
      "decision": {
        "rejection": true,
        "confidence": 0.72,
        "primary_reason": "Solid extension of bilingual multi-sense embeddings to multilingual parallel data with a DP prior and reasonable WSI gains, but the core technical novelty is incremental (largely a multi-corpus/joint-training variant of existing DP multi-sense + bilingual skip-gram objectives), and the empirical case is not strong enough for ICML/ICLR/NeurIPS: evaluation is narrow (mostly WSI/SCWS), improvements are modest/uneven (multilingual hurts SCWS), comparisons to strongest contemporary contextual/sense models are limited, and key claims about efficiency/“first to leverage multilingual efficiently” are under-justified."
      },
      "token": {
        "prompt_tokens": 7136,
        "completion_tokens": 153,
        "total_tokens": 7289
      },
      "time": "2026-02-09T20:39:41.420710+00:00"
    }
  ],
  "test": false
}