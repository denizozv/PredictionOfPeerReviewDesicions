{
  "id": "645",
  "title": "Deep Generalized Canonical Correlation Analysis",
  "abstract": "We present Deep Generalized Canonical Correlation Analysis (DGCCA) â€“ a method for learning nonlinear transformations of arbitrarily many views of data, such that the resulting transformations are maximally informative of each other. While methods for nonlinear two-view representation learning (Deep CCA, (Andrew et al., 2013)) and linear many-view representation learning (Generalized CCA (Horst, 1961)) exist, DGCCA is the first CCA-style multiview representation learning technique that combines the flexibility of nonlinear (deep) representation learning with the statistical power of incorporating information from many independent sources, or views. We present the DGCCA formulation as well as an efficient stochastic optimization algorithm for solving it. We learn DGCCA representations on two distinct datasets for three downstream tasks: phonetic transcription from acoustic and articulatory measurements, and recommending hashtags and friends on a dataset of Twitter users. We find that DGCCA representations soundly beat existing methods at phonetic transcription and hashtag recommendation, and in general perform no worse than standard linear many-view techniques.",
  "accepted": false,
  "zeroShot": [
    {
      "model": "claude-haiku-4-5",
      "decision": {
        "rejection": false,
        "confidence": 0.82,
        "primary_reason": "Novel combination of deep learning with multi-view CCA addressing clear limitations of prior work (DCCA limited to 2 views, GCCA limited to linear). Solid theoretical contribution with efficient optimization algorithm and convincing experiments on multiple datasets. Minor concerns: limited experimental scope (small speaker subset), some hyperparameter choices appear ad-hoc, and Twitter results mixed. Suitable for ICML/ICLR."
      },
      "token": {
        "prompt_tokens": 5716,
        "completion_tokens": 118,
        "total_tokens": 5834
      },
      "time": "2026-02-05T23:08:04.079841+00:00"
    },
    {
      "model": "gpt-5.2",
      "decision": {
        "rejection": false,
        "confidence": 0.72,
        "primary_reason": "Strong, relevant and original multiview representation learning method (nonlinear many-view CCA) with sound optimization/derivation and solid empirical gains on two real datasets; evidence is adequate though not exhaustive by modern standards."
      },
      "token": {
        "prompt_tokens": 5026,
        "completion_tokens": 63,
        "total_tokens": 5089
      },
      "time": "2026-02-09T21:36:50.035525+00:00"
    }
  ],
  "article": [
    {
      "heading": "",
      "text": "We present Deep Generalized Canonical Correlation Analysis (DGCCA) a method for learning nonlinear transformations of arbitrarily many views of data, such that the resulting transformations are maximally informative of each other. While methods for nonlinear two-view representation learning (Deep CCA, (Andrew et al., 2013)) and linear many-view representation learning (Generalized CCA (Horst, 1961)) exist, DGCCA is the first CCA-style multiview representation learning technique that combines the flexibility of nonlinear (deep) representation learning with the statistical power of incorporating information from many independent sources, or views. We present the DGCCA formulation as well as an efficient stochastic optimization algorithm for solving it. We learn DGCCA representations on two distinct datasets for three downstream tasks: phonetic transcription from acoustic and articulatory measurements, and recommending hashtags and friends on a dataset of Twitter users. We find that DGCCA representations soundly beat existing methods at phonetic transcription and hashtag recommendation, and in general perform no worse than standard linear many-view techniques.",
      "exclude": true
    },
    {
      "heading": "1 INTRODUCTION",
      "text": "Multiview representation learning refers to settings where one has access to many views of data, at train time. Views often correspond to different modalities or independent information about examples: a scene represented as a series of audio and image frames, a social media user characterized by the messages they post and who they friend, or a speech utterance and the configuration of the speakers tongue. Multiview techniques learn a representation of data that captures the sources of variation common to all views. Multiview representation techniques are attractive for intuitive reasons. A representation that is able to explain many views of the data is more likely to capture meaningful variation than a representation that is a good fit for only one of the views. They are also attractive for the theoretical reasons. For example, Anandkumar et al. (2014) show that certain classes of latent variable models, such as Hidden Markov Models, Gaussian Mixture Models, and Latent Dirichlet Allocation models, can be optimally learned with multiview spectral techniques. Representations learned from many views will generalize better than one, since the learned representations are forced to accurately capture variation in all views at the same time (Sridharan & Kakade, 2008) each view acts as a regularizer, constraining the possible representations that can be learned. These methods are often based on canonical correlation analysis (CCA), a classical statisical technique proposed by Hotelling (1936). In spite of encouraging theoretical guarantees, multiview learning techniques cannot freely model nonlinear relationships between arbitrarily many views. Either they are able to model variation across many views, but can only learn linear mappings to the shared space (Horst, 1961), or they simply cannot be applied to data with more than two views using existing techniques based on kernel CCA (Hardoon et al., 2004) and deep CCA (Andrew et al., 2013). Here we present Deep Generalized Canonical Correlation Analysis (DGCCA). Unlike previous correlation-based multiview techniques, DGCCA learns a shared representation from data with arbitrarily many views and simultaneously learns nonlinear mappings from each view to this shared space. The only (mild) constraint is that these nonlinear mappings from views to shared space must be differentiable. Our main methodological contribution is the derivation of the gradient update for the Generalized Canonical Correlation Analysis (GCCA) objective (Horst, 1961). As a practical contribution, we have also released an implementation of DGCCA1. We also evaluate DGCCA-learned representations on two distinct datasets and three downstream tasks: phonetic transcription from aligned speech and articulatory data, and Twitter hashtag and friend recommendation from six text and network feature views. We find that downstream performance of DGCCA representations is ultimately task-dependent. However, we find clear gains in performance from DGCCA for tasks previously shown to benefit from representation learning on more than two views, with up to 4% improvement in heldout accuracy for phonetic transcription. The paper is organized as follows. We review prior work in Section 2. In Section 3 we describe DGCCA. Empirical results on a synthetic dataset, and three downstream tasks are presented in Section 4. In Section 5, we describe the differences between DGCCA and other non-CCA-based multiview learning work and conclude with future directions in Section 6.",
      "exclude": true
    },
    {
      "heading": "2 PRIOR WORK",
      "text": "Some of most successful techniques for multiview representation learning are based on canonical correlation analysis (Wang et al., 2015a;b) and its extension to the nonlinear and many view settings, which we describe in this section. For other related multiview learning techniques, see Section 5.",
      "exclude": false
    },
    {
      "heading": "2.1 CANONICAL CORRELATION ANALYSIS (CCA)",
      "text": "Canonical correlation analysis (CCA) (Hotelling, 1936) is a statistical method that finds maximally correlated linear projections of two random vectors and is a fundamental multiview learning technique. Given two input views, X1 Rd1 and X2 Rd2 , with covariance matrices, 11 and 22, respectively, and cross-covariance matrix, 12, CCA finds directions that maximize the correlation between them: (u1, u 2) = argmax u1Rd1 ,u2Rd2 corr(u>1 X1, u > 2 X2) = argmax u1Rd1 ,u2Rd2 u>1 12u2 u>1 11u1u > 2 22u2 Since this formulation is invariant to affine transformations of u1 and u2, we can write it as the following constrained optimization formulation: (u1, u 2) = argmax u>1 11u1=u > 2 22u2=1 u>1 12u2 (1) This technique has two limitations that have led to significant extensions: First, it is limited to learning representations that are linear transformations of the data in each view, and second, it can only leverage two input views.",
      "exclude": false
    },
    {
      "heading": "2.2 DEEP CANONICAL CORRELATION ANALYSIS (DCCA)",
      "text": "Deep CCA (DCCA) (Andrew et al., 2013) is an extension of CCA that addresses the first limitation by finding maximally linearly correlated non-linear transformations of two vectors. It does this by passing each of the input views through stacked non-linear representations and performing CCA on the outputs. Let us use f1(X1) and f2(X2) to represent the network outputs. The weights, W1 and W2, of these networks are trained through standard backpropagation to maximize the CCA objective: (u1, u 2,W 1 ,W 2 ) = argmax u1,u2 corr(u>1 f1(X1), u > 2 f2(X2)) DCCA is still limited to only 2 input views. 1See https://bitbucket.org/adrianbenton/dgcca-py3 for implementation of DGCCA along with data from the synthetic experiments.",
      "exclude": false
    },
    {
      "heading": "2.3 GENERALIZED CANONICAL CORRELATION ANALYSIS (GCCA)",
      "text": "Another extension of CCA, which addresses the limitation on the number of views, is Generalized CCA (GCCA) (Horst, 1961). It corresponds to solving the optimization problem in Equation (2), of finding a shared representation G of J different views, where N is the number of data points, dj is the dimensionality of the jth view, r is the dimensionality of the learned representation, and Xj RdjN is the data matrix for the jth view.2 minimize UjRdjr,GRrN J j=1 G U>j Xj2F subject to GG> = Ir (2) Solving GCCA requires finding an eigendecomposition of an N N matrix, which scales quadratically with sample size and leads to memory constraints. Unlike CCA and DCCA, which only learn projections or transformations on each of the views, GCCA also learns a view-independent representation G that best reconstructs all of the view-specific representations simultaneously. The key limitation of GCCA is that it can only learn linear transformations of each view.",
      "exclude": false
    },
    {
      "heading": "3 DEEP GENERALIZED CANONICAL CORRELATION ANALYSIS (DGCCA)",
      "text": "In this section, we present deep GCCA (DGCCA): a multiview representation learning technique that benefits from the expressive power of deep neural networks and can also leverage statistical strength from more than two views in data, unlike Deep CCA which is limited to only two views. More fundamentally, deep CCA and deep GCCA have very different objectives and optimization problems, and it is not immediately clear how to extend deep CCA to more than two views. DGCCA learns a nonlinear map for each view in order to maximize the correlation between the learnt representations across views. In training, DGCCA passes the input vectors in each view through multiple layers of nonlinear transformations and backpropagates the gradient of the GCCA objective with respect to network parameters to tune each views network, as illustrated in Figure 1. The objective is to train networks that reduce the GCCA reconstruction error among their outputs. At test time, new data can be projected by feeding them through the learned network for each view. We now formally define the DGCCA problem. We consider J views in our data, and let Xj RdjN denote the jth input matrix.3 The network for the jth view consists of Kj layers. Assume, for simplicity, that each layer in the jth view network has cj units with a final (output) layer of size oj . The output of the kth layer for the jth view is h j k = s(W j kh j k1), where s : R R is a nonlinear activation function and W jk Rckck1 is the weight matrix for the kth layer of the jth view network. We denote the output of the final layer as fj(Xj). 2Horsts original presentation of GCCA is in a very different form from the one used here but has been shown to be equivalent (Kettenring, 1971). 3Our notation for this section closely follows that of Andrew et al. (2013) DGCCA can be expressed as the following optimization problem: find weight matrices W j = W j1 , . . . ,W j Kj defining the functions fj , and linear transformations Uj (of the output of the jth network), for j = 1, . . . , J , that minimize UjRojr,GRrN J j=1 G U>j fj(Xj)2F subject to GG> = Ir , (3) where G RrN is the shared representation we are interested in learning. Optimization: We solve the DGCCA optimization problem using stochastic gradient descent (SGD) with mini-batches. In particular, we estimate the gradient of the DGCCA objective in Problem 3 on a mini-batch of samples that is mapped through the network and use back-propagation to update the weight matrices, W js. However, note that the DGCCA optimization problem is a constrained optimization problem. It is not immediately clear how to perform projected gradient descent with back-propagation. Instead, we characterize the objective function of the GCCA problem at an optimum, and compute its gradient with respect to the inputs to GCCA, i.e. with respect to the network outputs. These gradients are then back-propagated through the network to update W js. Although the relationship between DGCCA and GCCA is analogous to the relationship between DCCA and CCA, derivation of the GCCA objective gradient with respect to the network output layers is non-trivial. The main difficulty stems from the fact that there is no natural extension of the correlation objective to more than two random variables. Instead, we consider correlations between every pair of views, stack them in a J J matrix and maximize a certain matrix norm for that matrix. For GCCA, this suggests an optimization problem that maximizes the sum of correlations between a shared representation and each view. Since the objective as well as the constraints of the generalized CCA problem are very different from that of the CCA problem, it is not immediately obvious how to extend Deep CCA to Deep GCCA. Next, we show a sketch of the gradient derivation, the full derivation is given in appendix A. It is straightforward to show that the solution to the GCCA problem is given by solving an eigenvalue problem. In particular, defineCjj = f(Xj)f(Xj)> Rojoj , to be the scaled empirical covariance matrix of the jth network output, and Pj = f(Xj)>C1jj f(Xj) RNN be the corresponding projection matrix that whitens the data; note that Pj is symmetric and idempotent. We define M =J j=1 Pj . Since each Pj is positive semi-definite, so is M . Then, it is easy to check that the rows of G are the top r (orthonormal) eigenvectors of M , and Uj = C1jj f(Xj)G >. Thus, at the minimum of the objective, we can rewrite the reconstruction error as follows: J j=1 G U>j fj(Xj)2F = J j=1 GGfj(Xj)>C1jj fj(Xj) 2 F = rJ Tr(GMG>) Minimizing the GCCA objective (w.r.t. the weights of the neural networks) means maximizing Tr(GMG>), which is the sum of eigenvalues L = r i=1 i(M). Taking the derivative of L with respect to each output layer fj(Xj) we have: L fj(Xj) = 2UjG 2UjU>j fj(Xj) Thus, the gradient is the difference between the r-dimensional auxiliary representation G embedded into the subspace spanned by the columns of Uj (the first term) and the projection of the actual data in fj(Xj) onto the said subspace (the second term). Intuitively, if the auxiliary representation G is far away from the view-specific representation U>j fj(Xj), then the network weights should receive a large update. Computing the gradient descent update has time complexity O(JNrd), where d = max(d1, d2, . . . , dJ) is the largest dimensionality of the input views.",
      "exclude": false
    },
    {
      "heading": "4 EXPERIMENTS",
      "text": "",
      "exclude": false
    },
    {
      "heading": "4.1 SYNTHETIC MULTIVIEW MIXTURE MODEL",
      "text": "In this section, we apply DGCCA to a small synthetic data set to show how it preserves the generative structure of data sampled from a multiview mixture model. The data we use for this experiment are plotted in Figure 2. Points that share the same color across different views are sampled from the same mixture component. Importantly, in each view, there is no linear transformation of the data that separates the two mixture components, in the sense that the generative structure of the data could not be exploited by a linear model. This point is reinforced by Figure 3(a), which shows the two-dimensional representation G learned by applying (linear) GCCA to the data in Figure 2. The learned representation completely loses the structure of the data. We can contrast the failure of GCCA to preserve structure with the result of applying DGCCA; in this case, the input neural networks had three hidden layers with ten units each with weights randomly initialized. We plot the representation G learned by DGCCA in Figure 3 (b). In this representation, the mixture components are easily separated by a linear classifier; in fact, the structure is largely preserved even after projection onto the first coordinate of G. It is also illustrative to consider the view-specific representations learned by DGCCA, that is, to consider the outputs of the neural networks that were trained to maximize the GCCA objective. We plot the representations in Figure 4. For each view, we have learned a nonlinear mapping that does remarkably well at making the mixture components linearly separable. Recall that absolutely no direct supervision was given about which mixture component each point was generated from. The only training signals available to the networks were the reconstruction errors between the network outputs and the learned representation G.",
      "exclude": false
    },
    {
      "heading": "4.2 PHONEME CLASSIFICATION",
      "text": "In this section, we discuss experiments on the University of Wisconsin X-ray Microbeam Database (XRMB) (Westbury, 1994). XRMB contains acoustic and articulatory recordings as well as phonemic labels. We present phoneme classification results on the acoustic vectors projected using DCCA, GCCA, and DGCCA. We set acoustic and articulatory data as the two views and phoneme labels as the third view for GCCA and DGCCA. For classification, we run K-nearest neighbor classification (Cover & Hart, 1967) on the projected result.",
      "exclude": false
    },
    {
      "heading": "4.2.1 DATA",
      "text": "We use the same train/tune/test split of the data as Arora & Livescu (2014). To limit experiment runtime, we use a subset of speakers for our experiments. We run a set of cross-speaker experiments using the male speaker JW11 for training and two splits of JW24 for tuning and testing. We also perform parameter tuning for the third view with 5-fold cross validation using a single speaker, JW11. For both experiments, we use acoustic and articulatory measurements as the two views in DCCA. Following the pre-processing in Andrew et al. (2013), we get 273 and 112 dimensional feature vectors for the first and second view respectively. Each speaker has 50,000 frames. For the third view in GCCA and DGCCA, we use 39-dimensional one-hot vectors corresponding to the labels for each frame, following Arora & Livescu (2014).",
      "exclude": false
    },
    {
      "heading": "4.2.2 PARAMETERS",
      "text": "We use a fixed network size and regularization for the first two views, each containing three hidden layers with sigmoid activation functions. Hidden layers for the acoustic view were all width 1024, and layers in the articulatory view all had width 512 units. L2 penalty constants of 0.0001 and 0.01 were used to train the acoustic and articulatory view networks, respectively. The output layer dimension of each network is set to 30 for DCCA and DGCCA. For the 5-fold speaker-dependent experiments, we performed a grid search for the network sizes in 128, 256, 512, 1024 and covariance matrix regularization in 102, 104, 106, 108 for the third view in each fold. We fix the hyperparameters for these experiments optimizing the networks with minibatch stochastic gradient descent with a step size of 0.005, batch size of 2000, and no learning decay or momentum. The third view neural network had an L2 penalty of 0.0005.",
      "exclude": false
    },
    {
      "heading": "4.2.3 RESULTS",
      "text": "As we show in Table 1, DGCCA improves upon both the linear multiview GCCA and the non-linear 2-view DCCA for both the cross-speaker and speaker-dependent cross-validated tasks. In addition to accuracy, we examine the reconstruction error, i.e. the objective in Equation 3, obtained from the objective in GCCA and DGCCA.4 This sharp improvement in reconstruction error shows that a non-linear algorithm can better model the data. In this experimental setup, DCCA under-performs the baseline of simply running KNN on the original acoustic view. Prior work considered the output of DCCA stacked on to the central frame of the original acoustic view (39 dimensions). This poor performance, in the absence of original features, indicates that it was not able to find a more informative projection than original acoustic features based on correlation with the articulatory view within the first 30 dimensions. To highlight the improvements of DGCCA over GCCA, Figure 5 presents a subset of the the confusion matrices on speaker-dependent test data. In particular, we observe large improvements in the classification of D, F , K, SH , V and Y . GCCA outperforms DGCCA for UH and DH . These matrices also highlight the common misclassifications that DGCCA improves upon. For instance, 4For 2-view experiments, correlation is a common metric to compare performance. Since that metric is unavailable in a multiview setting, reconstruction error is the analogue. DGCCA rectifies the frequent misclassification of V as P , R and B by GCCA. In addition, commonly incorrect classification of phonemes such as S and T is corrected by DGCCA, which enables better performance on other voiceless consonants such as like F , K and SH . Vowels are classified with almost equal accuracy by both the methods.",
      "exclude": false
    },
    {
      "heading": "4.3 TWITTER USER HASHTAG & FRIEND RECOMMENDATION",
      "text": "Linear multiview techniques are effective at recommending hashtag and friends for Twitter users (Benton et al., 2016). In this experiment, six views of a Twitter user were constructed by applying principal component analysis (PCA) to the bag-of-words representations of (1) tweets posted by the ego user, (2) other mentioned users, (3) their friends, and (4) their followers, as well as one-hot encodings of the local (5) friend and (6) follower networks. We learn and evaluate DGCCA models on identical training, development, and test sets as Benton et al. (2016), and evaluate the DGCCA representations on macro precision at 1000 (P@1000) and recall at 1000 (R@1000) for the hashtag and friend recommendation tasks described there. We trained 40 different DGCCA model architectures, each with identical architectures across views, where the width of the hidden and output layers, c1 and c2, for each view are drawn uniformly from [10, 1000], and the auxiliary representation width r is drawn uniformly from [10, c2]5. All networks used ReLUs as activation functions, and were optimized with Adam (Kingma & Ba, 2014) for 200 epochs6. Networks were trained on 90% of 102,328 Twitter users, with 10% of users used as a tuning set to estimate heldout reconstruction error for model selection. We report development and test results for the best performing model on the downstream task development set. Learning rate was set to 104 with an L1 and L2 regularization constants of 0.01 and 0.001 for all weights 7. Table 2 displays the performance of DGCCA compared to PCA[text+net] (PCA applied to concatenation of view feature vectors), linear GCCA applied to the four text views, [text], and all 5We chose to restrict ourselves to a single hidden layer with non-linear activation and identical architectures for each view, so as to avoid a fishing expedition. If DGCCA is appropriate for learning Twitter user representations, then a good architecture should require little exploration. 6From preliminary experiments, we found that Adam pushed down reconstruction error more quickly than SGD with momentum, and that ReLUs were easier to optimize than sigmoid activations. 7This setting of regularization constants led to low reconstruction error in preliminary experiments. views, [text+net], along with a weighted GCCA variant (WGCCA). We learned PCA, GCCA, and WGCCA representations of width r 10, 20, 50, 100, 200, 300, 400, 500, 750, 1000, and report the best performing representations on the development set. There are several points to note: First is that DGCCA outperforms linear methods at hashtag recommendation by a wide margin in terms of recall. This is exciting because this task was shown to benefit from incorporating more than just two views from Twitter users. These results suggest that a nonlinear transformation of the input views can yield additional gains in performance. In addition, WGCCA models sweep over every possible weighting of views with weights in 0, 0.25, 1.0. WGCCA has a distinct advantage in that the model is allowed to discriminatively weight views to maximize downstream performance. The fact that DGCCA is able to outperform WGCCA at hashtag recommendation is encouraging, since WGCCA has much more freedom to discard uninformative views, whereas the DGCCA objective forces networks to minimize reconstruction error equally across all views. As noted in Benton et al. (2016), only the friend network view was useful for learning representations for friend recommendation (corroborated by performance of PCA applied to friend network view), so it is unsurprising that DGCCA when applied to all views cannot compete with WGCCA representations learned on the single useful friend network view8.",
      "exclude": false
    },
    {
      "heading": "5 OTHER MULTIVIEW LEARNING WORK",
      "text": "There has been strong work outside of CCA-related methods to combine nonlinear representation and learning from multiple views. Kumar et al. (2011) elegantly outlines two main approaches these methods take to learn a joint representation from many views: either by 1) explicitly maximizing pairwise similarity/correlation between views or by 2) alternately optimizing a shared, consensus representation and view-specific transformations to maximize similarity. Models such as the siamese network proposed by Masci et al. (2014), fall in the former camp, minimizing the squared error between embeddings learned from each view, leading to a quadratic increase in the terms of the loss function size as the number of views increase. Rajendran et al. (2015) extend Correlational Neural Networks (Chandar et al., 2015) to many views and avoid this quadratic explosion in the loss function by only computing correlation between each view embedding and the embedding of a pivot view. Although this model may be appropriate for tasks such as multilingual image captioning, there are many datasets where there is no clear method of choosing a pivot view. The DGCCA objective does not suffer from this quadratic increase w.r.t. the number of views, nor does it require a privileged pivot view, since the shared representation is learned from the per-view representations. Approaches that estimate a consensus representation, such as the multiview spectral clustering approach in Kumar et al. (2011), typically do so by an alternating optimization scheme which depends on a strong initialization to avoid bad local optima. The GCCA objective our work builds on is particularly attractive, since it admits a globally optimal solution for both the view-specific projections U1 . . . UJ , and the shared representation G by singular value decomposition of a single matrix: a sum of the per-view projection matrices. Local optima arise in the DGCCA objective only because we are also learning nonlinear transformations of the input views. Nonlinear multiview methods often avoid learning these nonlinear transformations by assuming that a kernel or graph Laplacian (e.g. in multiview clustering) is given (Kumar et al., 2011; Xiaowen, 2014; Sharma et al., 2012).",
      "exclude": false
    },
    {
      "heading": "6 CONCLUSION",
      "text": "We present DGCCA, a method for non-linear multiview representation learning from an arbitrary number of views. We show that DGCCA clearly outperforms prior work when using labels as a third view (Andrew et al., 2013; Arora & Livescu, 2014; Wang et al., 2015c), and can successfully exploit multiple views to learn user representations useful for downstream tasks such as hashtag recommendation for Twitter users. To date, CCA-style multiview learning techniques were either restricted to learning representations from no more than two views, or strictly linear transformations of the input views. This work overcomes these limitations. 8The performance of WGCCA suffers compared to PCA because whitening the friend network data ignores the fact that the spectrum of the decays quickly with a long tail the first few principal components made up a large portion of the variance in the data, but it was also important to compare users based on other components.",
      "exclude": true
    },
    {
      "heading": "APPENDIX A DERIVING THE GCCA OBJECTIVE GRADIENT",
      "text": "In order to train the neural networks in DGCCA, we need to compute the gradient of the GCCA objective with respect to any one of its input views. This gradient can then be backpropagated through the input networks to derive updates for the network weights. Let N be the number of data points and J the number of views. Let Yj Rc j KN be the data matrix representing the output of the jth neural network, i.e. Yj = fj(Xj), where c j K is the number of neurons in the output layer of the jth network. Then, GCCA can be written as the following optimization problem, where r is the dimensionality of the learned auxiliary representation: minimize UjRc j K r,GRrN J j=1 G U>j Yj2F subject to GG> = Ir It can be shown that the solution is found by solving a certain eigenvalue problem. In particular, define Cjj = YjY >j Rc j Kc j K , Pj = Y >j C 1 jj Yj (note that Pj is symmetric and idempotent), and M = J j=1 Pj (since each Pj is psd, so is M ). Then the rows of G are the top r (orthonormal) eigenvectors of M , and Uj = C1jj YjG >. Thus, at the minima of the objective, we can rewrite the reconstruction error as follows: J j=1 G U>j Yj2F = J j=1 GGY >j C1jj Yj 2 F = J j=1 G(IN Pj)2F = J j=1 Tr[G(IN Pj)G>] = J j=1 Tr(Ir) Tr(GMG>) = Jr Tr(GMG>) Note that we can write the rank-1 decomposition of M as N k=1 kgkg > k . Furthermore, since the kth row of G is gk, and since the matrix product Ggk = ek, GMG> = N k=1 kGgk(Ggk) > = r k=1 keke > k But this is just an N N diagonal matrix containing the top r eigenvalues of M , so we can write the GCCA objective as Jr r i=1 i(M) Thus, minimizing the GCCA objective (w.r.t. the weights of the neural nets) means maximizing the sum of eigenvalues r i=1 i(M), which we will henceforth denote by L. Now, we will derive an expression for LYj for any view Yj . First, by the chain rule, and using the fact that LM = G >G (Petersen & Pedersen, 2012), L (Yj)ab = N c,d=1 L Mcd Mcd (Yj)ab = N c,d=1 (G>G)cd Mcd (Yj)ab Since M = J j=1 Pj , and since the only projection matrix that depends on Yj is Pj , M Yj = Pj Yj . Since Pj = Y >j C 1 jj Yj , (Pj)cd = cjK k,`=1 (Yj)kc(C 1 jj )k`(Yj)`d Thus, by the product rule, (Pj)cd (Yj)ab = cb cjK `=1 (Yj)`d(C 1 jj )a` + db cjK k=1 (Yj)kc(C 1 jj )ka + cjK k,`=1 (Yj)kc(Yj)`d (C1jj )k` (Yj)ab = cb(C 1 jj Yj)ad + db(C 1 jj Yj)ac + cjK k,`=1 (Yj)kc(Yj)`d (C1jj )k` (Yj)ab The derivative in the last term can also be computed using the chain rule: (C1jj )k` (Yj)ab = N m,n=1 (C1jj )k` (Cjj)mn (Cjj)mn (Yj)ab = N m,n=1 (C1jj )km(C 1 jj )n` [am(Yj)nb + an(Yj)mb] = N n=1 (C1jj )ka(C 1 jj )n`(Yj)nb N m=1 (C1jj )km(C 1 jj )a`(Yj)mb = (C1jj )ka(C 1 jj Yj)`b (C1jj )a`(C 1 jj Yj)kb Substituting this into the expression for (Pj)cd(Yj)ab and simplifying matrix products, we find that (Pj)cd (Yj)ab = cb(C 1 jj Yj)ad + db(C 1 jj Yj)ac (C1jj Yj)ac(Y > j C 1 jj Yj)bd (C1jj Yj)ad(Y > j C 1 jj Yj)bc = (IN Pj)cb(C1jj Yj)ad + (IN Pj)db(C1jj Yj)ac Finally, substituting this into our expression for L(Yj)ab , we find that L (Yj)ab = N c,d=1 (G>G)cd(IN Pj)cb(C1jj Yj)ad + N c,d=1 (G>G)cd(IN Pj)db(C1jj Yj)ac = 2[C1jj YjG >G(IN Pj)]ab Therefore, L Yj = 2C1jj YjG >G(IN Pj) But recall that Uj = C1jj YjG >. Using this, the gradient simplifies as follows: L Yj = 2UjG 2UjU>j Yj Thus, the gradient is the difference between the r-dimensional auxiliary representation G embedded into the subspace spanned by the columns of Uj (the first term) and the projection of the network outputs in Yj = fj(Xj) onto said subspace (the second term). Intuitively, if the auxiliary representation G is far away from the view-specific representation U>j fj(Xj), then the network weights should receive a large update.",
      "exclude": true
    },
    {
      "heading": "APPENDIX B DGCCA OPTIMIZATION PSEUDOCODE",
      "text": "Algorithm 1 contains the pseudocode for the DGCCA optimization algorithm. In practice we use stocastic optimization with minibatches, following Wang et al. (2015c). Algorithm 1 Deep Generalized CCA Input: multiview data: X1, X2, . . . , XJ , number of iterations T , learning rate Output: O1, O2, . . . , OJ Initialize weights W1,W2, . . . ,WJ for iteration t = 1, 2, . . . , T do for each view j = 1, 2, . . . , J do Oj forward pass of Xj with weights Wj mean-center Oj end for U1, . . . , UJ , G gcca(O1, . . . , OJ ) for each view j = 1, 2, . . . , J do F/Oj UjU>j Oj UjG Wj backprop(F/Oj ,Wj) Wj Wj Wj end for end for for each view j = 1, 2, . . . , J do Oj forward pass of Xj with weights Wj mean-center Oj end for U1, . . . , UJ , G gcca(O1, . . . , OJ ) for each view j = 1...J do Oj U>j Oj end for",
      "exclude": true
    },
    {
      "heading": "APPENDIX C RECONSTRUCTION ERROR AND DOWNSTREAM",
      "text": "PERFORMANCE CCA methods are typically evaluated intrinsically by the amount of correlation captured, or reconstruction error. These measures are dependent on the width of the shared embeddings and viewspecific output layers, and do not necessarily predict downstream performance. Although reconstruction error cannot solely be relied on for model selection for a downstream task, we found that it was a useful as a signal to weed out very poor models. Figure 6 shows the reconstruction error against hashtag prediction Recall at 1000 for an initial grid search of DGCCA hyperparameters. Models with tuning reconstruction error greater than 103 can safely be ignored, while there is some variability in the performance of models with achieving lower error. Since a DGCCA model with high reconstruction error suggests that the views do not agree with each other at all, it makes sense that the shared embedding will likely be noisy, whereas a relatively lowly reconstruction error suggests that the transformed views have converged to a stable solution.",
      "exclude": true
    }
  ],
  "fewShot": [
    {
      "model": "gpt-5.2",
      "decision": {
        "rejection": false,
        "confidence": 0.78,
        "primary_reason": "Strong, clearly novel extension of CCA/GCCA to the deep + many-view setting with a nontrivial objective/gradient derivation and scalable stochastic optimization, supported by solid experiments on real multiview datasets showing consistent gains over key baselines (DCCA, GCCA, WGCCA) and a convincing synthetic sanity check."
      },
      "token": {
        "prompt_tokens": 7660,
        "completion_tokens": 93,
        "total_tokens": 7753
      },
      "time": "2026-02-09T20:40:19.650358+00:00"
    },
    {
      "model": "claude-haiku-4-5",
      "decision": {
        "rejection": false,
        "confidence": 0.72,
        "primary_reason": "Combines nonlinear deep learning with multi-view CCA in a principled way, addresses a clear gap (DCCA handles 2 views, GCCA is linear), derives gradient computation for constrained optimization, and demonstrates improvements on phoneme classification and hashtag recommendation tasks with reasonable experimental validation."
      },
      "token": {
        "prompt_tokens": 8612,
        "completion_tokens": 100,
        "total_tokens": 8712
      },
      "time": "2026-02-11T19:13:07.197080+00:00"
    }
  ],
  "test": false
}