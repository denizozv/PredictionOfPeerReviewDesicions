{
  "id": "641",
  "title": "Two Methods for Wild Variational Inference",
  "abstract": "Variational inference provides a powerful tool for approximate probabilistic inference on complex, structured models. Typical variational inference methods, however, require to use inference networks with computationally tractable probability density functions. This largely limits the design and implementation of variational inference methods. We consider wild variational inference methods that do not require tractable density functions on the inference networks, and hence can be applied in more challenging cases. As an example of application, we treat stochastic gradient Langevin dynamics (SGLD) as an inference network, and use our methods to automatically adjust the step sizes of SGLD to maximize its convergence speed, significantly outperforming the hand-designed step size schemes.",
  "accepted": false,
  "zeroShot": [
    {
      "model": "claude-haiku-4-5",
      "decision": {
        "rejection": false,
        "confidence": 0.72,
        "primary_reason": "Novel framework for variational inference without tractable densities using Stein discrepancy and SVGD. Solid theoretical foundation with practical applications to SGLD step-size optimization. However, limited experimental validation, modest sample sizes in some experiments, and presentation could be clearer. Methodologically sound but incremental contribution combining existing techniques (SVGD, KSD, amortization)."
      },
      "token": {
        "prompt_tokens": 6396,
        "completion_tokens": 121,
        "total_tokens": 6517
      },
      "time": "2026-02-05T23:07:14.985344+00:00"
    }
  ],
  "article": [
    {
      "heading": "1 INTRODUCTION",
      "text": "Probabilistic modeling provides a principled approach for reasoning under uncertainty, and has been increasingly dominant in modern machine learning where highly complex, structured probabilistic models are often the essential components for solving complex problems with increasingly larger datasets. A key challenge, however, is to develop computationally efficient Bayesian inference methods to approximate, or draw samples from the posterior distributions. Variational inference (VI) provides a powerful tool for scaling Bayesian inference to complex models and big data. The basic idea of VI is to approximate the true distribution with a simpler distribution by minimizing the KL divergence, transforming the inference problem into an optimization problem, which is often then solved efficiently using stochastic optimization techniques (e.g., Hoffman et al., 2013; Kingma & Welling, 2013). However, the practical design and application of VI are still largely restricted by the requirement of using simple approximation families, as we explain in the sequel. Let p(z) be a distribution of interest, such as the posterior distribution in Bayesian inference. VI approximates p(z) with a simpler distribution q(z) found in a set Q = q(z) of distributions indexed by parameter by minimizing the KL divergence objective: min KL(q || p) Ezq [log(q(z)/p(z))] , (1) where we can get exact result p = q if Q is chosen to be broad enough to actually include p. In practice, however, Q should be chosen carefully to make the optimization in (1) computationally tractable; this casts two constraints on Q: 1. A minimum requirement is that we should be able to sample from q efficiently, which allows us to make estimates and predictions based on q in placement of the more intractable p. The samples from q can also be used to approximate the expectation Eq[] in (1) during optimization. This means that there should exist some computable function f(; ), called the inference network, which takes a random seed , whose distribution is denoted by q0, and outputs a random variable z = f(; ) whose distribution is q . 2. We should also be able to calculate the density q(z) or it is derivative in order to optimize the KL divergence in (1). This, however, casts a much more restrictive condition, since it requires us to use only simple inference network f(; ) and input distributions q0 to ensure a tractable form for the density q of the output z = f(; ). In fact, it is this requirement of calculating q(z) that has been the major constraint for the design of state-of-the-art variational inference methods. The traditional VI methods are often limited to using simple mean field, or Gaussian-based distributions as q and do not perform well for approximating complex target distributions. There is a line of recent work on variational inference with rich approximation families (e.g., Rezende & Mohamed, 2015b; Tran et al., 2015; Ranganath et al., 2015, to name only a few), all based on handcrafting special inference networks to ensure the computational tractability of q(z) while simultaneously obtaining high approximation accuracy. These approaches require substantial mathematical insights and research effects, and can be difficult to understand or use for practitioners without a strong research background in VI. Methods that allow us to use arbitrary inference networks without substantial constraints can significantly simplify the design and applications of VI methods, allowing practical users to focus more on choosing proposals that work best with their specific tasks. We use the term wild variational inference to refer to variants of variational methods working with general inference networks f(, ) without tractability constraints on its output density q(z); this should be distinguished with the black-box variational inference (Ranganath et al., 2014) which refers to methods that work for generic target distributions p(z) without significant model-by-model consideration (but still require to calculate the proposal density q(z)). Essentially, wild variational inference makes it possible to learn to draw samples, constructing black-box neural samplers for given distributions. This enables more adaptive and automatic design of efficient Bayesian inference procedures, replacing the hand-designed inference algorithms with more efficient ones that can improve their efficiency adaptively over time based on past tasks they performed. In this work, we discuss two methods for wild variational inference, both based on recent works that combine kernel techniques with Steins method (e.g., Liu & Wang, 2016; Liu et al., 2016). The first method, also discussed in Wang & Liu (2016), is based on iteratively adjusting parameter to make the random output z = f(; ) mimic a Stein variational gradient direction (SVGD) (Liu & Wang, 2016) that optimally decreases its KL divergence with the target distribution. The second method is based on minimizing a kernelized Stein discrepancy, which, unlike KL divergence, does not require to calculate density q(z) for the optimization thanks to its special form. Another critical problem is to design good network architectures well suited for Bayesian inference. Ideally, the network design should leverage the information of the target distribution p(z) in a convenient way. One useful perspective is that we can view the existing MC/MCMC methods as (hand-designed) stochastic neural networks which can be used to construct native inference networks for given target distributions. On the other hand, using existing MC/MCMC methods as inference networks also allow us to adaptively adjust the hyper-parameters of these algorithms; this enables amortized inference which leverages the experience on past tasks to accelerate the Bayesian computation, providing a powerful approach for designing efficient algorithms in settings when a large number of similar tasks are needed. As an example, we leverage stochastic gradient Langevin dynamics (SGLD) (Welling & Teh, 2011) as the inference network, which can be treated as a special deep residential network (He et al., 2016), in which important gradient information z log p(z) is fed into each layer to allow efficient approximation for the target distribution p(z). In our case, the network parameter are the step sizes of SGLD, and our method provides a way to adaptively improve the step sizes, providing speed-up on future tasks with similar structures. We show that the adaptively estimated step sizes significantly outperform the hand-designed schemes such as Adagrad. Related Works The idea of amortized inference (Gershman & Goodman, 2014) has been recently applied in various domains of probabilistic reasoning, including both amortized variational inference (e.g., Kingma & Welling, 2013; Rezende & Mohamed, 2015a) and date-driven designs of Monte Carlo based methods (e.g., Paige & Wood, 2016), to name only a few. Most of these methods, however, require to explicitly calculate q(z) (or its gradient). One well exception is a very recent work (Ranganath et al., 2016) that also avoids calculating q(z) and hence works for general inference networks; their method is based on a similar idea related to Stein discrepancy (Liu et al., 2016; Oates et al., 2017; Chwialkowski et al., 2016; Gorham & Mackey, 2015), for which we provide a more detailed discussion in Section 3.2. The auxiliary variational inference methods (e.g., Agakov & Barber, 2004) provide an alternative way when the variational distribution q(z) can be represented as a hidden variable model. In particular, Salimans et al. (2015) used the auxiliary variational approach to leverage MCMC as a variational approximation. These approaches, however, still require to write down the likelihood function on the augmented spaces, and need to introduce an additional inference network related to the auxiliary variables. There is a large literature on traditional adaptive MCMC methods (e.g., Andrieu & Thoms, 2008; Roberts & Rosenthal, 2009) which can be used to adaptively adjust the proposal distribution of MCMC by exploiting the special theoretical properties of MCMC (e.g., by minimizing the autocorrelation). Our method is simpler, more generic, and works efficiently in practice thanks to the use of gradient-based back-propagation. Finally, connections between stochastic gradient descent and variational inference have been discussed and exploited in Mandt et al. (2016); Maclaurin et al. (2015). Outline Section 2 introduces background on Stein discrepancy and Stein variational gradient descent. Section 3 discusses two methods for wild variational inference. Section 4 discuss using stochastic gradient Langevin dynamics (SGLD) as the inference network. Empirical results are shown in Section 5.",
      "exclude": true
    },
    {
      "heading": "2 STEIN’S IDENTITY, STEIN DISCREPANCY, STEIN VARIATIONAL GRADIENT",
      "text": "Steins identity Steins identity plays a fundamental role in our framework. Let p(z) be a positive differentiable density on Rd, and (z) = [1(z), , d(z)]> is a differentiable vector-valued function. Definez = i zi. Steins identity is Ezp[z log p(z), (z)+z (z)] = X z (p(z)(z))dx = 0, (2) which holds once p(z)(z) vanishes on the boundary of X by integration by parts or Stokes theorem; It is useful to rewrite Steins identity in a more compact way: Ezp[Tp(z)] = 0, with Tp def = z log p, +z , (3) where Tp is called a Stein operator, which acts on function and returns a zero-mean function Tp(z) under z p. A key computational advantage of Steins identity and Stein operator is that they depend on p only through the derivative of the log-density z log p(z), which does not depend on the cumbersome normalization constant of p, that is, when p(z) = p(z)/Z, we have z log p(z) = z log p(z), independent of the normalization constant Z. This property makes Steins identity a powerful practical tool for handling unnormalized distributions widely appeared in machine learning and statistics. Stein Discrepancy Although Steins identity ensures that Tp has zero expectation under p, its expectation is generally non-zero under a different distribution q. Instead, for p 6= q, there must exist a which distinguishes p and q in the sense that Ezq[Tp(z)] 6= 0. Stein discrepancy leverages this fact to measure the difference between p and q by considering the maximum violation of Steins identity for in certain function set F : D(q || p) = max F Ezq[Tp(z)] , (4) where F is the set of functions that we optimize over, and decides both the discriminative power and computational tractability of Stein discrepancy. Kernelized Stein discrepancy (KSD) is a special Stein discrepancy that takes F to be the unit ball of vector-valued reproducing kernel Hilbert spaces (RKHS), that is, F = Hd : ||||Hd 1, (5) where H is a real-valued RKHS with kernel k(z, z). This choice of F makes it possible to get a closed form solution for the optimization in (4) (Liu et al., 2016; Chwialkowski et al., 2016; Oates et al., 2017): D(q || p) = max Hd Ezq[Tp(z)], s.t. ||||Hd 1 , (6) = Ez,zq[p(z, z)], (7) where p(z, z) is a positive definite kernel obtained by applying Stein operator on k(z, z) twice: p(z, z ) = T z p (T zp k(z, z)), = sp(z)sp(z )k(z, z) + sp(z)zk(z, z) + sp(z)zk(z, z) +z (zk(z, z)), (8) where sp(z) = z log p(z) and T zp and T zp denote the Stein operator when treating k(z, z) as a function of z and z, respectively; here we defined T zp k(z, z) = x log p(x)k(z, z)+xk(z, z) which returns a d 1 vector-valued function. It can be shown that D(q || p) = 0 if and only if q = p when k(z, z) is strictly positive definite in a proper sense (Liu et al., 2016; Chwialkowski et al., 2016). D(q || p) can treated as a variant of maximum mean discrepancy equipped with kernel p(z, z ) which depends on p (which makes D(q || p) asymmetric on q and p). The form of KSD in (6) allows us to estimate the discrepancy between a set of sample zi (e.g., drawn from q) and a distribution p specified byz log p(z), D2u(zi || p) = 1 n(n 1) i6=j [p(zi, zj)], D2v(zi || p) = 1 n2 i,j [p(zi, zj)], (9) where D2u(q || p) provides an unbiased estimator (hence called a U -statistic) for D2(q || p), and D2v(q || p), called V -statistic, provides a biased estimator but is guaranteed to be always nonnegative: D2v(zi || p) 0. Stein Variational Gradient Descent (SVGD) Stein operator and Stein discrepancy have a close connection with KL divergence, which is exploited in Liu & Wang (2016) to provide a general purpose deterministic approximate sampling method. Assume that zini=1 is a sample (or a set of particles) drawn from q, and we want to update zini=1 to make it move closer to the target distribution p to improve the approximation quality. We consider updates of form zi zi + (zi), i = 1, . . . , n, (10) where is a perturbation direction, or velocity field, chosen to maximumly decrease the KL divergence between the distribution of updated particles and the target distribution, in the sense that = arg max F d d KL(q[ ] || p) =0 , (11) where q[ ] denotes the density of the updated particle z = z + (z) when the density of the original particle z is q, and F is the set of perturbation directions that we optimize over. A key observation (Liu & Wang, 2016) is that the optimization in (11) is in fact equivalent to the optimization for KSD in (4); we have d d KL(q[ ] || p) =0 = Ezq[Tp(z)], (12) that is, the Stein operator transforms the perturbation on the random variable (the particles) to the change of the KL divergence. Taking F to be unit ball of Hd as in (5), the optimal solution of (11) equals that of (6), which is shown to be (e.g., Liu et al., 2016) (z) Ezq[T zp k(z, z)] = Ezq[z log p(z)k(z, z) +zk(z, z)]. Algorithm 1 Amortized SVGD and KSD Minimization for Wild Variational Inference for iteration t do 1. Draw random ini=1, calculate zi = f(; i), and the Stein variational gradient zi in (13). 2. Update parameter using (14) or (15) for amortized SVGD, or (17) for KSD minimization. end for By approximating the expectation under q with the empirical mean of the current particles zini=1, SVGD admits a simple form of update that iteratively moves the particles towards the target distribution, zi zi + zi, i = 1, . . . , n, zi = Ezzini=1 [z log p(z)k(z, zi) +zk(z, zi)], (13) where Ezzini=1 [f(z)] = i f(zi)/n. The two terms in zi play two different roles: the term with the gradient z log p(z) drives the particles towards the high probability regions of p(z), while the term with zk(z, zi) serves as a repulsive force to encourage diversity; to see this, consider a stationary kernel k(z, z) = k(z z), then the second term reduces to Ezzk(z, zi) = Ezzik(z, zi), which can be treated as the negative gradient for minimizing the average similarity Ezk(z, zi) in terms of zi. It is easy to see from (13) that zi reduces to the typical gradient z log p(zi) when there is only a single particle (n = 1) and zk(z, zi) when z = zi, in which case SVGD reduces to the standard gradient ascent for maximizing log p(z) (i.e., maximum a posteriori (MAP)).",
      "exclude": false
    },
    {
      "heading": "3 TWO METHODS FOR WILD VARIATIONAL INFERENCE",
      "text": "Since the direct parametric optimization of the KL divergence (1) requires calculating q(z), there are two essential ways to avoid calculating q(z): either using alternative (approximate) optimization approaches, or using different divergence objective functions. We discuss two possible approaches in this work: one based on amortizing SVGD (Wang & Liu, 2016) which trains the inference network f(, ) so that its output mimic the SVGD dynamics in order to decrease the KL divergence; another based on minimizing the KSD objective (9) which does not require to evaluate q(z) thanks to its special form.",
      "exclude": false
    },
    {
      "heading": "3.1 AMORTIZED SVGD",
      "text": "SVGD provides an optimal updating direction to iteratively move a set of particles zi towards the target distribution p(z). We can leverage it to train an inference network f(; ) by iteratively adjusting so that the output of f(; ) changes along the Stein variational gradient direction in order to maximumly decrease its KL divergence with the target distribution. By doing this, we amortize SVGD into a neural network, which allows us to leverage the past experience to adaptively improve the computational efficiency and generalize to new tasks with similar structures. Amortized SVGD is also presented in Wang & Liu (2016); here we present some additional discussion. To be specific, assume i are drawn from q0 and zi = f(; i) the corresponding random output based on the current estimation of . We want to adjust so that zi changes along the Stein variational gradient direction zi in (13) so as to maximumly decrease the KL divergence with target distribution. This can be done by updating via arg min n i=1 ||f(; i) zi zi||22. (14) Essentially, this projects the non-parametric perturbation direction zi to the change of the finite dimensional network parameter . If we take the step size to be small, then the updated by (14) should be very close to the old value, and a single step of gradient descent of (14) can provide a good approximation for (14). This gives a simpler update rule: + i f(; i)zi, (15) which can be intuitively interpreted as a form of chain rule that back-propagates the SVGD gradient to the network parameter . In fact, when we have only one particle, (15) reduces to the standard gradient ascent for max log p(f(; )), in which f is trained to learn to optimize (e.g., Andrychowicz et al., 2016), instead of learn to sample p(z). Importantly, as we have more than one particles, the repulsive term zk(z, zi) in zi becomes active, and enforces an amount of diversity on the network output that is consistent with the variation in p(z). The full algorithm is summarized in Algorithm 1. Amortized SVGD can be treated as minimizing the KL divergence using a rather special algorithm: it leverages the non-parametric SVGD which can be treated as approximately solving the infinite dimensional optimization minq KL(q || p) without explicitly assuming a parametric form on q, and iteratively projecting the non-parametric update back to the finite dimensional parameter space of . It is an interesting direction to extend this idea to amortize other MC/MCMC-based inference algorithms. For example, given a MCMC with transition probability T (z|z) whose stationary distribution is p(z), we may adjust to make the network output move towards the updated values z drawn from the transition probability T (z|z). The advantage of using SVGD is that it provides a deterministic gradient direction which we can back-propagate conveniently and is particle efficient in that it reduces to learning to optimize with a single particle. We have been using the simple L2 loss in (14) mainly for convenience; it is possible to use other two-sample discrepancy measures such as maximum mean discrepancy.",
      "exclude": false
    },
    {
      "heading": "3.2 KSD VARIATIONAL INFERENCE",
      "text": "Amortized SVGD attends to minimize the KL divergence objective, but can not be interpreted as a typical finite dimensional optimization on parameter . Here we provide an alternative method based on directly minimizing the kernelized Stein discrepancy (KSD) objective, for which, thanks to its special form, the typical gradient-based optimization can be performed without needing to estimate q(z) explicitly. To be specific, take q to be the density of the random output z = f(; ) when q0, and we want to find to minimize D(q || p). Assuming i is i.i.d. drawn from q0, we can approximate D2(q || p) unbiasedly with a U-statistics: D2(q || p) 1 n(n 1) i 6=j p(f(; i), f(; j)), (16) for which a standard gradient descent can be derived for optimizing : 2 n(n 1) i 6=j f(; i)zip(zi, zj), where zi = f(; i). (17) This enables a wild variational inference method based on directly minimizing with standard (stochastic) gradient descent. See Algorithm 1. Note that (17) is similar to (15) in form, but replaces zi with a zi j : i 6=j zip(zi, zj). It is also possible to use the V -statistic in (9), but we find that the U -statistic performs much better in practice, possibly because of its unbiasedness property. Minimizing KSD can be viewed as minimizing a constrastive divergence objective function. To see this, recall that q[ ] denotes the density of z = z + (z) when z q. Combining (11) and (6), we can show that D2(q || p) 1 (KL(q || p)KL(q[ ] || p)). That is, KSD measures the amount of decrease of KL divergence when we update the particles along the optimal SVGD perturbation direction given by (11). If q = p, then the decrease of KL divergence equals zero and D2(q || p) equals zero. In fact, as shown in Liu & Wang (2016) KSD can be explicitly represented as the magnitude of a functional gradient of KL divergence: D(q || p) = d d KL(q[] || p) =0 Hd , where q[] is the density of z = z +(z) when z q, and ddF () denotes the functional gradient of functional F () w.r.t. defined in RKHSHd, and ddF () is also an element inH d. Therefore, KSD variational inference can be treated as explicitly minimizing the magnitude of the gradient of KL divergence, in contract with amortized SVGD which attends to minimize the KL divergence objective itself. This idea is also similar to the contrastive divergence used for learning restricted Boltzmann machine (RBM) (Hinton, 2002) (which, however, optimizes p with fixed q). It is possible to extend this approach by replacing z = z + (z) with other transforms, such as these given by a transition probability of a Markov chain whose stationary distribution is p. In fact, according the so called generator method for constructing Stein operator (Barbour, 1988), any generator of a Markov process defines a Stein operator that can be used to define a corresponding Stein discrepancy. This idea is related to a very recent work by Ranganath et al. (2016), which is based on directly minimizing the variational form of Stein discrepancy in (4); Ranganath et al. (2016) assumes F consists of a neural network (z) parametrized by , and find by solving the following min-max problem: min max Ezq[Tp (z)]. In contrast, our method leverages the closed form solution by taking F to be an RKHS and hence obtains an explicit optimization problem, instead of a min-max problem that can be computationally more expensive, or have difficulty in achieving convergence. Because p(x, x) (defined in (8)) depends on the derivative x log p(x) of the target distribution, the gradient in (17) depends on the Hessian matrix 2x log p(x) and is hence less convenient to implement compared with amortized SVGD (the method by Ranganath et al. (2016) also has the same problem). However, this problem can be alleviated using automatic differentiation tools, which be used to directly take the derivative of the objective in (16) without manually deriving its derivatives.",
      "exclude": false
    },
    {
      "heading": "4 LANGEVIN INFERENCE NETWORK",
      "text": "With wild variational inference, we can choose more complex inference network structures to obtain better approximation accuracy. Ideally, the best network structure should leverage the special properties of the target distribution p(z) in a convenient way. One way to achieve this by viewing existing MC/MCMC methods as inference networks with hand-designed (and hence potentially suboptimal) parameters, but good architectures that take the information of the target distribution p(z) into account. By applying wild variational inference on networks constructed based on existing MCMC methods, we effectively provide an hyper-parameter optimization for these existing methods. This allows us to fully optimize the potential of existing Bayesian inference methods, significantly improving the result with less computation cost, and decreasing the need for hyper-parameter tuning by human experts. This is particularly useful when we need to solve a large number of similar tasks, where the computation cost spent on optimizing the hyper-parameters can significantly improve the performance on the future tasks. Stochastic Gradient Langevin Dynamics We first take the original stochastic gradient Langevin dynamics (SGLD) algorithm (Welling & Teh, 2011) as an example. SGLD starts with a random initialization z0, and perform iterative update of form zt+1 zt + t z log p(zt; Mt) + 2t t, t = 1, T, (18) where log p(zt; Mt) denotes an approximation of log p(zt) based on, e.g., a random mini-batch Mt of observed data at t-th iteration, and t is a standard Gaussian random vector of the same size as z, and t denotes a (vector) step-size at t-th iteration; here denotes element-wise product. When running SGLD for T iterations, we can treat zT as the output of a T -layer neural network parametrized by the collection of step sizes = tTt=1, whose random inputs include the random initialization z0, the mini-batch Mt and Gaussian noise t at each iteration t. We can see that this defines a rather complex network structure with several different types of random inputs (z0, Mt and t). This makes it intractable to explicitly calculate the density of zT and traditional variational inference methods can not be applied directly. But wild variational inference can still allow us to adaptively improve the optimal step-size in this case. General Langevin Networks Based on the original formula of SGLD, we proposed a more general langevin network structure, and each layer of the network has a form zt+1 Atzt + h(BtBt>z log p(zt; Mt) +Btt +Dt), t = 1, T, (19) where At, Bt and Dt are network parameters at t-th iteration(whose size is d d, and d is the size of zt), and h() denotes a smooth element-wise non-linearity function; here t is still a standard gaussian random vector with the same size as z. With this more complex network, we can use fewer layers to construct more powerful back-box samplers.",
      "exclude": false
    },
    {
      "heading": "5 EMPIRICAL RESULTS",
      "text": "",
      "exclude": false
    },
    {
      "heading": "5.1 SGLD INFERENCE NETWORK",
      "text": "We first test our algorithm with SGLD inference network with (18) formula on both a toy Gaussian mixture model and a Bayesian logistic regression example. We find that we can adaptively learn step sizes that significantly outperform the existing hand-designed step size schemes, and hence save computational cost in the testing phase. In particular, we compare with the following step size schemes, for all of which we report the best results (testing accuracy in Figure 3(a); testing likelihood in Figure 3(b)) among a range of hyper-parameters: 1. Constant Step Size. We select a best constant step size in 1, 2, 23, . . . , 229 106. 2. Power Decay Step Size. We consider t = 10a (b + t) where = 0.55, a 6,5, . . . , 1, 2, b 0, 1, . . . , 9. 3. Adagrad, Rmsprop, Adadelta, all with the master step size selected in 1, 2, 23, . . . , 229 106, with the other parameters chosen by default values. Gaussian Mixture We start with a simple 1D Gaussian mixture example shown in Figure 2 where the target distribution p(z) is shown by the red dashed curve. We use amortized SVGD and KSD to optimize the step size parameter of the Langevin inference network in (18) with T = 20 layers (i.e., SGLD with T = 20 iterations), with an initial z0 drawn from a q0 far away from the target distribution (see the green curve in Figure 2(a)); this makes it critical to choose a proper step size to achieve close approximation within T = 20 iterations. We find that amortized SVGD and KSD allow us to achieve good performance with 20 steps of SGLD updates (Figure 2(b)-(c)), while the result of the best constant step size and power decay step-size are much worse (Figure 2(d)-(e)). Bayesian Logistic Regression We consider Bayesian logistic regression for binary classification using the same setting as Gershman et al. (2012), which assigns the regression weights w with a Gaussian prior p0(w|) = N (w,1) and p0() = Gamma(, 1, 0.01). The inference is applied on the posterior of z = [w, log]. We test this model on the binary Covertype dataset1 with 581,012 data points and 54 features. To demonstrate that our estimated learning rate can work well on new datasets never seen by the algorithm. We partition the dataset into mini-datasets of size 50, 000, and use 80% of them for training and 20% for testing. We adapt our amortized SVGD/KSD to train on the whole population of the training mini-datasets by randomly selecting a mini-dataset at each iteration of Algorithm 1, and evaluate the performance of the estimated step sizes on the remaining 20% testing mini-datasets. Figure 3 reports the testing accuracy and likelihood on the 20% testing mini-datasets when we train the Langevin network with T = 10, 50, 100 layers, respectively. We find that our methods outperform all the hand-designed learning rates, and allow us to get performance closer to the fully converged SGLD and SVGD with a small number T of iterations. Figure 4 shows the testing accuracy and testing likelihood of all the intermediate results when training Langevin network with T = 100 layers. It is interesting to observe that amortized SVGD and KSD learn rather different behavior: KSD tends to increase the performance quickly at the first few iterations but saturate quickly, while amortized SVGD tends to increase slowly in the beginning and boost the performance quickly in the last few iterations. Note that both algorithms are set up to optimize the performance of the last layers, while need to decide how to make progress on the intermediate layers to achieve the best final performance.",
      "exclude": false
    },
    {
      "heading": "5.2 GENERAL LANGEVIN INFERENCE NETWORK",
      "text": "We further test our algorithm with general Langevin inference network. We firstly construct one single layer general Langevin network to approach the posterior of Bayesian logistic regression parameters and we can achieve 74.58% average accuracy and 0.5216 average testing log-likelihood in 100 repeat experiments. This result proves the proposed general Langevin Inference Network is quite competitive and worth to explore. Moreover, we use it as a black-box sampler to approach more complicate Gaussian Mixture distributions. Gaussian Mixture We consider 10 components Gaussian Mixture Models with mean and covariance matrix of each component randomly drawed from a uniform distribution, and we test our methods on different dimensions models. We construct 6 layers of general Langevin networks as a black-box sampler, and our proposed two methods to train the black-box sampler to approximate the target distribution. Figure 5 shows our 1https://www.csie.ntu.edu.tw/ cjlin/libsvmtools/datasets/binary.html results on 50 dimension Gaussian Mixture case and figure 6 shows results of different dimensions of Gaussian Mixture. From the figures we can know that our proposed sampling structure is quite competive comparing with NUT sampler(Hoffman & Gelman, 2014), and these two variational inference methods can both train a good black-box sampler.",
      "exclude": false
    },
    {
      "heading": "6 CONCLUSION",
      "text": "We consider two methods for wild variational inference that allows us to train general inference networks with intractable density functions, and apply it to adaptively estimate step sizes of stochastic gradient Langevin dynamics. More studies are needed to develop better methods, more applications and theoretical understandings for wild variational inference, and we hope that the two methods we discussed in the paper can motivate more ideas and studies in the field.",
      "exclude": true
    }
  ],
  "fewShot": [
    {
      "model": "claude-haiku-4-5",
      "decision": {
        "rejection": false,
        "confidence": 0.72,
        "primary_reason": "Novel framework for variational inference without tractable densities using Stein discrepancy and amortized SVGD; solid theoretical grounding and practical application to adaptive SGLD step-size optimization with reasonable experimental validation."
      },
      "token": {
        "prompt_tokens": 6558,
        "completion_tokens": 84,
        "total_tokens": 6642
      },
      "time": "2026-02-08T22:37:17.875101+00:00"
    },
    {
      "model": "gpt-5.2",
      "decision": {
        "rejection": true,
        "confidence": 0.78,
        "primary_reason": "Interesting use of Stein/KSD for “wild” variational inference and amortized SVGD, but the core methods are largely incremental/derivative of prior Stein VI work (SVGD, amortized SVGD, KSD, operator-based objectives) and the empirical evidence is not at NeurIPS/ICML/ICLR level: limited benchmarks (toy GMM + Bayesian logistic regression), unclear fairness/strength of baselines (step-size heuristics, NUTS comparison not well-controlled), and missing rigorous evaluation of posterior quality, scalability, and ablations; writing/presentation also appears rough with gaps and overclaims."
      },
      "token": {
        "prompt_tokens": 8215,
        "completion_tokens": 153,
        "total_tokens": 8368
      },
      "time": "2026-02-09T20:39:48.926648+00:00"
    }
  ],
  "test": false
}