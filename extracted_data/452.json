{
  "id": "452",
  "title": "Learning to Repeat: Fine Grained Action Repetition for Deep Reinforcement Learning",
  "abstract": "Reinforcement Learning algorithms can learn complex behavioral patterns for sequential decision making tasks wherein an agent interacts with an environment and acquires feedback in the form of rewards sampled from it. Traditionally, such algorithms make decisions, i.e., select actions to execute, at every single time step of the agent-environment interactions. In this paper, we propose a novel framework, Fine Grained Action Repetition (FiGAR), which enables the agent to decide the action as well as the time scale of repeating it. FiGAR can be used for improving any Deep Reinforcement Learning algorithm which maintains an explicit policy estimate  by enabling temporal abstractions in the action space and implicitly enabling planning through sequences of repetitive macro-actions.   We empirically demonstrate the efficacy of our framework by showing performance improvements on top of three policy search algorithms in different domains: Asynchronous Advantage Actor Critic in the Atari 2600 domain, Trust Region Policy Optimization in Mujoco domain and Deep Deterministic Policy Gradients in the TORCS car racing domain.",
  "accepted": true,
  "zeroShot": [
    {
      "model": "claude-haiku-4-5",
      "decision": {
        "rejection": false,
        "confidence": 0.72,
        "primary_reason": "Novel framework for learning action repetition in DRL with solid empirical validation across multiple algorithms (A3C, TRPO, DDPG) and domains (Atari, MuJoCo, TORCS). Good methodological contribution with decoupled policy learning. However, limited theoretical analysis, arbitrary hyperparameter choices (|W|=30), and marginal improvements in some domains prevent higher confidence."
      },
      "token": {
        "prompt_tokens": 5411,
        "completion_tokens": 125,
        "total_tokens": 5536
      },
      "time": "2026-02-05T22:48:12.076224+00:00"
    }
  ],
  "article": [
    {
      "heading": "1 INTRODUCTION",
      "text": "Reinforcement learning (RL) is used to solve goal-directed sequential decision making problems wherein explicit supervision in the form of correct decisions is not provided to the agent, but only evaluative feedback in the form of the rewards sampled from the environment. RL algorithms model goal-directed sequential decision making problems as Markov Decision Processes (MDP) [Sutton & Barto (1998)]. However, for problems with an exponential or continuous state space, tabular RL algorithms that maintain value or policy estimates for every state become infeasible. Therefore, there is a need to be able to generalize decision making to unseen states. Recent advances in representation learning through deep neural networks provide an efficient mechanism for such generalization [LeCun et al. (2015)]. Such a combination of representation learning through deep neural networks with reinforcement learning objectives has shown promising results in many sequential decision making domains such as the Atari 2600 domain [Bellemare et al. (2013); Mnih et al. (2015); Schaul et al. (2015); Mnih et al. (2016)], Mujoco simulated physics tasks domain [Todorov et al. (2012); Lillicrap et al. (2015)], the Robosoccer domain [Hausknecht et al. (2016)] and the TORCS domain [Wymann et al. (2000); Mnih et al. (2016)]. Often, MDP settings consist of an agent interacting with the environment at discrete time steps. A common feature shared by all the Deep Reinforcement Learning (DRL) algorithms above is that they repeatedly execute a chosen action for a fixed number of time steps k. If at represents the action taken at time step t, then for the said algorithms, a1 = a2 = = ak, ak+1 = ak+2 = = a2k and in general aik+1 = aik+2 = = a(i+1)k, i 0. Action repetition allows these algorithms to compute the action once every k time steps and hence operate at higher speeds, thus achieving real-time performance. This also offers other advantages such as smooth action policies. More importantly, as shown in Lakshminarayanan et al. (2017) and Durugkar et al. (2016), macro-actions constituting the same action repeated k times could be interpreted as introducing temporal abstractions in the induced policies thereby enabling transitions between temporally distant advantageous states. The time scale for action repetition has largely been static in DRL algorithms until now [Mnih et al. (2015; 2016); Schaul et al. (2015)]. Lakshminarayanan et al. (2017) are the first to explore dynamic time scales for action repetition in the DRL setting and show that it leads to significant improvement in performance on a few Atari 2600 games. However, they choose only two time scales and the experiments are limited to a few representative games. Moreover the method is limited to tasks with a discrete action space. We propose FiGAR, a framework that enables any DRL algorithm regardless of whether its action space is continuous or discrete, to learn temporal abstractions in the form of temporally extended macro-actions. FiGAR uses a structured and factored representation of the policy whereby the policy for choosing the action is decoupled from that for the action repetition selection. Note that deciding actions and the action repetitions independently enables us to find temporal abstractions without blowing up the action space, unlike Vezhnevets et al. (2016) and Lakshminarayanan et al. (2017). The contribution of this work is twofold. First, we propose a generic extension to DRL algorithms by coming up with a factored policy representation for temporal abstractions (see figure 1 for sequences of macro actions learnt in 2 Atari 2600 games). Second, we empirically demonstrate FiGARs efficiency in improving policy gradient DRL algorithms with improvements in performance over several domains: 31 Atari 2600 games with Asynchronous Advantage Actor Critic [Mnih et al. (2016)], 5 tasks in MuJoCo Simulated physics tasks domain with Trust Region Policy Optimization [Schulman et al. (2015)] and the TORCS domain with Deep Deterministic Policy Gradients [Lillicrap et al. (2015)].",
      "exclude": true
    },
    {
      "heading": "2 RELATED WORK",
      "text": "Our framework is centered on a very general idea of only deciding when necessary. There have been similar ideas outside the RL domains. For instance, Gu et al. (2016) and Satija & Pineau (2016) explore Real Time Neural Machine Translation where the action at every time step is to decide whether to output a new token in the target language or not based on current context. Transition Point Dynamic Programming (TPDP) [Buckland & Lawrence (1994)] algorithm is a modification to the tabular dynamic programming paradigm that can reduce the learning time and memory required for control of continuous stochastic dynamic systems. This is done by determining a set of transition points in the underlying MDP. The policy changes only at these transition point states. The algorithm learns an optimal set of transition point states by using a variant of Q-Learning to evaluate whether or not to add/delete a particular state from the set of transition points. FiGAR learns the transition points in the underlying MDP on the fly with generalization across the state space unlike TPDP which is tabular and infeasible for large problems. The Dynamic Frameskip Deep Q-network [Lakshminarayanan et al. (2017)] proposes to use multiple time scales of action repetition by augmenting the Deep Q Network (DQN) [Mnih et al. (2015)] with separate streams of the same primitive actions corresponding to each time scale. This way, the time scale of action repetition is dynamically learned. Although this framework leads to a significant improvement in the performance on a few Atari 2600 games, it suffers from not being able to support multiple time scales due to potential explosion of the action space and is restricted to discrete action spaces. Durugkar et al. (2016) also explore learning macro-actions composed using the same action repeated for different time scales. However, their framework is limited to discrete action spaces and performance improvements are not significant. Learning temporally extended actions and abstractions have been of interest in RL for a long time. Vezhnevets et al. (2016) propose Strategic Attentive Writer (STRAW) for learning macro-actions and building dynamic action-plans directly from reinforcement learning signals. Instead of outputting a single action after each observation, STRAW maintains a multi-step action plan. The agent periodically updates the plan based on observations and commits to the plan between the replanning steps. Although the STRAW framework represents a more general temporal abstraction than FiGAR, FiGAR should be seen as a framework that can compliment STRAW whereby the decision to repeat could now be hierarchical at plan and base action levels. FiGAR is a framework that has a structured policy representation where the time scale of execution could be thought as parameterizing the chosen action. The only other work that explores parameterized policies in DRL is Hausknecht & Stone (2016) where discrete actions are parameterized by continuous values. In our case, discrete/continuous actions are parameterized by discrete values. The state spaces in Atari are also more sophisticated than the kind explored in Hausknecht et al. (2016). FiGAR is also very naturally connected to the Semi-MDPs (SMDPs) framework. SMDPs are MDPs with durative actions. The assumption in SMDPs is that actions take some holding time to complete [Duff (1995); Mahadevan et al. (1997); Dietterich (2000)]. Typically, they are modeled with two distributions, one corresponding to the next state transition and the other corresponding to the holding time which denotes the number of time steps between the current action from the policy until the next action from the policy. The rewards over the entire holding time of an action is the credit assigned for picking the action. In our framework, we naturally have durative actions due to the policy structure where the decision consists of both the choice of the action and the time scale of its execution. Therefore, we convert the original MDP to an SMDP trivially. In fact, we give more structure to the SMDP because we are clear that we repeat the chosen action during the holding time, while what happens during the holding time is not specified in the SMDP framework. One can think of the part of the policy that outputs the probability distribution over the time scales as a holding time distribution. Therefore, our framework naturally fits into the SMDP definition with the action repetition rate characterizing the holding time. We also sum up the rewards over the holding time with an an appropriate discounting factor as in an SMDP framework.",
      "exclude": true
    },
    {
      "heading": "3 BACKGROUND",
      "text": "",
      "exclude": false
    },
    {
      "heading": "3.1 ASYNCHRONOUS ADVANTAGE ACTOR CRITIC",
      "text": "Actor critic algorithms execute policy gradient updates by maintaining parametric estimates for the policy a(a|s) and the value function Vc(s) [Sutton & Barto (1998)]. The value function estimates are used to reduce the variance in the policy gradient updates. Asynchronous Advantage Actor Critic (A3C) [Mnih et al. (2016)] learns policies based on an asynchronous n-step returns. The k learner threads execute k copies of the policy asynchronously and the parameter updates are sent to a central parameter server at regular intervals. This ensures that temporal correlations are broken between subsequent updates since the different threads possibly explore different parts of the state space in parallel. The objective function for policy improvement in A3C is: L(a) = log a(at|st) (Gt V (st)) where Gt is an estimate for the return at time step t. The A3C algorithm uses n-step returns for estimating Gt which is a biased estimate for Q(st, at). Hence one can think of Gt V (st) as an estimate for A(st, at) which represents the advantage of taking action at in state st. The value function Vc(st) is updated by using n-step TD error as: L(c) = ( V (st) Vc(st) )2 where V (st) is an estimate of the n-step return from the current state. In A3C j-step returns are used where j n and n is a fixed hyper-parameter. For simplicity assume that t n. Then the definition for V (st) is: V (st) = n1 j=t tjrj + ntV (sn) The policy and value functions are parameterized by Deep Neural Networks.",
      "exclude": false
    },
    {
      "heading": "3.2 TRUST REGION POLICY OPTIMIZATION",
      "text": "TRPO [Schulman et al. (2015)] is a policy optimization algorithm. Constrained optimization of a surrogate loss function is proposed, with theoretical guarantees for monotonic policy improvement. The TRPO surrogate loss function L for potential next policies () is: Lold() = () + s (s) a (a|s)A(s, a) where old are the parameters of policy and are parameters of . This surrogate loss function is optimized subject to the constraint: DmaxKL (, ) which ensures that the policy improvement can be done in non-trivial step sizes and at the same time the new policy does not deviate much from the current policy due to the KL-divergence constraint.",
      "exclude": false
    },
    {
      "heading": "3.3 DEEP DETERMINISTIC POLICY GRADIENTS",
      "text": "According to the Deterministic Policy Gradient (DPG) Theorem [Lever (2014)], the gradient of the performance objective (J) of the deterministic policy () in continuous action spaces with respect to the policy parameters () is given by: J() = S (s)(s)aQ(s, a)|a=(s)ds = Es [(s)aQ(s, a)|a=(s)] (1) for an appropriately defined performance objective J . The DPG model built according to this theorem consists of an actor which outputs an action vector in the continuous action space and a critic model Q(s, a) which evaluates the action chosen at a state. The DDPG algorithm [Lillicrap et al. (2015)] extends the DPG algorithm by introducing non-linear neural network based function approximators for the actor and critic.",
      "exclude": false
    },
    {
      "heading": "4 FIGAR: FINE GRAINED ACTION REPETITION",
      "text": "FiGAR provides a DRL algorithm with the ability to model temporal abstractions by augmenting it with the ability to predict the number of time steps for which an action chosen for execution is to be repeated. This prediction is conditioned on the current state in the environment. The FiGAR framework can be used to extend any DRL algorithm (say Z) which maintains an explicit policy. Let Z denote the extension of Z under FiGAR. Z has two independent decoupled Algorithm 1 Create FiGAR Z 1: function MAKEFIGAR(DRLAlgorithm Z, ActionRepetitionSet W) 2: st state at time t 3: at action taken in st at time t 4: a action policy of Z 5: fa(st) action network for realizing action policy a 6: L(a, st, at) As objective function for improving a 7: x construct action repetition policy for FiGAR-Z. 8: fx(st) repetition network with output of size |W | for action repetition policy x. 9: L(x, st, at) L evaluated at x 10: T (st, at) L(x, st, at) L(a, st, at) // Total Loss 11: return T, fa , fx policy components. The policy a for choosing actions and the policy x for choosing action repetitions. Algorithm 1 describes the generic framework for deriving DRL algorithm Z from algorithm Z. Let W stand for the set of all action repetitions that Z would be able to perform. In tradition DRL algorithms, W = c, where c is a constant. This implies that the action repetition is static and fixed. In FiGAR, The set of action repetitions from which Z can choose is W = w1, w2, , w|W |. The central idea behind FiGAR is that the objective function used to update the parameters aof a maintained by Z will be used to update the parameters x of the action repetition policy x of Z as well (illustrated by the sharing of L in Algorithm 1). In the first subsection, we desribe how Z operates. In the next two sub-sections, we describe the instantiations of FiGAR extensions for 3 policy gradient DRL algorithms: A3C, TRPO and DDPG.",
      "exclude": false
    },
    {
      "heading": "4.1 HOW FIGAR OPERATES",
      "text": "The following procedure describes how FiGAR variant Z navigates the MDP that it is solving: 1. In the very first state s0 seen by Z , it predicts a tuple (a0, x0) of action to execute and number of time steps for which to execute it. a0 is decided based on a(s0) whereas x0 is decided based on x(s0). Each such tuple is known as an action decision. 2. We denote by sj the state of the agent after j such action decisions have been made. Similarly xj and aj denote the action repetition and the action chosen after j such action decisions. Note that xj w1, w2, , w|W |, the set of all allowed action repetitions. 3. From time step 0 until x0 , Z executes a0. 4. At time step x0, Z again decides, based on current state s1 and policy components (a(s1), x(s1)), the tuple of action to execute and the number of times for which to execute it, (a1, x1). 5. It can seen that in general if Z executes action ak for xk successive time steps, the next action is decided at time step t = k i=0 xi on the basis of (a(sk+1), x(sk+1)), where sk+1 is the state seen at time step t.",
      "exclude": false
    },
    {
      "heading": "4.2 FIGAR-A3C",
      "text": "A3C uses fa(sj) and fc(sj) which represent the policy (a|sj) and the value function V (sj) respectively. (a|sj) is a vector of size equal to the action space of the underlying MDP while V (sj) is a scalar. FiGAR extends the A3C algorithm as follows: 1. With sj defined as in the previous sub-section, in addition to fa(sj) and fc(sj) , FiGARA3C defines a neural network fx(sj). This neural network outputs a |W |-dimensional vector representing the probability distribution over the elements of the set W . The sampled time scale from this multinomial distribution decides how long the action decided with fa(sj) is repeated. The actor is now composed of both fa(sj) (action network) and fx(sj) (repetition network). 2. The objective function for the actor is modified to be: L(a, x) = (log fa(a|sj) + log fx(x|sj))A(sj , a, x) where A(sj , a, x) represents the advantage of executing action a for x time steps at state sj . This implies that for FiGAR-A3C the combination operator defined in Algorithm 1 is in fact scalar addition. 3. The objective function for the critic is the same except that estimated value function used in the target for the critic is changed as: V (sj) = n1 k=j ykjrk + ynjV (sn) where we define y0 = 0, yk = yk1 + xk, k 1 and action ak was repeated xk times when state sk was encountered. Note that the return used in target is based on n decision steps, steps at which a potential change in actions executed takes place. It is not based on n time steps. Note that point 2 above implies that the action space has been extended by |W | and has a dimension of |A|+ |W |. It is only because of this factored representation of the FiGAR policy that the number of parameters do not blow up. If one were to extend the action space in a naive way by coupling the actions and the action repetitions, one would end up suffering the kind of action-space blowup as seen in [Lakshminarayanan et al. (2017); Vezhnevets et al. (2016)] wherein for being able to control with respect to |W | different action repetition levels (or |W |-length policy plans in the case of STRAW) , one would need to model |A| |W | actions or action-values which would blow up the final layer size |W | times.",
      "exclude": false
    },
    {
      "heading": "4.3 FIGAR-TRPO",
      "text": "Although fa(sj) in A3C is generic enough to output continuous or discrete actions, we consider A3C only for discrete action spaces. Preserving the notation from the previous subsection, we describe FiGAR-TRPO where we consider the case of the output generated by the network fa(sj) to be A dimensional with each dimension being independent and describing a continuous valued action. The stochastic policy is hence modeled as a multi-variate Gaussian with diagonal co-variance matrix. The parameters of the mean as well as the co-variance matrix are together represented by a and the concatenated mean-covariance vector is represented by the function fa(sj). FiGAR-TRPO is constructed as follows: 1. In TRPO,the objective function Lold() is constructed based on trajectories drawn according to the current policy. Hence, for FiGAR-TRPO the objective function is modified to be: La,old,x,old(a) ( La,old,x,old(x) )ar where x are the parameters of sub-network fx which computes the action repetition distribution. This implies that for FiGAR-TRPO the combination operator defined in Algorithm 1 is in some sense the scalar multiplication. ar controls the relative learning rate of the core-policy parameters and the action repetition parameters. 2. The constraint in TRPO corresponding to the KL divergence between old and new policies is modified to be: DmaxKL (a, a) + KLD max KL (x, x) where a denotes the Gaussian distribution for the action to be executed and x denotes the multinomial softmax-based action repetition probability distribution. KL controls the relative divergence of x and a from the new corresponding policies. See Appendix C for an explanation of the loss function used.",
      "exclude": false
    },
    {
      "heading": "4.4 FIGAR-DDPG",
      "text": "In this subsection, we present an extension of DDPG under the FiGAR framework. DDPG consists of fa(sj) which denotes a deterministic policy (s) and is a vector of size equal to the action space of the underlying MDP; and fc(sj , aj) which denotes the critic network whose output is a single number, the estimated state-action value function Q(sj , aj). FiGAR framework extends the DDPG algorithm as follows: 1. fx is introduced, similar to FiGAR-A3C. This implies that the complete policy for FiGARDDPG (a , x) is computed by the tuple of neural networks: (fa , fx) . Similar to DDPG [Lillicrap et al. (2015)], FiGAR-DDPG has no loss function for the actor. The actor receives gradients from the critic. This is because the actors proposed policy is directly fed to the critic and the critic provides the actor with gradients which the proposed policy follows for improvement. In FiGAR-DDPG the total policy is a concatenation of vectors a and x. Hence the gradients for the total policy are also simply the concatenation of the gradients for the policies a and x. 2. To ensure sufficient exploration, the exploration policy for action repetition is an -greedy version of the behavioral action repetition policy. The action part of the policy, (fa(sj)), continues to use temporally correlated noise for exploration, generated by an Ornstein-Uhlenbeck process (see Lillicrap et al. (2015) for details). 3. The critic is modeled by the equation f(sj , aj , xj) = fc(sj , fa(sj), fx(sj)) As stated above, fx is learnt by back-propagating the gradients produced by the critic with respect to fx , in exactly the same way that fa is learnt.",
      "exclude": false
    },
    {
      "heading": "5 EXPERIMENTAL SETUP AND RESULTS",
      "text": "The experiments are designed to understand the answers to the following questions: 1. For different DRL algorithms, can FiGAR extensions learn to use the dynamic action repetition? 2. How does FiGAR impact the performance of the different algorithms on various tasks? 3. Is FiGAR able to learn control on several different kinds of Action Repetition sets W ? In the next three sub-sections, we experiment with the simplest possible action repetition set W = 1, 2, , |W |. In the fourth sub-section, we understand the effects that changing the action repetition set W has on the policies learnt.",
      "exclude": false
    },
    {
      "heading": "5.1 FIGAR-A3C ON ATARI 2600",
      "text": "This set of experiments was performed with FiGAR-A3C on the Atari 2600 domain. The hyperparameters were tuned on a subset of games (Beamrider, Breakout, Pong, Seaquest and Space Invaders) and kept constant across all games. W is perhaps the most important hyper-parameter and depicts our confidence in the ability of a DRL agent to predict the future. Such a choice has to depend on the domain in which the DRL agent is operating. We only wanted to demonstrate the ability of FiGAR to learn temporal abstractions and hence instead of tuning for an optimal |W |, it was chosen to be 30, arbitrarily. The specific set of time scales we choose is 1, 2, 3, , 30. FiGAR-A3C as well as A3C were trained for 100 million decision steps. They were evaluated in terms of the final policy learnt. Treating the score obtained by the A3C algorithm as baseline (b), we calculated the percentage improvement (i) offered by FiGARA3C (f) as: i = fbb . Figure 2 plots this metric versus the game names. The improvement for Enduro and Atlantis is staggering and more than 900 and 35 respectively. Figure 2s y-axis has been clipped at 1000% to make it more presentable. Appendix A contains the experimental details, the raw scores obtained by both the methods. Appendix B contains experiments on validating our setup. To answer the first question we posed, experiments were conducted to record the percentage of times that a particular action repetition was chosen. Figure 3 presents the action repetition distribution across a selection of games, chosen arbitrarily. The values have been rounded to 2 decimal places and hence do not sum to 1 in each game. Each game was played for 10 episodes using the same policy used to calculate average scores in Figure 2. The two tables together show that FiGAR-A3C generally prefers lower action repetition but does come up with temporal abstractions in policy space (specially in games like Pong and Crazy Climber). Some such abstractions have been demonstrated in Figure 1. Such temporal abstractions do not always help general gameplay (Demon Attack). However, as can be seen from Figure 2, FiGAR-A3C outperforms A3C in 26 out of 33 games. One could potentially think of FiGAR as a deep exploration framework by using the learnt policy a for predicting actions at every time step and completely discarding the action-repetition policy x , at evaluation time. Appendix F contains an empirical argument against such a usage of FiGAR and demonstrates that the temporal abstractions encoded by x are indeed important for game play performance.",
      "exclude": false
    },
    {
      "heading": "5.2 FIGAR-TRPO ON MUJOCO TASKS",
      "text": "In this sub-section we demonstrate that FiGAR-TRPO can learn to solve the Mujoco simulated physics tasks reasonably successfully. Similar to FiGAR-A3C, |W | is chosen to be 30 arbitrarily. The full policy (fa , fx) is trained jointly. The policies learnt after each TRPO optimization step (details in Appendix C) are compared to current best known policy to arrive at the overall best policy. The results in this sub-section are for this best policy. Table 1 compares the performance of TRPO and FiGAR-TRPO. The number in the brackets is the average action repetition chosen. As can be seen from the table, FiGAR learns either policies which are much faster to execute albeit at cost of slight loss in optimality or it learns policies similar to non-repetition case, performance being competitive with the baseline algorithm. This best policy was then evaluated on 100 episodes to arrive at average scores which are contained in Table 1. TRPO is a difficult baseline on the MuJoCo tasks domain. On the whole, FiGAR outperforms TRPO in 3 out of 5 domains, although the gains are marginal in most tasks. Appendix C contains experimental details. A video showing FiGAR-TRPOs learned behavior policies can be found at http://youtu.be/JiaO2tBtH-k.",
      "exclude": false
    },
    {
      "heading": "5.3 FIGAR-DDPG ON TORCS",
      "text": "FiGAR-DDPG was trained and tested on the TORCS domain. |W | was chosen to be 15 arbitrarily. FIGAR-DDPG manages to complete the race task flawlessly and manages to finish 20 laps of the circuit, after which the simulator stops. The total reward obtained by FiGAR-DDPG was 557929.68 as against 59519.70 obtained by DDPG. We also observed that FiGAR-DDPG learnt policies which were smoother than those learnt by DDPG. A video showing the learned driving behavior of the FiGAR-DDPG agent can be found at https://youtu.be/dX8J-sF-WX4. See Appendix D for experimental and architectural details.",
      "exclude": false
    },
    {
      "heading": "5.4 EFFECT OF ACTION REPETITION SET ON FIGAR",
      "text": "This sub-section answers the third question raised at the beginning of this section in affirmative. We demonstrate that there is nothing sacrosanct about the set of action repetitions W = 1, 2, , 30 on which FiGAR-A3C performed well, and that the good performance carries over to other action repetition sets. To demonstrate the generality of FiGAR with respect to W , we chose a wide variety of action repetition sets W , trained and evaluated FiGAR-A3C variants which learn to repeat with respect to their respective Action Repetition sets. Table 3 describes the various FiGAR-variants considered for these experiments in terms of their action repetition set W . Note that the hyper-parameters of the various variants of FiGAR-A3C were not tuned but rather the same ones obtained by tuning for FiGAR-30 were used. Table 2 contains a comparison of the raw scores obtained by the various FiGAR-A3C variants in comparison to the A3C baseline. It is clear that FiGAR is able to learn over any action repetition set W and the performance does not fall by a lot even when hyper-parameters tuned for FiGAR-30 are used for other variants. Appendix E contains additional graphs showing the evolution of average game scores against number of training steps as well as a bar graph visualization of Table 2.",
      "exclude": false
    },
    {
      "heading": "6 CONCLUSION, SHORTCOMINGS AND FUTURE WORK",
      "text": "We propose a light-weight framework (FiGAR) for improving current Deep Reinforcement Learning algorithms for policy optimization whereby temporal abstractions are learned in the policy space. The framework is generic and applicable to DRL algorithms concerned with policy gradients for continuous as well as discrete action spaces such as A3C, TRPO and DDPG. FiGAR maintains a structured policy wherein the action probability distribution is augmented with a probability distribution for choosing the time scale of repeating the chosen action. Our results demonstrate that FiGAR can be used to significantly improve the current policy gradient and Actor-Critic algorithms thereby learning better control policies across several domains by discovering optimal sequences of temporally elongated macro-actions. Atari, TORCS and MuJoCo represent environments which are largely deterministic with a minimal degree of stochasticity in environment dynamics. In such highly deterministic environments we would expect FiGAR agents to build a latent model of the environment dynamics and hence be able to execute large action repetitions without dying. This is exactly what we see in a highly deterministic environment like the game Freeway. Figure 1 (a) demonstrates that the chicken is able to judge the speed of the approaching cars appropriately and cross the road in a manner which takes it to the goal without colliding with the cars and at the same time avoiding them narrowly. Having said that, certainly the ability to stop an action repetition (or a macro-action) in general would be very important, especially in stochastic environments. In our setup, we do not consider the ability to stop executing a macro-action that the agent has committed to. However, this is a necessary skill in the event of unexpected changes in the environment while executing a chosen macro-action. Thus, stop and start actions for stopping and committing to macro-actions can be added to the basic dynamic time scale setup for more robust policies. We believe the modification could work for more general stochastic worlds like Minecraft and leave it for future work.",
      "exclude": true
    },
    {
      "heading": "ACKNOWLEDGMENTS",
      "text": "We used the open source implementation of A3C at https://github.com/miyosuda/ async_deep_reinforce. We thank Volodymr Mnih for giving valuable hyper-parameter information. We thank Aravind Rajeswaran (University of Washington) for very helpful discussions regarding and feedback on the MuJoCo domain tasks. The TRPO implementation was a modification of https://github.com/aravindr93/robustRL. The DDPG implementation was a modification of https://github.com/yanpanlau/DDPG-Keras-Torcs. We thank ILDS (http://web.iitm.ac.in/ilds/) for the compute resources we used for running A3C experiments.",
      "exclude": true
    },
    {
      "heading": "APPENDIX A: EXPERIMENTAL DETAILS FOR FIGAR-A3C",
      "text": "EXPERIMENTAL DETAILS AND RESULTS We used the LSTM-variant of A3C [Mnih et al. (2016)] algorithm for FiGAR-A3C experiments. The async-rmsprop algorithm [Mnih et al. (2016)] was used for updating parameters with the same hyper-parameters as in Mnih et al. (2016). The initial learning rate used was 103 and it was linearly annealed to 0 over 100 million steps. The n used in n-step returns was 20. Entropy regularization was used to encourage exploration, similar to Mnih et al. (2016). The for entropy regularization was found to be 0.02 after hyper-parameter tuning, both for the action-policy fa and the action repetition policy fx . Since the Atari 2600 games tend to be quite complex, jointly learning a factored policy from random weight initializations proved to be less optimal as compared to a more stage-wise approach. The approach we followed for training FiGAR-A3C was to first train the networks using the regular A3C-objective function. This stage trains the action part of the policy fa and value function fc for a small number of iterations with a fixed action repetition rate (in this stage, gradients are not back-propagated for fx and all action repetition predictions made are discarded). The next stage was to then train the entire architecture (fa , fx , fc) jointly. This kind of a non-stationary training objective ensures that we have a good value function estimator fc and a good action policy estimator fa before we start training the full policy (fa , fx) jointly. Every time FiGAR decides to execute action at for xt time steps, we say one step of action selection has been made. Since the number of time steps for which an action is repeated is variable, training time is measured in terms of action selections carried out. The first stage of the training was executed for 20 million (a hyper-parameter we found by doing grid search) action selections (called steps here onwards) and the next stage was executed for 80 million steps. In comparison the baseline ran for 100 million steps (action selections). Since a large entropy regularization was required to explore both components (fa and fx) of the policy-space, this also ends up meaning that the policies learnt are more diffused than one would like them to be. Evaluation was done after every 1 million steps and followed a strategy similar to -greedy. With = 0.1 probability, the action and action repetition was drawn from the output distribution ((fa and fx respectively) and with probability 1 the action (and independently the action selection) with maximum probability was selected. This evaluation was done for 100 episodes or 100000 steps whichever was smaller, to arrive at an average score. Table 4 contains the raw scores obtained by the final FiGAR-A3C and A3C policies on 33 Atari 2600 games. The numbers inside the brackets depict the confidence interval at a confidence threshold of 0.95, calculated by averaging scores over 100 episodes. Table 5 contains scores for a competing method, STRAW [Vezhnevets et al. (2016)], which learns temporal abstractions by maintaining action plans, for the subset of games on which both FiGAR and STRAW were trained and tested. Note that the scores obtained by STRAW agents are averages over top 5 performing replicas. We can infer from Tables 4 and 5 that FiGAR and STRAW and competitive with each other, with FiGAR clearly out-performing STRAW in Breakout and STRAW clearing outperforming FiGAR in Frostbite. Figure 4 demonstrates the evolution of the performance of FiGAR-A3C versus training progress. It also contains corresponding metrics for A3C to facilitate comparisons. In the 100 episode long evaluation phase we also keep track of the best episodic score. We also plot the best episodes score versus time to get an idea of how bad the learnt policy is compared to the best it could have been. ARCHITECTURE DETAILS We used the same low level architecture as Mnih et al. (2016) which in turn uses the same low level architecture as Mnih et al. (2015), except that the pre-LSTM hidden layer had size 256 instead of 512 as in Mnih et al. (2016). Similar to Mnih et al. (2016) the Actor and Critic share all but one layer. Hence all but the final layer of fa , fx and fc are the same. Each of the 3 networks has a different final layer with fa and fx having a softmax-non linearity as output non-linearity, to model the multinomial distribution and the fc (critic)s output being linear. APPENDIX B: ADDITIONAL EXPERIMENTS FOR ATARI 2600 These additional experiments are geared at understanding the repercussions of the evaluation strategy chosen by us. THE CHOICE OF WHETHER TO BE GREEDY OR STOCHASTIC Note that in Appendix A, we state that for evaluating the policy learnt by the agent, we simply chose to sample from the output probability distributions with probability 0.1 and chose the optimal action/action repetition with probability 0.9. This choice of 0.1 might seem rather arbitrary. Hence we conducted experiments to understand how well the agent performs as we shift more and more from choosing the maximal action(0.1-greedy policy) towards sampling from output distributions (stochastic policy). Figure 5 demonstrates that the performance of FiGAR-A3C does not deteriorate significantly, in comparison to A3C, even if we always sample from policy distributions, for most of the games. In the cases that there is a significant deterioration, we believe it is due to the diffused nature of the policy distributions (action and action repetition) learnt. Hence, although our choice of evaluation scheme might seem arbitrary, it is in fact reasonable. PERFORMANCE VERSUS SPEED TRADEOFF The previous discussion leads to a novel way to trade-off game-play performance versus speed. Figure 3 demonstrated that although FiGAR-A3C learns to use temporally elongated macro-actions, it does favor shorter actions for many games. Since the action repetition distribution x is diffused (as will be shown by Table 6), sampling from the distribution should help FiGAR choose larger action repetition rates probably at the cost of optimality of game play. Table 6 demonstrates that this is exactly what FiGAR does. It was generated by playing 10 episodes, or 100000 steps, whichever is lesser and recording the fraction of times each action repetition was chosen. The policy used in populating table 6 was the stochastic policy (described in previous subsection). Contrast Table 6 to Table 7 which is an expanded version of Figure 3. Both Figure 3 and Table 7 were created using the 0.1-greedy policy described in previous subsection. The reason that we compare the stochastic policy with the 0.1-greedy version instead of the fully-greedy version (wherein the optimal action and action repetition is always chosen) is that such a policy would end up being deterministic would not be good for evaluations. It can hence be seen that FiGAR learns to trade-off optimality of game-play for speed by choosing whether to sample from policy probability distributions (a and x ) with probability 1 and thus behave stochastically, or behave 0.1-greedily, and sample from the distributions with only a small probability. Table 6 can be compared to Figure 3 to understand how stochasticity in final policy affects action repetition chosen. A clear trend can be seen in all games wherein the stochastic variant of final policy learns to use longer and longer actions, albeit at a small cost of some loss in the optimality of game-play (as shown by Figure 5). An expanded version of Figure 3 is presented as Table 7 for comparison with Table 6. As explained in Appendix A, the policy used for populating Table 7 is such that it picks a greedy action (or action repetition) with probability 0.9 and stochastically samples from output probability distributions with probability 0.1. Table 8 contains the average action repetition chosen in each of the games for the two FiGARvariants. The same episodes used to populate Table 6 and 7 were used to fill Table 8. It can be seen that in most games, the Stochastic variant of policy learns to play at a higher speed, although this might result in some loss in optimality of game play, as demonstrated in Figure 5.",
      "exclude": true
    },
    {
      "heading": "APPENDIX C: EXPERIMENTAL SETUP FOR FIGAR-TRPO",
      "text": "EXPERIMENTAL DETAILS FiGAR-TRPO and the corresponding baseline algorithm operate on low dimensional feature vector observations. The TRPO (and hence FiGAR-TRPO) algorithm operates in two phases. In the first phase (P1), K trajectories are sampled according to current behavioral policy to create the surrogate loss function. In the second phase (P2) a policy improvement step is performed by carrying out an optimization step on the surrogate loss function, subject to the KL-divergence constraint on the new policy. In our experiments, 500 such policy improvement steps were performed. K varies with the learning progress and the schedule on what value K would take in next iteration of P1 is defined linearly in terms of the return in the last iteration of P1. Hence if the return was large in previous iteration of P1, a small number of episodes are are used to construct the surrogate loss function in current iteration. The best policy was found by keeping track of the average returns seen during the training phase P1. This policy was then evaluated on 100 episodes to obtain the average score of the TRPO policy learnt. The most important hyper-parameters for FiGAR-TRPO are ar and KL. By using a grid search on the set 0.01, 0.02, 0.04, 0.08, 0.16, 0.32, 0.64, 1.28 we found the optimal hyper-parameters ar = 1.28 and KL = 0.64. These were tuned on all the 5 tasks. LOSS FUNCTION AND ARCHITECTURE The tanh non-linearity is used throughout. The mean vector is realized using a 2-Hidden Layer neural network (mean network) with hidden layer sizes (128, 64). The standard deviation is realized using a Parameter layer (std-dev layer) which parameterizes the standard deviation but does not depend on the input. Hence the concatenation of the output of mean network and the std-dev layer forms the action policy fa as described in Section 4. The Action Repetition function fx is realized using a 2-Hidden Layer neural (act-rep network) network similar to the mean network albeit with smaller hidden layer sizes: (128, 64). However, its output non-linearity is a softmax layer of size 30 as dictated by the value of W . The action repetition network was kept small to ensure that FiGAR-TRPO does not have significantly more parameters than TRPO. The mean network, std-dev layer and act-rep network do not share any parameters or layers (See appendix G for experiments on FiGAR-TRPO with shared layers). The surrogate loss function in TRPO when the Single Path method of construction is followed reduces to [Schulman et al. (2015)]: Lold() = Esold,aold [ (a|s) old(a|s) Qold(s, a) ] where q, the sampling distribution is just the old behavioral policy old (defining characteristic of Single-Path method) and is the improper discounted state visitation distribution. The surrogate loss function for a factored policy such as that of FiGAR-TRPO is: La,old,x,old(a, x) = Es,a,x [ a(a|s) a,old(a|s) x(x|s) x,old(x|s) Qa,old,x,old(s, a, x) ] where s a,xold , a a,old , x x,old and a = fa , a,old = fa,old , x = fx and x,old = fx,old This kind of a splitting of probability distributions happens because the action-policy fa and the action-repetition policy fx are independent probability distributions. The theoretically sound way to realize FiGAR-TRPO is to minimize the loss La,old,x,old(a, x). However, we found that in practice, optimizing a relaxed version of the objective function, that is, La,old,x,old(a) La,old,x,old(x)ar works better. This leads to the FiGAR-TRPO objective defined in Section 4.3.",
      "exclude": true
    },
    {
      "heading": "APPENDIX D: EXPERIMENTAL DETAILS FOR FIGAR-DDPG",
      "text": "EXPERIMENTAL DETAILS The DDPG algorithm also operates on the low-dimensional (29 dimensional) feature-vector observations. The domain consists of 3 continuous actions, acceleration, break and steering. The W hyper-parameter used in main experiments was chosen to be 15 arbitrarily. Unlike Lillicrap et al. (2015), we did not find it useful to use batch normalization and hence it was not used. However, a replay memory was used of size 10000. Target networks were also used with soft updates being applied with = 0.001. Sine DDPG is an off-policy actor-critic method, we need to ensure that sufficient exploration takes place. Use of an Ornstein-Uhlenbeck process (refer to Lillicrap et al. (2015) for details) ensured that exporation was carried out in action-policy space. To ensure exploration in the action-repetition policy space, we adopted two strategies. First, an -greedy version of the policy was used during train time. The was annealed from 0.2 to 0 over 50000 training steps. The algorithm was run for 40000 training steps for baselines as well as FiGAR-DDPG. Second, with probability 1 , instead of picking the greedy action-repetition , we sampled from the output distribution fx(s). ARCHITECTURAL DETAILS Through the architecture, the hidden layer non-linearity used was ReLU. All hidden layer weights were initialized using the He initialization [He et al. (2015)] The actor network consisted of a 2-hidden layer neural network with hidden sizes (300, 600) (call the second hidden layer representation h2. We learn two different output layers on top of this common hidden representation. fa was realized by transforming h2 with an output layer of size 3. The output neuron corresponding to the action steering used tanh non linearity where as those corresponding to acceleration and break used the sigmoid non-linearity. The fx network was realized by transforming h2 using a softmax output layer of size |W |. The output of the Actor network is a 3 + |W | = 18 dimensional vector. The critic network takes as input the state vector (29-dimensional) and the action vector (18- dimensional). The critic is a 3 hidden layer network of size (300, 600, 600). Similar to Lillicrap et al. (2015), actions were not included until the 2nd hidden layer of fc . The final output is linear and is trained using the TD-error objective function, similar to Lillicrap et al. (2015)",
      "exclude": true
    },
    {
      "heading": "APPENDIX E: DETAILS FOR FIGAR-VARIANTS",
      "text": "It is clear from Figure 6 that even though FiGAR A3C needs to explore in 2 separate action-spaces (those of primitive actions and the action repetitions), the training progress is not slowed down as a result of this exploration, for any FiGAR variant. Table 2 contains final evaluation scores attained by various FiGAR variants. Figure 7 contains a bargraph visualization of the same table to demonstrate the advantage of all FiGAR variants relative to the baselines. APPENDIX F: IMPORTANCE OF x One could potentially use FiGAR at evaluation stage (after training has been completed) at an actionrepetition rate of 1 by picking every action according to a and completely discarding the learnt repetition policy x . Such a FiGAR variant is denoted as FiGAR-wo-x . We demonstrate that FiGAR-wo-x is worse than FiGAR on most games and hence the temporal abstractions learnt by and encoded in x are indeed non-trivial and important for gameplay performance. Table 9 contains the comparison between standard FiGAR agent and FiGAR-wo-x . Evaluation scheme is the same as Appendix A. We observe that in 24 out of 33 games, x helps the agent learn temporal abstractions which result in a significant boost in performance compared to the FiGAR-wo-x agents.",
      "exclude": true
    },
    {
      "heading": "APPENDIX G: SHARED REPRESENTATION EXPERIMENTS FOR FIGAR-TRPO",
      "text": "Section 5.2 contains results of experiments on FiGAR-TRPO. Appendix C contains the experimental setup for the same. Throughout these experiments on FiGAR-TRPO the policy components fa and fx do not share any representations. This appendix contains experimental results in the setting wherein (fa and fx) share all layers except the final one. This agent/network is denoted with the name FiGAR-shared-TRPO. All the hyper-parameters are the same as those in Appendix C except ar and KL which were obtained through a grid-search similar to appendix C. These were tuned on all the 5 tasks. The values for these hyper-parameters that we found to be optimal are ar = 1.28 and KL = 0.16. The same training and evaluation regime as appendix C was used. The performance of the best policy learnt is tabulated in Table 10 FiGAR-shared-TRPO on the whole does not perform much better than FiGAR-TRPO. In these TRPO experiments, the neural networks we used were rather shallow at only two hidden layers deep. Hence, we believe that sharing of layers thus leads to only small gains in terms of optimality of policy learnt.",
      "exclude": true
    }
  ],
  "fewShot": [
    {
      "model": "claude-haiku-4-5",
      "decision": {
        "rejection": false,
        "confidence": 0.72,
        "primary_reason": "Novel factored policy representation for temporal abstraction in RL with consistent empirical validation across three different algorithms (A3C, TRPO, DDPG) and multiple domains. Clear methodological contribution despite incremental gains in some settings."
      },
      "token": {
        "prompt_tokens": 5573,
        "completion_tokens": 87,
        "total_tokens": 5660
      },
      "time": "2026-02-08T22:18:05.053100+00:00"
    }
  ],
  "test": false
}