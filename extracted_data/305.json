{
  "id": "305",
  "title": "End-to-end Optimized Image Compression",
  "abstract": "We describe an image compression method, consisting of a nonlinear analysis transformation, a uniform quantizer, and a nonlinear synthesis transformation. The transforms are constructed in three successive stages of convolutional linear filters and nonlinear activation functions. Unlike most convolutional neural networks, the joint nonlinearity is chosen to implement a form of local gain control, inspired by those used to model biological neurons. Using a variant of stochastic gradient descent, we jointly optimize the entire model for rate-distortion performance over a database of training images, introducing a continuous proxy for the discontinuous loss function arising from the quantizer. Under certain conditions, the relaxed loss function may be interpreted as the log likelihood of a generative model, as implemented by a variational autoencoder. Unlike these models, however, the compression model must operate at any given point along the rate-distortion curve, as specified by a trade-off parameter. Across an independent set of test images, we find that the optimized method generally exhibits better rate-distortion performance than the standard JPEG and JPEG 2000 compression methods. More importantly, we observe a dramatic improvement in visual quality for all images at all bit rates, which is supported by objective quality estimates using MS-SSIM.",
  "accepted": true,
  "zeroShot": [
    {
      "model": "claude-haiku-4-5",
      "decision": {
        "rejection": false,
        "confidence": 0.95,
        "primary_reason": "Seminal work on neural image compression with novel end-to-end optimization framework, strong methodological contributions (GDN transforms, continuous relaxation for quantization), comprehensive experiments, and significant practical improvements over JPEG/JPEG2000. Highly suitable for top-tier ML conferences."
      },
      "token": {
        "prompt_tokens": 7566,
        "completion_tokens": 89,
        "total_tokens": 7655
      },
      "time": "2026-02-05T22:33:32.110244+00:00"
    },
    {
      "model": "gpt-5.2",
      "decision": {
        "rejection": false,
        "confidence": 0.74,
        "primary_reason": "Strong, relevant and methodologically sound end-to-end learned transform coding with convincing rate–distortion gains over JPEG/JPEG2000; evidence is solid though test set is modest and comparisons lack newer learned baselines."
      },
      "token": {
        "prompt_tokens": 6797,
        "completion_tokens": 62,
        "total_tokens": 6859
      },
      "time": "2026-02-09T21:06:54.241046+00:00"
    }
  ],
  "article": [
    {
      "heading": "1 INTRODUCTION",
      "text": "Data compression is a fundamental and well-studied problem in engineering, and is commonly formulated with the goal of designing codes for a given discrete data ensemble with minimal entropy (Shannon, 1948). The solution relies heavily on knowledge of the probabilistic structure of the data, and thus the problem is closely related to probabilistic source modeling. However, since all practical codes must have finite entropy, continuous-valued data (such as vectors of image pixel intensities) must be quantized to a finite set of discrete values, which introduces error. In this context, known as the lossy compression problem, one must trade off two competing costs: the entropy of the discretized representation (rate) and the error arising from the quantization (distortion). Different compression applications, such as data storage or transmission over limited-capacity channels, demand different ratedistortion trade-offs. Joint optimization of rate and distortion is difficult. Without further constraints, the general problem of optimal quantization in high-dimensional spaces is intractable (Gersho and Gray, 1992). For this reason, most existing image compression methods operate by linearly transforming the data vector into a suitable continuous-valued representation, quantizing its elements independently, and then encoding the resulting discrete representation using a lossless entropy code (Wintz, 1972; Netravali and Limb, 1980). This scheme is called transform coding due to the central role of the transforma- JB and EPS are supported by the Howard Hughes Medical Institute. tion. For example, JPEG uses a discrete cosine transform on blocks of pixels, and JPEG 2000 uses a multi-scale orthogonal wavelet decomposition. Typically, the three components of transform coding methods transform, quantizer, and entropy code are separately optimized (often through manual parameter adjustment). We have developed a framework for end-to-end optimization of an image compression model based on nonlinear transforms (figure 1). Previously, we demonstrated that a model consisting of linear nonlinear block transformations, optimized for a measure of perceptual distortion, exhibited visually superior performance compared to a model optimized for mean squared error (MSE) (Balle, Laparra, and Simoncelli, 2016). Here, we optimize for MSE, but use a more flexible transforms built from cascades of linear convolutions and nonlinearities. Specifically, we use a generalized divisive normalization (GDN) joint nonlinearity that is inspired by models of neurons in biological visual systems, and has proven effective in Gaussianizing image densities (Balle, Laparra, and Simoncelli, 2015). This cascaded transformation is followed by uniform scalar quantization (i.e., each element is rounded to the nearest integer), which effectively implements a parametric form of vector quantization on the original image space. The compressed image is reconstructed from these quantized values using an approximate parametric nonlinear inverse transform. For any desired point along the ratedistortion curve, the parameters of both analysis and synthesis transforms are jointly optimized using stochastic gradient descent. To achieve this in the presence of quantization (which produces zero gradients almost everywhere), we use a proxy loss function based on a continuous relaxation of the probability model, replacing the quantization step with additive uniform noise. The relaxed ratedistortion optimization problem bears some resemblance to those used to fit generative image models, and in particular variational autoencoders (Kingma and Welling, 2014; Rezende, Mohamed, and Wierstra, 2014), but differs in the constraints we impose to ensure that it approximates the discrete problem all along the ratedistortion curve. Finally, rather than reporting differential or discrete entropy estimates, we implement an entropy code and report performance using actual bit rates, thus demonstrating the feasibility of our solution as a complete lossy compression method.",
      "exclude": true
    },
    {
      "heading": "2 CHOICE OF FORWARD, INVERSE, AND PERCEPTUAL TRANSFORMS",
      "text": "Most compression methods are based on orthogonal linear transforms, chosen to reduce correlations in the data, and thus to simplify entropy coding. But the joint statistics of linear filter responses exhibit strong higher order dependencies. These may be significantly reduced through the use of joint local nonlinear gain control operations (Schwartz and Simoncelli, 2001; Lyu, 2010; Sinz and Bethge, 2013), inspired by models of visual neurons (Heeger, 1992; Carandini and Heeger, 2012). Cascaded versions of such models have been used to capture multiple stages of visual transformation (Simoncelli and Heeger, 1998; Mante, Bonin, and Carandini, 2008). Some earlier results suggest that incorporating local normalization in linear block transform coding methods can improve coding performance (Malo et al., 2006), and can improve object recognition performance of cascaded convolutional neural networks (Jarrett et al., 2009). However, the normalization parameters in these cases were not optimized for the task. Here, we make use of a generalized divisive normalization (GDN) transform with optimized parameters, that we have previously shown to be highly efficient in Gaussianizing the local joint statistics of natural images, much more so than cascades of linear transforms followed by pointwise nonlinearities (Balle, Laparra, and Simoncelli, 2015). Note that some training algorithms for deep convolutional networks incorporate batch normalization, rescaling the responses of linear filters in the network so as to keep it in a reasonable operating range (Ioffe and Szegedy, 2015). This type of normalization is different from local gain control in that the rescaling factor is identical across all spatial locations. Moreover, once the training is completed, the scaling parameters are typically fixed, which turns the normalization into an affine transformation with respect to the data unlike GDN, which is spatially adaptive and can be highly nonlinear. Specifically, our analysis transform ga consists of three stages of convolution, subsampling, and divisive normalization. We represent the ith input channel of the kth stage at spatial location (m,n) as u(k)i (m,n). The input image vector x corresponds to u (0) i (m,n), and the output vector y is u (3) i (m,n). Each stage then begins with an affine convolution: v (k) i (m,n) = j ( hk,ij u(k)j ) (m,n) + ck,i, (1) where denotes 2D convolution. This is followed by downsampling: w (k) i (m,n) = v (k) i (skm, skn), (2) where sk is the downsampling factor for stage k. Each stage concludes with a GDN operation: u (k+1) i (m,n) = w (k) i (m,n)( k,i + j k,ij ( w (k) j (m,n) )2) 12 . (3) The full set of h, c, , and parameters (across all three stages) constitute the parameter vector to be optimized. Analogously, the synthesis transform gs consists of three stages, with the order of operations reversed within each stage, downsampling replaced by upsampling, and GDN replaced by an approximate inverse we call IGDN (more details in the appendix). We define u(k)i (m,n) as the input to the kth synthesis stage, such that y corresponds to u(0)i (m,n), and x to u (3) i (m,n). Each stage then consists of the IGDN operation: w (k) i (m,n) = u (k) i (m,n) ( k,i + j k,ij ( u (k) j (m,n) )2) 12 , (4) which is followed by upsampling: v (k) i (m,n) = w (k) i (m/sk, n/sk) if m/sk and n/sk are integers, 0 otherwise, (5) where sk is the upsampling factor for stage k. Finally, this is followed by an affine convolution: u (k+1) i (m,n) = j ( hk,ij v(k)j ) (m,n) + ck,i. (6) Analogously, the set of h, c, , and make up the parameter vector . Note that the down/upsampling operations can be implemented jointly with their adjacent convolution, improving computational efficiency. In previous work, we used a perceptual transform gp, separately optimized to mimic human judgements of grayscale image distortions (Laparra et al., 2016), and showed that a set of one-stage transforms optimized for this distortion measure led to visually improved results (Balle, Laparra, and Simoncelli, 2016). Here, we set the perceptual transform gp to the identity, and use mean squared error (MSE) as the metric (i.e., d(z, z) = z z22). This allows a more interpretable comparison to existing methods, which are generally optimized for MSE, and also allows optimization for color images, for which we do not currently have a reliable perceptual metric.",
      "exclude": false
    },
    {
      "heading": "3 OPTIMIZATION OF NONLINEAR TRANSFORM CODING MODEL",
      "text": "Our objective is to minimize a weighted sum of the rate and distortion, R+D, over the parameters of the analysis and synthesis transforms and the entropy code, where governs the trade-off between the two terms (figure 2, left panel). Rather than attempting optimal quantization directly in the image space, which is intractable due to the high dimensionality, we instead assume a fixed uniform scalar quantizer in the code space, and aim to have the nonlinear transformations warp the space in an appropriate way, effectively implementing a parametric form of vector quantization (figure 1). The actual rates achieved by a properly designed entropy code are only slightly larger than the entropy (Rissanen and Langdon, 1981), and thus we define the objective functional directly in terms of entropy: L[ga, gs, Pq] = E [ log2 Pq ] + E [ d(z, z) ] , (7) where both expectations will be approximated by averages over a training set of images. Given a powerful enough set of transformations, we can assume without loss of generality that the quantization bin size is always one and the representing values are at the centers of the bins. That is, yi = qi = round(yi), (8) where index i runs over all elements of the vectors, including channels and spatial locations. The marginal density of yi is then given by a train of discrete probability masses (Dirac delta functions, figure 2, right panel) with weights equal to the probability mass function of qi: Pqi(n) = n+ 12 n 12 pyi(t) dt, for all n Z . (9) Note that both terms in (7) depend on the quantized values, and the derivatives of the quantization function (8) are zero almost everywhere, rendering gradient descent ineffective. To allow optimization via stochastic gradient descent, we replace the quantizer with an additive i.i.d. uniform noise source y, which has the same width as the quantization bins (one). This relaxed formulation has two desirable properties. First, the density function of y = y+ y is a continuous relaxation of the probability mass function of q (figure 2, right panel): py(n) = Pq(n), for all n ZM , (10) which implies that the differential entropy of y can be used as an approximation of the entropy of q. Second, independent uniform noise approximates quantization error in terms of its marginal moments, and is frequently used as a model of quantization error (Gray and Neuhoff, 1998). We can thus use the same approximation for our measure of distortion. We examine the empirical quality of these rate and distortion approximations in section 4. We assume independent marginals in the code space for both the relaxed probability model of y and the entropy code, and model the marginals pyi non-parametrically to reduce model error. Specifically, we use finely sampled piecewise linear functions which we update similarly to onedimensional histograms (see appendix). Since pyi = pyi U(0, 1) is effectively smoothed by a box-car filter the uniform density on the unit interval, U(0, 1) the model error can be made arbitrarily small by decreasing the sampling interval. Given this continuous approximation of the quantized coefficient distribution, the loss function for parameters and can be written as: L(,) = Ex,y [ i log2 pyi(ga(x;) + y; (i)) + d ( gp ( gs(ga(x;) + y;) ) , gp(x) )] . (11) where vector (i) parameterizes the piecewise linear approximation of pyi (trained jointly with and ). This is continuous and differentiable, and thus well-suited for stochastic optimization.",
      "exclude": false
    },
    {
      "heading": "3.1 RELATIONSHIP TO VARIATIONAL GENERATIVE IMAGE MODELS",
      "text": "We derived our formulation directly from the classical ratedistortion optimization problem. However, once the transition to a continuous loss function is made, the optimization problem resembles those encountered in fitting generative models of images, and can more specifically be cast in the context of variational autoencoders (Kingma and Welling, 2014; Rezende, Mohamed, and Wierstra, 2014). In Bayesian variational inference, we are given an ensemble of observations of a random variable x along with a generative model px|y(x|y). We seek to find a posterior py|x(y|x), which generally cannot be expressed in closed form. The approach followed by Kingma and Welling (2014) consists of approximating this posterior with a density q(y|x), by minimizing the Kullback Leibler divergence between the two: DKL [ qpy|x ] = Eyq log q(y|x) Eyq log py|x(y|x) = Eyq log q(y|x) Eyq log px|y(x|y) Eyq log py(y) + const. (12) This objective function is equivalent to our relaxed ratedistortion optimization problem, with distortion measured as MSE, if we define the generative model as follows (figure 3): px|y(x|y;,) = N ( x; gs(y;), (2) 11 ) , (13) py(y; (0),(1), . . . ) = i pyi(yi; (i)), (14) and the approximate posterior as follows: q(y|x;) = i U(yi; yi, 1) with y = ga(x;), (15) where U(yi; yi, 1) is the uniform density on the unit interval centered on yi. With this, the first term in the KullbackLeibler divergence is constant; the second term corresponds to the distortion, and the third term corresponds to the rate (both up to additive constants). Note that if a perceptual transform gp is used, or the metric d is not Euclidean, px|y is no longer Gaussian, and equivalence to variational autoencoders cannot be guaranteed, since the distortion term may not correspond to a normalizable density. For any affine and invertible perceptual transform and any translation-invariant metric, it can be shown to correspond to the density px|y(x|y;,) = 1 Z() exp ( d ( gp ( gs(y;) ) , gp(x) )) , (16) where Z() normalizes the density (but need not be computed to fit the model). Despite the similarity between our nonlinear transform coding framework and that of variational autoencoders, it is worth noting several fundamental differences. First, variational autoencoders are continuous-valued, and digital compression operates in the discrete domain. Comparing differential entropy with (discrete) entropy, or entropy with an actual bit rate, can potentially lead to misleading results. In this paper, we use the continous domain strictly for optimization, and perform the evaluation on actual bit rates, which allows comparison to existing image coding methods. We assess the quality of the rate and distortion approximations empirically. Second, generative models aim to minimize differential entropy of the data ensemble under the model, i.e., explaining fluctuations in the data. This often means minimizing the variance of a slack term like (13), which in turn maximizes . Transform coding methods, on the other hand, are optimized to achieve the best trade-off between having the model explain the data (which increases rate and decreases distortion), and having the slack term explain the data (which decreases rate and increases distortion). The overall performance of a compression model is determined by the shape of the convex hull of attainable model distortions and rates, over all possible values of the model parameters. Finding this convex hull is equivalent to optimizing the model for particular values of (see figure 2). In contrast, generative models operate in a regime where is inferred and ideally approaches infinity for noiseless data, which corresponds to the regime of lossless compression. Even so, lossless compression methods still need to operate in a discretized space, typically directly on quantized luminance values. For generative models, the discretization of luminance values is usually considered a nuisance (Theis, van den Oord, and Bethge, 2015), although there are examples of generative models that operate on quantized pixel values (van den Oord, Kalchbrenner, and Kavukcuoglu, 2016). Finally, although correspondence between the typical slack term (13) of a generative model (figure 3, left panel) and the distortion metric in ratedistortion optimization holds for simple metrics (e.g., Euclidean distance), a more general perceptual measure would be considered a peculiar choice from a generative modeling perspective, if it corresponds to a density at all.",
      "exclude": false
    },
    {
      "heading": "4 EXPERIMENTAL RESULTS",
      "text": "We jointly optimized the full set of parameters , , and all over a subset of the ImageNet database (Deng et al., 2009) consisting of 6507 images using stochastic descent. This optimization was performed separately for each , yielding separate transforms and marginal probability models for each value. For the grayscale analysis transform, we used 128 filters (size 9 9) in the first stage, each subsampled by a factor of 4 vertically and horizontally. The remaining two stages retain the number of channels, but use filters operating across all input channels (5 5 128), with outputs subsampled by a factor of 2 in each dimension. The net output thus has half the dimensionality of the input. The synthesis transform is structured analogously. For RGB images, we trained a separate set of models, with the first stage augmented to operate across three (color) input channels. For the two largest values of , and for RGB models, we increased the network capacity by increasing the number of channels in each stage to 256 and 192, respectively. Further details about the parameterization of the transforms and their training can be found in the appendix. We first verified that the continuously-relaxed loss function given in section 3 provides a good approximation to the actual ratedistortion values obtained with quantization (figure 4). The relaxed distortion term appears to be mostly unbiased, and exhibits a relatively small variance. The relaxed (differential) entropy provides a somewhat positively biased estimate of the discrete entropy for the coarser quantization regime, but the bias disappears for finer quantization, as expected. Note that since the values of do not have any intrinsic meaning, but serve only to map out the convex hull of optimal points in the ratedistortion plane (figure 2, left panel), a constant bias in either of the terms would simply alter the effective value of , with no effect on the compression performance. We compare the ratedistortion performance of our method to two standard methods: JPEG and JPEG 2000. For our method, all images were compressed using uniform quantization (the continuous relaxation using additive noise was used only for training purposes). To make the comparisons more fair, we implemented a simple entropy code based on the context-based adaptive binary arithmetic coding framework (CABAC; Marpe, Schwarz, and Wiegand, 2003). All sideband information needed by the decoder (size of images, value of , etc.) was included in the bit stream (see appendix). Note that although the computational costs for training our models are quite high, encoding or decoding an image with the trained models is efficient, requiring only execution of the optimized analysis transformation and quantizer, or the synthesis transformation, respectively. Evaluations were performed on the Kodak image dataset1, an uncompressed set of images commonly used to evaluate image compression methods. We also examined a set of relatively standard (if outdated) images used by the compression community (known by the names Lena, Barbara, Peppers, and Mandrill) as well as a set of our own digital photographs. None of these test images was included in the training set. All test images, compressed at a variety of bit rates using all three methods, along with their associated ratedistortion curves, are available online at http://www.cns.nyu.edu/ lcv/iclr2017. 1Downloaded from http://www.cipr.rpi.edu/resource/stills/kodak.html Although we used MSE as a distortion metric for training, the appearance of compressed images is both qualitatively different and substantially improved, compared to JPEG and JPEG 2000. As an example, figure 5 shows an image compressed using our method optimized for a low value of (and thus, a low bit rate), compared to JPEG/JPEG 2000 images compressed at equal or greater bit rates. The image compressed with our method has less detail than the original (not shown, but available online), with fine texture and other patterns often eliminated altogether, but this is accomplished in a way that preserves the smoothness of contours and sharpness of many of the edges, giving them a natural appearance. By comparison, the JPEG and JPEG 2000 images exhibit artifacts that are common to all linear transform coding methods: since local features (edges, contours, texture elements, etc.) are represented using particular combinations of localized linear basis functions, independent scalar quantization of the transform coefficients causes imbalances in these combinations, and leads to visually disturbing blocking, aliasing, and ringing artifacts that reflect the underlying basis functions. Remarkably, we find that the perceptual advantages of our method hold for all images tested, and at all bit rates. The progression from high to low bit rates is shown for an example image in figure 6 (additional examples provided in appendix and online). As bit rate is reduced, JPEG and JPEG 2000 degrade their approximation of the original image by coarsening the precision of the coefficients of linear basis functions, thus exposing the visual appearance of those basis functions. On the other hand, our method appears to progressively simplify contours and other image features, effectively concealing the underlying quantization of the representation. Consistent with the appearance of these example images, we find that distortion measured with a perceptual metric (MS-SSIM; Wang, Simoncelli, and Bovik, 2003), indicates substantial improvements across all tested images and bit rates (figure 7; additional examples provided in the appendix and online). Finally, when quantified with PSNR, we find that our method exhibits better ratedistortion performance than both JPEG and JPEG 2000 for most (but not all) test images, especially at the lower bit rates.",
      "exclude": false
    },
    {
      "heading": "5 DISCUSSION",
      "text": "We have presented a complete image compression method based on nonlinear transform coding, and a framework to optimize it end-to-end for ratedistortion performance. Our compression method offers improvements in ratedistortion performance over JPEG and JPEG 2000 for most images and bit rates. More remarkably, although the method was optimized using mean squared error as a distortion metric, the compressed images are much more natural in appearance than those compressed with JPEG or JPEG 2000, both of which suffer from the severe artifacts commonly seen in linear transform coding methods. Consistent with this, perceptual quality (as estimated with the MS-SSIM index) exhibits substantial improvement across all test images and bit rates. We believe this visual improvement arises because the cascade of biologically-inspired nonlinear transformations in the model have been optimized to capture the features and attributes of images that are represented in the statistics of the data, parallel to the processes of evolution and development that are believed to have shaped visual representations within the human brain (Simoncelli and Olshausen, 2001). Nevertheless, additional visual improvements might be possible if the method were optimized using a perceptual metric in place of MSE (Balle, Laparra, and Simoncelli, 2016). For comparison to linear transform coding methods, we can interpret our analysis transform as a single-stage linear transform followed by a complex vector quantizer. As in many other optimized representations e.g., sparse coding (Lewicki and Olshausen, 1998) as well as many engineered representations e.g., the steerable pyramid (Simoncelli, Freeman, et al., 1992), curvelets (Candes and Donoho, 2002), and dual-tree complex wavelets (Selesnick, Baraniuk, and Kingsbury, 2005) the filters in this first stage are localized and oriented and the representation is overcomplete. Whereas most transform coding methods use complete (often orthogonal) linear transforms with spatially separable filters, the overcompleteness and orientation tuning of our initial transform may explain the ability of the model to better represent features and contours with continuously varying orientation, position and scale (Simoncelli, Freeman, et al., 1992). Our work is related to two previous publications that optimize image representations with the goal of image compression. Gregor, Besse, et al. (2016) introduce an interesting hierarchical representation of images, in which degradations are more natural looking than those of linear representations. However, rather than optimizing directly for ratedistortion performance, their modeling is generative. Due to the differences between these approaches (as outlined in section 3.1), their procedure of obtaining coding representations from the generative model (scalar quantization, and elimination of hierarchical levels of the representation) is less systematic than our approach and unlikely to be optimal. Further, no entropy code is provided, and the authors therefore resort to comparing entropy estimates to bit rates of established compression methods, which can be unreliable. The model developed by Toderici et al. (2016) is optimized to provide various ratedistortion trade-offs and directly output a binary representation, making it more easily comparable to other image compression methods. Moreover, their formulation has the advantage over ours that a single representation is sought for all rate points. However, it is not clear whether their formulation necessarily leads to ratedistortion optimality (and their empirical results suggest that this is not the case). We are currently testing models that use simpler rectified-linear or sigmoidal nonlinearities, to determine how much of the performance and visual quality of our results is due to use of biologicallyinspired joint nonlinearities. Preliminary results indicate that qualitatively similar results are achievable with other activation functions we tested, but that rectified linear units generally require a substantially larger number of model parameters/stages to achieve the same ratedistortion performance as the GDN/IGDN nonlinearities. This suggests that GDN/IGDN transforms are more efficient for compression, producing better models with fewer stages of processing (as we previously found for density estimation; Balle, Laparra, and Simoncelli, 2015), which might be an advantage for deployment of our method, say, in embedded systems. However, such conclusions are based on a somewhat limited set of experiments and should at this point be considered provisional. More generally, GDN represents a multivariate generalization of a particular type of sigmoidal function. As such, the observed efficiency advantage relative to pointwise nonlinearities is expected, and a variant of a universal function approximation theorem (e.g., Leshno et al., 1993) should hold. The ratedistortion objective can be seen as a particular instantiation of the general unsupervised learning or density estimation problems. Since the transformation to a discrete representation may be viewed as a form of classification, it is worth considering whether our framework offers any insights that might be transferred to more specific supervised learning problems, such as object recognition. For example, the additive noise used in the objective function as a relaxation of quantization might also serve the purpose of making supervised classification networks more robust to small perturbations, and thus allow them to avoid catastrophic adversarial failures that have been demonstrated in previous work (Szegedy et al., 2013). In any case, our results provide a strong example of the power of end-to-end optimization in achieving a new solution to a classical problem.",
      "exclude": true
    },
    {
      "heading": "ACKNOWLEDGMENTS",
      "text": "We thank Olivier Henaff and Matthias Bethge for fruitful discussions.",
      "exclude": true
    },
    {
      "heading": "6 APPENDIX",
      "text": "",
      "exclude": true
    },
    {
      "heading": "6.1 NETWORK ARCHITECTURE AND OPTIMIZATION",
      "text": "As described in the main text, our analysis transform consists of three stages of convolution, downsampling, and GDN. The number and size of filters, downsampling factors, and connectivity from layer to layer are provided in figure 8 for the grayscale transforms. The transforms for RGB images and for high bit rates differ slightly in that they have an increased number of channels in each stage. These choices are somewhat ad-hoc, and a more thorough exploration of alternative architectures could potentially lead to significant performance improvements. We have previously shown that GDN is highly efficient in Gaussianizing the local joint statistics of natural images (Balle, Laparra, and Simoncelli, 2015). Even though Gaussianization is a quite different optimization problem than the ratedistortion objective with the set of constraints defined above, it is similar in that a marginally independent latent model is assumed in both cases. When optimizing for Gaussianization, the exponents in the parametric form of GDN control the tail behavior of the Gaussianized densities. Since tail behavior is less important here, we chose to simplify the functional form, fixing the exponents as well as forcing the weight matrix to be symmetric (i.e., k,ij = k,ji). The synthesis transform is meant to function as an approximate inverse transformation, so we construct it by applying a principle known from the LISTA algorithm (Gregor and LeCun, 2010) to the fixed point iteration previously used to invert the GDN transform (Balle, Laparra, and Simoncelli, 2015). The approximate inverse consists of one iteration, but with a separate set of parameters from the forward transform, which are constrained in the same way, but trained separately. We refer to this nonlinear transform as inverse GDN (IGDN). The full model (analysis and synthesis filters, GDN and IGDN parameters) were optimized, for each , over a subset of the ImageNet database (Deng et al., 2009) consisting of 6507 images. We applied a number of preprocessing steps to the images in order to reduce artifacts and other unwanted contaminations: first, we eliminated images with excessive saturation. We added a small amount of uniform noise, corresponding to the quantization of pixel values, to the remaining images. Finally, we downsampled and cropped the images to a size of 256 256 pixels each, where the amount of downsampling and cropping was randomized, but depended on the size of the original image. In order to reduce high-frequency noise and compression artifacts, we only allowed resampling factors less than 0.75, discarding images that were too small to satisfy this constraint. To ensure efficient and stable optimization, we used the following techniques: We used the Adam optimization algorithm (Kingma and Ba, 2014) to obtain values for the parameters and , starting with = 104, and subsequently lowering it by a factor of 10 whenever the improvement of both rate and distortion stagnated, until = 107. Linear filters were parameterized using their discrete cosine transform (DCT) coefficients. We found this to be slightly more effective in speeding up the convergence than discrete Fourier transform (DFT) parameterization (Rippel, Snoek, and Adams, 2015). We parameterized the GDN parameters in terms of the elementwise relationship k,i = ( k,i) 2 210. The squaring ensures that gradients are smaller around parameter values close to 0, a regime in which the optimization can otherwise become unstable. To obtain an unambiguous mapping, we projected each k,i onto the interval [2 5,) after each gradient step. We applied the same treatment to k,ij , and additionally averaged k,ij with its transpose after each step in order to make it symmetric as explained above. The IGDN parameters were treated in the same way. To remove the scaling ambiguity between the each linear transform and its following nonlinearity (or preceding nonlinearity, in the case of the synthesis transform), we renormalized the linear filters after each gradient step, dividing each filter by the square root of the sum of its squared coefficients. For the analysis transform, the sum runs over space and all input channels, and for the synthesis transform, over space and all output channels. We represented each of the marginals pyi as a piecewise linear function (i.e., a linear spline), using 10 sampling points per unit interval. The parameter vector (i) consists of the value of pyi at these sampling points. We did not use Adam to update (i); rather, we used ordinary stochastic gradient descent to minimize the negative expected likelihood: L( (0),(1), . . . ) = Ey i pyi(yi; (i)). (17) and renormalized the marginal densities after each step. After every 106 gradient steps, we used a heuristic to adapt the range of the spline approximation to cover the range of values of yi obtained on the training set.",
      "exclude": false
    },
    {
      "heading": "6.2 ENTROPY CODE",
      "text": "We implemented an entropy code based on the context-adaptive binary arithmetic coding (CABAC) framework defined by Marpe, Schwarz, and Wiegand (2003). Arithmetic entropy codes are designed to compress discrete-valued data to bit rates closely approaching the entropy of the representation, assuming that the probability model used to design the code describes the data well. The following information was encoded into the bitstream: the size of the image (two 16-bit integers, bypassing arithmetic coding), whether the image is grayscale or RGB (one bit, bypassing arithmetic coding), the value of (one 16-bit integer, bypassing arithmetic coding), which provides an index for the parameters of the analysis and synthesis transforms as well as the initial probability models for the entropy codes (these are fixed after optimization, and assumed to be available to encoder and decoder). the value of each element of q, iterating over channels, and over space in raster-scan order, using the arithmetic coding engine. Since CABAC operates on binary values, the quantized values in q need to be converted to binary decisions. We follow a simple scheme inspired by the encoding of H.264/AVC transform coefficients as detailed by Marpe, Schwarz, and Wiegand (2003). For each qi, we start by testing if the encoded value is equal to the mode of the distribution. If this is the case, the encoding of qi is completed. If not, another binary decision determines whether it is smaller or larger than the mode. Following that, each possible integer value is tested in turn, which yields a bifurcated chain of decisions as illustrated in figure 9. This process is carried out until either one of the binary decisions determines qi, or some minimum (qi,min) or maximum (qi,max) value is reached. In case qi is outside of that range, the difference between it and the range bound is encoded using an exponential Golomb code, bypassing the arithmetic coding engine. Adaptive codes, such as CABAC, can potentially further improve bit rates, and to some extent correct model error, by adapting the probability model on-line to the statistics of the data. In our code, this is achieved by sharing the marginal probability model Pqi of each element in q across space within each channel. We derived the initial probability models by subsampling the continuous densities pyi determined during optimization, as in (10). However, note that due to the simple raster-scan ordering, the coding scheme presented above only crudely exploits spatial adaptation of the probability model compared to existing coding methods such as JPEG 2000 and H.264/AVC. Thus, the performance gains compared to a well-designed non-adaptive entropy code are relatively small (figure 10), and likely smaller than those achieved by the entropy code in JPEG 2000, to which we compare.",
      "exclude": false
    },
    {
      "heading": "6.3 EVALUATION DETAILS AND ADDITIONAL EXAMPLE IMAGES",
      "text": "Although it is desirable to summarize and compare the ratedistortion behavior of JPEG, JPEG 2000, and our method across an image set, it is difficult to do this in a way that is fair and interpretable. First, ratedistortion behavior varies substantially across bit rates for different images. For example, for the image in figure 12, our method achieves the same MSE with roughly 50% of the bits needed by JPEG 2000 for low rates, and about 30% for high rates. For the image in figure 17, the gains are more modest, although still significant through the range. But for the image in figure 15, our method only slightly outperforms JPEG 2000 at low rates, and under-performs it at high rates. Note that these behaviors are again different for MS-SSIM, which shows a significant improvement for all images and bit rates (consistent with their visual appearance). Second, there is no obvious or agreed-upon method for combining ratedistortion curves across images. More specifically, one must decide which points in the curves to combine. For our method, it is natural to average the MSE and entropy values across images compressed using the same choice of , since these are all coded and decoded using exactly the same representation and quantization scheme. For JPEG, it seems natural to average over images coded at the same quality setting, which appear to be coded using the same quantization choices. The OpenJPEG implementation of JPEG 2000 we use allows selection of points on the ratedistortion curve either through specification of a target bit rate, or a target quality. This choice has no effect on ratedistortion plots for individual images (verified, but not shown), but has a substantial effect when averaging over images, since the two choices lead one to average over a different set of R-D points. This is illustrated in figure 11. Even if points were selected in exactly the same fashion for each of the methods (say, matched to a given set of target rates), summary plots can still over- or underemphasize high rate vs. low rate performance. We conclude that summaries of ratedistortion are of limited use. Instead, we encourage the reader to browse our extensive collection of test images, with individual ratedistortion plots for each image, available at http://www.cns.nyu.edu/ lcv/iclr2017 in both grayscale and RGB. In the following pages, we show additional example images, compressed at relatively low bit rates, in order to visualize the qualitative nature of compression artifacts. On each page, the JPEG 2000 image is selected to have the lowest possible bit rate that is equal or greater than the bit rate of the proposed method. In all experiments, we compare to JPEG with 4:2:0 chroma subsampling, and the OpenJPEG implementation of JPEG 2000 with the default multiple component transform. For evaluating PSNR, we use the JPEG-defined conversion matrix to convert between RGB and YCbCr. For evaluating MS-SSIM (Wang, Simoncelli, and Bovik, 2003), we used only the resulting luma component. Original images are not shown, but are available online, along with compressed images at a variety of other bit rates, at http://www.cns.nyu.edu/ lcv/iclr2017.",
      "exclude": false
    }
  ],
  "fewShot": [
    {
      "model": "gpt-5.2",
      "decision": {
        "rejection": false,
        "confidence": 0.86,
        "primary_reason": "Strong, well-motivated end-to-end learned transform coding framework (analysis/quantize/synthesis) with a principled differentiable relaxation of quantization, biologically inspired GDN nonlinearity, and convincing rate–distortion gains over JPEG/JPEG2000 (including perceptual MS-SSIM improvements) with substantial implementation/experimental detail; overall novelty+impact and empirical validation meet top-tier conference standards."
      },
      "token": {
        "prompt_tokens": 9431,
        "completion_tokens": 111,
        "total_tokens": 9542
      },
      "time": "2026-02-09T20:06:38.559239+00:00"
    },
    {
      "model": "claude-haiku-4-5",
      "decision": {
        "rejection": false,
        "confidence": 0.95,
        "primary_reason": "Landmark contribution combining deep learning with classical rate-distortion theory for image compression. Introduces end-to-end optimization of nonlinear analysis/synthesis transforms with GDN, provides principled continuous relaxation of quantization for gradient-based learning, establishes connection to VAEs while clarifying fundamental differences, and demonstrates substantial practical improvements over JPEG/JPEG2000 in both objective metrics (MS-SSIM) and visual quality across diverse test images. Thorough experimental validation, clear presentation, and reproducible methodology with public code/results."
      },
      "token": {
        "prompt_tokens": 10460,
        "completion_tokens": 153,
        "total_tokens": 10613
      },
      "time": "2026-02-11T18:19:53.011182+00:00"
    }
  ],
  "test": false
}