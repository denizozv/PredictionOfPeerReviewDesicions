{
  "id": "723",
  "title": "Cortical-Inspired Open-Bigram Representation for Handwritten Word Recognition",
  "abstract": "Recent research in the cognitive process of reading hypothesized that we do not read words by sequentially recognizing letters, but rather by identifing open-bigrams, i.e. couple of letters that are not necessarily next to each other.  In this paper, we evaluate an handwritten word recognition method based on original open-bigrams representation. We trained Long Short-Term Memory Recurrent Neural Networks (LSTM-RNNs) to predict open-bigrams rather than characters, and we show that such models are able to learn the long-range, complicated and intertwined dependencies in the input signal, necessary to the prediction.  For decoding, we decomposed each word of a large vocabulary into the set of constituent bigrams, and apply a simple cosine similarity measure between this  representation and the bagged RNN prediction to retrieve the vocabulary word.  We compare this method to standard word recognition techniques based on  sequential character recognition. Experiments are carried out on two public databases of handwritten words (Rimes and IAM), an the results with our bigram decoder are comparable  to more conventional decoding methods based on sequences of letters.",
  "accepted": false,
  "zeroShot": [
    {
      "model": "claude-haiku-4-5",
      "decision": {
        "rejection": true,
        "confidence": 0.85,
        "primary_reason": "Limited novelty and marginal improvements over baselines. While the open-bigram representation is cognitively motivated, the experimental results show only comparable (not superior) performance to standard character-sequence methods. The approach lacks significant methodological innovation for a top-tier ML venue, and the cognitive science motivation is not rigorously validated. Better suited for domain-specific conferences (document analysis/handwriting recognition)."
      },
      "token": {
        "prompt_tokens": 5758,
        "completion_tokens": 112,
        "total_tokens": 5870
      },
      "time": "2026-02-05T23:14:27.994319+00:00"
    },
    {
      "model": "gpt-5.2",
      "decision": {
        "rejection": true,
        "confidence": 0.78,
        "primary_reason": "Limited novelty and insufficiently compelling methodology/experiments for a top-tier ML venue (bag-of-open-bigrams + cosine lexicon retrieval; heavy reliance on engineered features and multiple per-order RNNs; comparisons/ablations and modern baselines are not strong enough to justify acceptance)."
      },
      "token": {
        "prompt_tokens": 5290,
        "completion_tokens": 76,
        "total_tokens": 5366
      },
      "time": "2026-02-09T21:43:44.679331+00:00"
    }
  ],
  "article": [
    {
      "heading": "",
      "text": "Recent research in the cognitive process of reading hypothesized that we do not read words by sequentially recognizing letters, but rather by identifing openbigrams, i.e. couple of letters that are not necessarily next to each other. In this paper, we evaluate an handwritten word recognition method based on original open-bigrams representation. We trained Long Short-Term Memory Recurrent Neural Networks (LSTM-RNNs) to predict open-bigrams rather than characters, and we show that such models are able to learn the long-range, complicated and intertwined dependencies in the input signal, necessary to the prediction. For decoding, we decomposed each word of a large vocabulary into the set of constituent bigrams, and apply a simple cosine similarity measure between this representation and the bagged RNN prediction to retrieve the vocabulary word. We compare this method to standard word recognition techniques based on sequential character recognition. Experiments are carried out on two public databases of handwritten words (Rimes and IAM), an the results with our bigram decoder are comparable to more conventional decoding methods based on sequences of letters.",
      "exclude": true
    },
    {
      "heading": "1 INTRODUCTION",
      "text": "Taking inspiration in Biology is sometimes very efficient. For example, deep neural networks (NN) which are outperforming all other methods (including support vector machines, SVM) in image recognition are based on a series of several (usually 5 to 15) neurons layers, each layer involving sparsity in the activation pattern (a biological trait of the cortical map). The analogy continues with the modeling of the cortex as a hierarchy of cortical maps. Thanks to the analysis of reaction time in cognitive psychology experiments, the minimal number of cortical maps involved in a cognitive process is estimated to about ten, the same order of magnitude as the number of layers in deep neural networks for computer vision tasks. In the case of handwritten word recognition, Dehaene et al. have proposed a biologically plausible model of the cortical organization of reading (Dehaene et al., 2005) that assumes seven successive steps of increasing complexity, from the retinal ganglion cells to a cortical map of the orthographic word forms (Figure 1). One of the most recent successes of experimental psychology was the demonstration that human visual word recognition uses an explicit representation of letter position order based on letter pairs: the open-bigram coding (Whitney et al., 2012; Gomez et al., 2008; Grainger & Van Heuven, 2003; Glotin et al., 2010; Dufau, 2008). As demonstrated in (Touzet et al., 2014), open-bigrams (OB) allow an over-coding of the orthographic form of words that facilitates recognition. OB coding favors same length words (i.e., neighbors of similar lengths). In the context of learning to read, the existence of the OB layer just before the orthographic word representation has been used to explain the lack of efficiency of whole language method (today banned from reading teaching) compared to the phonics method which explic- itly supervises the organization of the OB map (with syllables), where the global method does not (Figure 1). Since cognitive psychology has demonstrated the existence of the OB layer, the hypothesis has been put forward (Touzet et al., 2014) that the orthographic representation of words may have evolved in order to take into account the topology of the OB space, instead of the topology of the single letter space. Our goal here is to test this hypothesis, comparing OB vs sequential character recognition for word recognition. A state-of-the-art decoder based on a Long Short-Term Memory Recurrent Neural Networks (LSTM-RNN) is used on two public databases of handwritten words (Rimes and IAM). The remaining of this paper will be divided as follows. In Section 2, we present related methods for handwritten word recognition. Then, we describe the open-bigram representation of words and the proposed decoder in Section 3. The experimental setup, including the data and the bigram prediction model, is explained in Section 4. Finally, we present our results in Section 5, before concluding in Section 6.",
      "exclude": true
    },
    {
      "heading": "2 RELATED WORK",
      "text": "In this section, we give a brief overview of existing techniques for handwritten word recognition. Historically, the methods may be divided in three broad categories. The first approach is whole word recognition, where the image of the full word is directly classified into word classes, without relying on the character level (e.g. in (Parisse, 1996; Madhvanath & Govindaraju, 1996)). In the second method, the word image is segmented into parts of characters (stokes or graphemes). The segments are grouped and scored, and character sequences are obtained with a graph search (e.g. in (Bengio et al., 1995)) or with hidden Markov models (HMMs, e.g. in (Knerr et al., 1998)). The last method, most popular nowadays, is a segmentation-free approach. The goal is to predict a character sequence from the image without segmenting it first. The techniques include scanning a sliding window to extract features used in an HMM (e.g. in (Kaltenmeier et al., 1993)), or to feed the image to a neural network able to output sequences of character predictions (e.g. SDNNs (LeCun et al., 1998) or MDLSTM-RNN (Graves & Schmidhuber, 2009)). More recently, different approaches have been proposed to recognize words using character bigrams, and therefore closer to the method we propose in this paper. Jaderberg et al. (2014) propose to predict both the characters and ngrams of characters with two distinct convolutional neural networks (CNNs) to recognize text in natural images. Their approach includes a conditional random field as decoder. Similarly, Poznanski & Wolf (2016) train a CNN with a cross-entropy loss to detect common unigrams, bigrams or trigrams of character in a handwritten word image. The output of the network is matched against the lexicon using canonical correlation analysis. Almazan et al. (2014) use Fisher vectors from images and pyramidal character histograms, to learn a feature space shared by the word images and labels, for word spotting, also using canonical correlation analysis.",
      "exclude": true
    },
    {
      "heading": "3 PROPOSED METHOD",
      "text": "",
      "exclude": false
    },
    {
      "heading": "3.1 AN OPEN-BIGRAM REPRESENTATION OF WORDS",
      "text": "The letter bigrams of a word w is the set of pairs of consecutive letters. The open-bigram of order d is the set of pairs of letters separated by d other letters in the word, which we call Bd(w): Bd(w) = wiwi+d : i 1 . . . |w| d. (1) The usual bigrams are open-bigrams of order 1. By extension, we call B0(w) the set of letters in the word w. For example, for word word, we have: B1(word) = or, rd, wo ; B2(word) = od, wr ; B3(word) = wd. The general open-bigram representation of a word is the union of Bd1,...,dn(w) = Bd1(w) . . . Bdn(w). (2) For example, B1,2,3(word) = od, or, rd, wd, wo, wr. We extend B into B by including special bigrams for the letters at the beginning and end of a word: B(w) = B(w) w0, w|w| . (3) So, for example, B1,2,3(word) = w, d , od, or, rd, wd, wo, wr. (4) In this paper, we will call B the set of all bigrams, and W the set of all words. We will represent a word of the vocabulary w W as a normalized binary vector vwW <|B| vw = [(b B(w))]bB |B(w)| , (5) i.e. the vector with 0 everywhere and 1/ |B(w)| at indices corresponding to bigrams of the word. The stacking of the vector representation of all the words in the vocabulary yields the vocabulary matrix V <|W ||B|. Note that in this representation, the bigrams form an unordered set. We do not know: (i) where the bigrams are, (ii) what is the order of a given bigram, (iii) how many times it occurs. The goal is to build a word recognition decoder in the bigram space.",
      "exclude": false
    },
    {
      "heading": "3.2 AN OPEN-BIGRAM DECODER",
      "text": "While the trivial representation of a word is an ordered sequence of letters, the order in the bigram space is locally embedded in the bigram representation. Most state-of-the-art word recognition systems recognize sequences of letters, and organize the vocabulary for a constrained search as directed graphs, such as prefix trees, or Finite-State Transducers. On the other hand, we can interpret the bigram representation as encoding directed edges in a graph, although we will not explicitly build such a graph for decoding. On Figure 2, we show the graph for a representation of the word into a sequence of letters. Gray edges show the potential risk of a misrecognition in the letter sequences. On Figure 2(b), we display the conceptual representation of bigrams as edges. We observe that a global order of letters can emerge from the local representation. Moreover, the constituent information of a word in the bigram space is redundant, potentially making this representation more robust to mispredictions of the optical model. The optical model is the system which provides the predictions of bigrams from the image (or, in the classical approach sequences of character predictions). That is, it provides a confidence measure that each bigram b is present in image x: 0 pb(x) 1. This is transformed into a vector in the bigram space: qx = [pb(x)]bB b p 2 b(x) . (6) For decoding, we chose the very simple cosine similarity between the query (qx) and a vocabulary word (vw). Since we normalized both vectors, this is simply the dot product: d(qx,vw) = v T wqx, (7) so the similarity with all words of the vocabulary can be computed with a matrix-vector product: DV (x) = V Tqx. (8) The recognized word is the one with maximum similarity with the query: w = argmaxDV (x) = argmax w bB(w) pb(x) |B(w)| b p 2 b(x) . (9) We carried out a few preliminary experiments to justify the open-bigram decoder. First, we considered the famous sentence with mixed up letters: aoccdrnig to a rscheearch at cmabrigde uinervtisy it deos not mttaer in waht oredr the ltteers in a wrod are the olny iprmoatnt tihng is taht the frist and lsat ltteers be at the rghit pclae the rset can be a toatl mses and you can sitll raed it wouthit porbelm tihs is bcuseae the huamn mnid deos not raed ervey lteter by istlef but the wrod as a wlohe. Although the origin and validity of this statement when letters are put in the right order has been discussed 1, it is true that most of us can read it without trouble. For each word of more than one letter in this sentence, we computed the open-bigram representation (d = 0..3), and replaced it with the word having the highest cosine similarity in the English vocabulary described in the next section. The result was: according to a researcher at abridged university it does not matter in what ordered the letters in a word are the only important thing is that the first and last letters be at the right place the rest can be a total messes and you can still read it outwith problem this is because the human mind does not read every letter by itself but the word as a whole. Note that the word cambridge was not in the vocabulary. Although the task in this paper is not to recognize mixed up words, it shows the ability of our decoder to perform a reading task that we naturally do. 1http://www.mrc-cbu.cam.ac.uk/people/matt.davis/cmabridge/ On Figure 3, we show the English vocabulary in bigram space (d = 1..3), reduced to two dimensions with t-SNE (Van der Maaten & Hinton, 2008). We observe that words which are close in the bigram space also have a close orthographic form.",
      "exclude": false
    },
    {
      "heading": "4 EXPERIMENTAL SETUP",
      "text": "",
      "exclude": false
    },
    {
      "heading": "4.1 DATA PREPARATION",
      "text": "We carried out the experiments on two public handwritten word databases: Rimes (Augustin et al., 2006) (French), and IAM (Marti & Bunke, 2002) (English). We simplified the problem by limiting ourselves to words of at least two lowercase characters (a to z). This selection removed approximately 30% of the words. The number of words and bigrams of different orders in the different sets are reported on Table 4, in Appendix A.1. We applied deslanting (Buse et al., 1997), contrast enhancement, and padded the images with 10px of white pixels to account for empty context on the left and right of words. From the preprocessed images, we extracted sequences of feature vectors with a sliding window of width 3px. The features are geometrical and statistical features described in (Bianne et al., 2011), which give state-of-the-art results in handwritten text line recognition (Bluche et al., 2014). We downloaded word frequency lists for French and English2. These lists were built from film subtitles 3 written by many contributors, and they contain many misspellings. We removed the misspelled words using GNU Aspell (Atkinson). We selected 50,000 words for each language. They are the most frequent words (length 2) and made only of lowercase characters between a and z, making sure to also include all the words of the database. For example, the 50,000 most frequent French words fulfilling these condition miss about 200 words of the Rimes database, so we selected the most frequent 49,800 and added the missing 200. Note that most of the words that were removed from the dataset are not shorter words, but words with special or upper case characters. The distribution of lengths of filtered-out words is shown on Figure 5 in Appendix A.1.",
      "exclude": false
    },
    {
      "heading": "4.2 RECOGNITION OF OPEN-BIGRAMS WITH RECURRENT NEURAL NETWORKS (RNNS)",
      "text": "To predict bigrams, we chose Bidirectional Long Short-Term Memory RNNs (BLSTM-RNNs) for their ability to consider the whole sequence of input vectors to make predictions. We trained one RNN for each order-d bigram, with the Connectionist Temporal Classification (CTC (Graves et al., 2006)) criterion. The CTC framework defines a sequence labeling problem, with an output sequence of labels, of smaller length than the input sequence of observations. 2http://invokeit.wordpress.com/frequency-word-lists/. 3http://opensubtitles.org We built the target sequences for training as sequences of bigrams, ordered according to the first letter of the bigram. For example, for d = 2, the target for example is ea-xm-ap-ml-pe. The CTC training criterion optimizes the Negative Log-Likelihood (NLL) of the correct label sequence. We set the learning rate to 0.001, and stopped the training when the NLL on the validation set did not decrease for 20 epochs. We kept the network yielding the best NLL on the validation set. We trained one RNN for each order d = 0 to 3, including the special bigrams for word extremities or not. We will refer to each of these RNNs with rnnd for order d (rnnd when extremities are included). The architecture of the networks is described in Appendix A.3. These RNNs are trained to predict sequences of fixed order bigrams. Here, we are interested in a word representation as a bag of bigrams, which does not carry any information about the sequence in which the bigrams appear, the number of times each bigram appears, or the order of each individual bigram. That is, we are interested in a decoder which considers an unordered set of bigrams predictions across bigram orders. We forget the temporal aspect of bigram predictions by taking the maximum value of a given bigram prediction by the RNN (where rnnd(x, t) if the output of the RNN for order d, input image x at timestep t): pd,b(x) = max t rnnd(x, t), (10) and we forget the bigram order by taking the maximum output across different values of d: pb(x) = max d max t rnnd(x, t). (11) It would have been more satisfying for this experiment to train an optical model to predict a set of bigrams for all orders. However, this work is focused on the decoder. Moreover, even the simpler task of predicting a sequence of bigrams of fixed order is challenging (the sequence error rates of these networks are detailed in Appendix B.2). On Figure 4, we show the hypothetical context needed to make two consecutive predictions, for bigram order d = 0..3. RNNs are popular for handwriting recognition, and can consider a context size of variable length but still local to predict characters (d = 0). For d = 1, the required context is still local (and would span two consecutive characters), but overlap, because each character is involved in two bigrams. For d > 1, the context is even split into two areas (covering the involved characters) that might be far apart depending on d. Contexts for different predictions are entangled: the whole area between two characters forming a bigram is not relevant for this bigram (and might be of varying size), but will be important to predict other bigrams. It means that the RNN will have to remember a character observation for some time, until it sees the second character of the bigram, while ignoring the area in between for this bigram prediction, but remembering it since it will be useful in order to predict other bigrams. The number of classes for bigrams is also 26 times larger than the number of characters, making the classification problem harder, and the number of examples per class in training smaller.",
      "exclude": false
    },
    {
      "heading": "5 RESULTS",
      "text": "In this paper, we focused on a subset of Rimes and IAM word databases, which makes the comparison with published results difficult. Instead, we compared the bigram decoder approach to decoding with standard models, consisting of a beam search with Viterbi algorithm in the lexicon. However, these standard models yield state-of-the art results on the reference task for the two considered database (Bluche et al., 2014).",
      "exclude": false
    },
    {
      "heading": "5.1 BASELINE SYSTEMS BASED ON HMMS AND VITERBI DECODING",
      "text": "We built several models and used the same vocabulary as for the bigram decoder, and no language model (all words have the same prior probability). These baseline systems are based on HMMs, with emission models made either of Gaussian mixtures (GMM/HMM), Multi-Layer Perceptrons (MLP/HMM) or Recurrent Neural Networks (rnn0/HMM). They are almost identical to those presented in a previous work (Bluche et al., 2014), where a comparison is made with state-of-the-art systems for handwritten text line recognition. More details about these models and their training procedure are presented in Appendix A.2. On Table 1, we report the percentages of word errors on the validation and test sets of Rimes and IAM. The best word error rates are around 10% (17.5% on the test set of IAM), and constitute the baseline performance to which the bigram approach is to be compared.",
      "exclude": false
    },
    {
      "heading": "5.2 MEASURING THE QUALITY OF BIGRAM PREDICTIONS",
      "text": "Since we keep a confidence value for all bigrams in the prediction vector, rather than using a binary vector (cf. Eq. 6), we modified the formulation of precision and recall. A bigram b B(w) is correctly retrieved with confidence pb(x), and missed with confidence (1 pb(x)). Similarly, a bigram not in the representation B(w) of word w is falsely recognized with confidence pb(x), and correctly ignored with confidence (1 pb(x)). It gives us the following expressions for precision and recall precision = (x,w) bB(w) pb(x) x bB pb(x) , recall = (x,w) bB(w) pb(x) wW |B(w)| , (12) which are the usual ones when pb(x) 0, 1. The F-measure is calculated from precision and recall with the usual formula. The results for all RNNs, and for the combination of orders, are reported on Table 2. We observe that the precision and recall results are correlated to the performance in terms of edit distance or sequence error rates. Namely, they decrease as the bigram order increases, which is not surprising, given that higher order bigrams are more difficult to recognize with these sequence models. We also see that including the special bigrams for word beginnings and endings generally improves the results. This is not surprising either: the RNNs are good at recognizing them. Despite this performance decrease, the precision remains above 70%, which limits the amount of noise that will be included in the bigram representation for recognition. Combining the recognition across orders, we obtain a precision of around 84% on Rimes and 80% on IAM. The recall tends to be higher than the precision, staying around or above 80% in all configurations. Across orders, the recall is above 88% on Rimes and 86% on IAM. The high recall will limit the amount of missing information in the bigram representation. Overall, the F-measure for bigram recognition is above 80%, which is a good starting point, given that (i) the vocabulary used in decoding will add constraints and may help recovering from some mistakes in the bigram recognition, and (ii) the redundancy and order encoded in the bigram may limit the impact of misrecognitions.",
      "exclude": false
    },
    {
      "heading": "5.3 WORD RECOGNITION USING BIGRAM PREDICTIONS",
      "text": "On Table 3, we report the results of bigram decoding. For each word image in the validation and test sets, we computed the bigram predictions with the RNNs described above. We combined the different orders as explained previously, and either added the special bigrams for word boundaries and/or the single character predictions or not. We computed the cosine similarity to the bigram decomposition of all words in the vocabularies in the same representation space (i.e. same orders, and same choices for the inclusion of special bigrams and single characters) by computing the product of the vocabulary matrix V by the recognition vector. We counted the number of times the correct word was not the most similar one. We see that adding the special bigrams for word boundaries improves the results, especially when single characters are not included in the representation. A possible explanation, besides the fact that they tend to be recognized more easily, could be that they provide a very useful information to disambiguate words having a similar bigram representation (e.g. them and theme). Adding single characters also improves the performance of the decoder, especially when the boundary bigrams are not included in the representation. The gain obtained with the single characters is about the same sometimes a little better as the gain with boundaries. It might be due to the much better recognition of the RNN for single characters (precision and recall over 90%), as well as the added redundancy and complementary information provided. The results of decoding with different combinations of orders are presented in the appendices in Table 7. They confirm those observations. The best performance is achieved with both single characters and word boundaries, although the gain compared to adding only one of them is slight. The error rates are competitive or better than the best error rates obtained by classical character sequence modeling and Viterbi decoding.",
      "exclude": false
    },
    {
      "heading": "6 CONCLUSION",
      "text": "State-of-the-art systems, as well as most of the systems for handwritten word recognition found in the literature, either try to model words as a whole, or as a sequence of characters. The latter, which currently gives the best results, is widely adopted by the community, and benefits from a lot of attention. In this paper, we have proposed a simple alternative model, inspired by the recent findings in cognitive neurosciences research on reading. We focused on the representation of words in the open-bigram space and built an handwritten word recognition system operating in that space. We were interested in observing how a simple decoding scheme, based on a mere cosine similarity measure in the bigram space, compared to traditional methods. The main apparent difficulty arises from the fact that the global ordering of characters and the distance between bigram constituents are lost in this representation. The qualitative results presented in the first section showed that the envisioned approach was viable. With the letter reordering example, we have seen that the correct orthographic form of words can be retrieved with a limited and local knowledge of character orders. Moreover, we validated that words that are close in orthographic form are also close in the bigram space. Thus, we demonstrated that the open-bigram representation shows interesting and competitive metric properties for the word recognition. Current work consists in learning most discriminant open-bigram at different order, possibly higher than three according to the length of the word and its similarity to others.",
      "exclude": true
    },
    {
      "heading": "ACKNOWLEDGMENTS",
      "text": "This work was conducted in COGNILEGO project 2012-15, supported by the French Research Agency under the contract ANR 2010-CORD-013 http://cognilego.univ-tln.fr.",
      "exclude": true
    },
    {
      "heading": "A DATA AND MODELS",
      "text": "A.2 BASELINE SYSTEMS We built several models and used the same vocabulary as for the bigram decoder, and no language model (all words have the same prior probability). The first one is a Hidden Markov Model, with 5 (Rimes) or 6 (IAM) states per characters, and an emission model based on Gaussiam Mixture Models. This system is trained with the Maximum Likelihood criterion, following the usual ExpectationMaximization procedure. At each iteration, the number of Gaussians is increased, until no improvement is observed on the validation set. The forced alignments with the GMM/HMM system are used to build a labeled dataset for training a Multi-Layer Perceptron (MLP) with 4 hidden layers of 1,024 sigmoid units. We optimized the cross-entropy criterion to train the network to predict the HMM states from the concatenation of 11 input frames, with a learning rate of 0.008. The learning rate was halved when the relative improvement was smaller than 1% from one epoch to the next. The MLP is integrated in the hybrid NN/HMM scheme by dividing the predicted state posteriors p(s|x) by the state priors p(s), estimated from the forced alignments. It is then further trained with a sequence-discriminative criterion: state-level Minimum Bayes Risk (Kingsbury, 2009) (sMBR) for 5 epochs. Finally, we also performed the sequential decoding with vocabulary constraints using rnn0. The input features for all systems are the same, described in 4.1. The results of similar systems for handwritten text line recognition can for example be found in a previous work (Bluche et al., 2014), where a comparison is made with state-of-the-art systems. A.3 RECURRENT NEURAL NETWORK The RNNs, depicted on Figure 6, have seven hidden layers, alternating Long Short-Term Memory (Hochreiter & Schmidhuber, 1997) recurrent layers in both direction and feed-forward layers. The first LSTM layers have 100 hidden LSTM units. The BLSTM outputs of two consecutive timesteps of both directions are fed to a feed-forward layer with 100 nodes, halving the size of the sequence. The next LSTM layers have 150 units, and are connected to a feed-forward layer with 150 units. The last two LSTM layers and feed-forward layer have 200 hidden units. The inputs of the networks are sequences of geometrical and statistical features (described in (Bianne et al., 2011)), extracted with a sliding window of width 3px. The outputs are sequences of openbigram predictions. The networks are trained with stochastic gradient descent to minimize the Connectionist Temporal Classification (CTC (Graves et al., 2006)) criterion, i.e. the Negative Log-Likelihood (NLL) of the correct label sequence. We set the learning rate to 0.001, and stopped the training when the NLL on the validation set did not decrease for 20 epochs, and kept the network yielding the best NLL on the validation set.",
      "exclude": false
    },
    {
      "heading": "B RESULTS",
      "text": "B.1 CORRELATION BETWEEN CHARACTER EDIT DISTANCES AND OB COSINE SIMILARITIES On Figure 7, we randomly selected pairs of words, and pairs of words with high cosine similarity in the French and English dictionary, and plotted their cosine similarity in the bigram space against the edit distance between the two words, normalized by the length of the longest word. We note that words with high cosine similarity also have short edit distance, supporting the idea that the bigram representation encode some global letter order, and therefore might favor word recognition. B.2 EVALUATION OF PERFORMANCE OF RNN ALONE We trained RNNs to predict sequences of bigrams of a given order. Their performance to accomplish this task can be measured, on the validation set, with the edit distance (edit.dist) between the recognized sequence and the true sequence, and with the percentage of sequences with at least one error: the Sequence Error Rate (SER). The results are reported on Table 6. We observe that the performance decreases as d increase. Yet the errors remain in a reasonable range compared to the simple case d = 0, and one should keep in mind that a small degradation should be expected from the larger number of classes and smaller number of training examples per class. We have shown that RNNs can perform the apparently difficult task of recognizing sequences of open-bigrams. Thus, we may use the RNN bigram predictions in our decoder, rather than building bigram predictions from letter ones, which would have been less satisfying considering the supposed interest of the redundancy of the bigram representation that allows error corrections. B.3 EVALUATION OF PERFORMANCE OF THE DECODER In Table 7, we report the results of the cosine decoder applied to the outputs of OB RNNs with different bigram orders. In Table 8, we evaluate the cosine decoder by applying it to the ground-truth OB decomposition with varying order of the validation sets, i.e. the performance assuming the RNN optical models are perfect. B.4 ERROR ANALYSIS B.4.1 COMPARISON OF ERRORS OF THE SEQUENTIAL AND OPEN-BIGRAM MODELS",
      "exclude": false
    }
  ],
  "fewShot": [
    {
      "model": "claude-haiku-4-5",
      "decision": {
        "rejection": true,
        "confidence": 0.85,
        "primary_reason": "Limited novelty and weak experimental validation. Open-bigrams are a known cognitive concept; the contribution is primarily engineering (LSTM + cosine similarity decoder). Results are merely comparable to baselines, not superior. Narrow application domain (handwritten word recognition), no significant methodological innovation, and experiments lack depth in analysis of why this approach should be preferred."
      },
      "token": {
        "prompt_tokens": 5920,
        "completion_tokens": 109,
        "total_tokens": 6029
      },
      "time": "2026-02-08T22:45:10.961598+00:00"
    },
    {
      "model": "gpt-5.2",
      "decision": {
        "rejection": true,
        "confidence": 0.78,
        "primary_reason": "Interesting cognitive-inspired representation, but the technical contribution is modest (bag-of-open-bigrams + cosine retrieval) and the empirical case is not at top-tier level: the approach relies on multiple separately trained CTC BLSTMs and then discards sequence structure via max-pooling, comparisons are mainly to older HMM/Viterbi baselines under a nonstandard filtered setting (hard to position vs strong modern end-to-end recognizers), and gains are described as merely comparable with limited ablations/analysis to justify why this decoder is preferable or broadly impactful."
      },
      "token": {
        "prompt_tokens": 7924,
        "completion_tokens": 137,
        "total_tokens": 8061
      },
      "time": "2026-02-09T20:48:25.328367+00:00"
    }
  ],
  "test": false
}