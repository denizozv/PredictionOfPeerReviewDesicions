{
  "id": "322",
  "title": "Nonparametric Neural Networks",
  "abstract": "Automatically determining the optimal size of a neural network for a given task without prior information currently requires an expensive global search and training many networks from scratch. In this paper, we address the problem of automatically finding a good network size during a single training cycle. We introduce {\\it nonparametric neural networks}, a non-probabilistic framework for conducting optimization over all possible network sizes and prove its soundness when network growth is limited via an $\\ell_p$ penalty. We train networks under this framework by continuously adding new units while eliminating redundant units via an $\\ell_2$ penalty. We employ a novel optimization algorithm, which we term ``Adaptive Radial-Angular Gradient Descent'' or {\\it AdaRad}, and obtain promising results.",
  "accepted": true,
  "zeroShot": [
    {
      "model": "claude-haiku-4-5",
      "decision": {
        "rejection": true,
        "confidence": 0.72,
        "primary_reason": "Mixed experimental results with inconsistent performance across datasets, limited scalability validation, and incremental methodological contributions that don't sufficiently justify the added complexity of the framework."
      },
      "token": {
        "prompt_tokens": 13519,
        "completion_tokens": 62,
        "total_tokens": 13581
      },
      "time": "2026-02-05T22:35:04.073396+00:00"
    }
  ],
  "article": [
    {
      "heading": "1 INTRODUCTION",
      "text": "Automatically choosing a neural network model for a given task without prior information is a challenging problem. Formally, let be the space of all models considered. The goal of model selection is then, usually, to find the value of the hyperparameter that minimizes a certain criterion c(), such as the validation error achieved by the model represented by when trained to convergence. Because is large, structured and heterogeneous, c is complex, and gradients of c are generally not available, the most popular methods for optimizing c perform zero-order, black-box optimization and do not use any information about c except its value for certain values of . These methods select one or more values of , compute c at those values and, based on the results, select new values of until convergence is achieved or a time limit is reached. The most popular such methods are grid search, random search (e.g. Bergstra & Bengio (2012)) and Bayesian optimization using Gaussian processes (e.g. Snoek et al. (2012)). Others utilize random forests (Hutter et al., 2009), deep neural networks (Snoek et al., 2015) and recently Bayesian neural networks (Springenberg et al., 2016) and reinforcement learning (Zoph & Le, 2017). These black-box methods have two drawbacks. (A) To obtain each value of c, they execute a full network training run. Each run can take days on many cores or multiple GPUs. (B) They do not exploit opportunities to improve the value of c further by altering during each training run. In this paper, we present a framework we term nonparametric neural networks for selecting network size. We dynamically and automatically shrink and expand the network as needed to select a good network size during a single training run. Further, by altering network size during training, the network ultimately chosen can achieve a higher accuracy than networks of the same size that are trained from scratch and, in some cases, achieve a higher accuracy than is possible by black-box methods. There has been a recent surge of interest in eliminating unnecessary units from neural networks, either during training or after training is complete. This strategy is called pruning. Alvarez & Salzmann (2016) utilize an `2 penalty to eliminate units and Molchanov et al. (2017) compare a variety of strategies, whereas Figurnov et al. (2016) focuses on thinning convolutional layers in the spatial dimensions. While some of these methods even allow some previously pruned units to be added back in (e.g. Feng & Darrell (2015)), all of these strategies require a high-performing network model as a starting point from which to prune, something that is generally only available in well-studied vision and NLP tasks. We do not require such a starting point in this paper. In section 2, we introduce the nonparametric framework and state its theoretical soundness, which we prove in section 7.1. In section 3, we develop the machinery for training nonparametric networks, including a novel normalization layer in section 3.2, CapNorm, and a novel training algorithm in section 3.3, AdaRad. We provide experimental evaluation and analysis in section 4, further relevant literature in section 5 and conclude in section 6.",
      "exclude": true
    },
    {
      "heading": "2 NONPARAMETRIC NEURAL NETWORKS",
      "text": "For the purpose of this section, we define a parametric neural network as a function f(x) = L.(L1.(..2.(1.(xW1)W2)..)WL) of a d0-dimensional row vector x, where Wl Rdl1dl , 1 l L are dense weight matrices of fixed dimension and l : R R, 1 l L are fixed non-linear transformations that are applied elementwise, as signified by the .() operator. The number of layers L is also fixed. Further, the weight matrices are trained by solving the minimization problem minW=(W )l 1 |D| (x,y)D e(f(W, x), y) + (W), where D is the dataset, e is an error function that consumes a vector of fixed size dL and the label y, and is the regularizer. We define a nonparametric neural network in the same way, except that the dimensionality of the weight matrices is undetermined. Hence, the optimization problem becomes min d=(d)l,dlZ+,1lL1 min W=(W )l,WlRdl1dl ,1lL 1 |D| (x,y)D e(f(W, x), y) + (W) (1) Note that the dimensions d0 and dL are fixed because the data and the error function e are fixed. The parameter value now takes the form of a pair (d,W). There is no guarantee that optimization problem 1 has a global minimum. We may be able to reduce the value of the objective further and further by using larger and larger networks. This would be problematic, because as networks become better and better with regards to the objective, they would become more and more undesirable in practice. It turns out that in an important case, this degeneration does not occur. Define the fan-in regularizer in and the fan-out regularizer out as in(W, , p) = L l=1 dl j=1 ||[Wl(1, j),Wl(2, j), ..,Wl(dl1, j)]||p (2) out(W, , p) = L l=1 dl1 i=1 ||[Wl(i, 1),Wl(i, 2), ..,Wl(i, dl)]||p (3) In plain language, we either penalize the incoming weights (fan-in) of each unit with a p-norm, or the outgoing weights (fan-out) of each unit. We now state the core theorem that justifies our formulation of nonparametric networks. The proof is found in the appendix in section 7.1. Theorem 1. Nonparametric neural networks achieve a global training error minimum at some finite dimensionality when is a fan-in or a fan-out regularizer with > 0 and 1 p <.",
      "exclude": false
    },
    {
      "heading": "3 TRAINING NONPARAMETRIC NETWORKS",
      "text": "Training nonparametric networks is more difficult than training parametric networks, because the space over which we optimize the parameter (d,W) is no longer a space of form Rd, but is an infinite, discrete union of such spaces. However, we would still like to utilize local, gradient-based search. We notice, like (Wei et al., 2016), that there are pairs of parameter values with different dimensionality that are still in some sense close to one another. Specifically, we say that two parameter values (d1,W1) and (d2,W2) are f -equivalent if x Rd0 , f(W1, x) = f(W2, x) where not necessarily d1 = d2. During iterative optimization, we can jump between those two parameter values while maintaining the output of f and thus preserving locality. We define a zero unit as any unit for which either the fan-in or fan-out or both are the zero vector. Given any parameter value, the most obvious way of generating another parameter value that is f -equivalent to it is to add a zero unit to any hidden layer l where l(0) = 0 holds. Further, if we have a parameter value that already contains a zero unit in such a hidden layer, removing it yields an f -equivalent parameter value. Thus, we will use the following strategy for training nonparametric networks. We use gradient-based methods to adjust W while periodically adding and removing zero units. We use only nonlinearities that satisfy (0) = 0. It should be noted that while adding and removing zero units leaves the output of f invariant, it does change the value of the fan-in and fan-out regularizers and thus the value of the objective. While it is possible to design regularizers that do not penalize such zero units, this is highly undesirable as it would stifle the regularizers ability to reign in the growth of the network during training. To be able to reduce the network size during training, we must produce zero units and, it turns out, the fan-in and fan-out regularizers naturally produce such units as they induce sparsity, i.e. they cause individual weights to become exactly zero. This is well studied under the umbrella of sparse regression (see e.g. Tibshirani (1996)). The cases p = 1 and p = 2 are especially attractive because it is computationally convenient to integrate them into a gradient-based optimization framework via a shrinkage / group shrinkage operator respectively (see e.g. Back & Teboulle (2006)). Further, p = 1 and p = 2 differ in their effect on the parameter value. p = 1 sets individual weights to zero and thus leads to sparse fan-ins and fan-outs and thus ultimately to sparse weight matrices. A unit can only become a zero unit if each weight in its fan-in or each weight in its fan-out has been set to zero individually. p = 2, on the other hand, sets entire fan-ins (for the fan-in regularizer) or fan-outs (for the fan-out regularizer) to zero at once. Once the resulting zero units are removed, we obtain dense weight matrices. (For a basic comparison of 1-norm and 2-norm regularizers, see Yuan & Lin (2006) and for a comparison in the context of neural networks, see Collins & Kohli (2014).) While there is recent interest in learning very sparse weight matrices (e.g. Guo et al. (2016)), current hardware is geared towards dense weight matrices (Wen et al., 2016). Hence, for the remainder of this paper, we will focus on the case p = 2. Further, we will focus on the fan-in rather than the fan-out regularizer. When a new zero unit is added, we must choose its fan-in and fan-out. While one of the two weight vectors must be zero, the other can have an arbitrary value. We make the simple choice of initializing the other weight vector randomly. Since we are going to use the fan-in regularizer, we will initialize the fan-out to zero and the fan-in randomly. This will give each new unit the chance to learn and become useful before the regularizer can shrink its fan-in to zero. If it does become zero nonetheless, the unit is eliminated.",
      "exclude": false
    },
    {
      "heading": "3.1 SELF-SIMILAR NONLINEARITIES",
      "text": "For layers 1 through L 1, it is best to use nonlinearities that satisfy (cs) = c(s) for all c R0 and s R. We call such nonlinearities self-similar. ReLU (Dahl et al., 2013) is an example of this. Self-similarity also implies (0) = 0. Recall that the fan-in and fan-out regularizers shrink the values of weights during training. This in turn affects the scale of the values to which the nonlinearities are applied. (These values are called pre-activations.) The advantage of self-similar nonlinearities is that this change of scale does not affect the shape of the feature. In contrast, the impact of a nonlinearity such as tanh on pre-activations varies greatly based on their scale. If the pre-activations have very large absolute values, tanh effectively has a binary output. If they have very small absolute values, tanh mimics a linear function. In fact, all nonlinearities that are differentiable at 0 behave approximately like a linear function if the pre-activations have sufficiently small absolute values. This would render the unit ineffective. Since we expect some units to have small pre-activations due to shrinkage, this is undesirable. By being invariant to the scale of pre-activations, self-similar nonlinearities further eliminate the need to tune how much regularization to assign to each layer. This is expressed in the following proposition which is proved in section 7.2. Proposition 1. If all nonlinearities in a nonparametric network model except possibly L are selfsimilar, then the objective function 1 using a fan-in or fan-out regularizer with different regularization parameters 1, .., L for each layer is equivalent to the same objective function using the single regularization parameter = ( L l=1 l) 1 L for each layer, up to rescaling of weights. 1 input: r: radial step size; : angular step size; : regularization hyperparameter; : mixing rate; : numerical stabilizer; d0: initial dimensions; W0: initial weights; : unit addition rate; freq: unit addition frequency; T : number of iterations 2 max = 0; cmax = 0; d = d0; W = W0; 3 for l = 1 to L do 4 set l (angular quadratic running average) and cl (angular quadratic running average capacity) to zero vectors of size d0l ; 5 end 6 for t = 1 to T do 7 set Dt to mini-batch used at iteration t; 8 G = 1|D|W (x,y)Dt e(f(W, x), y); 9 for l = L to 1 do 10 for j = dl to 1 do 11 decompose [Gl(i, j)]i into a component parallel to [Wl(i, j)]i (call it r) and a component orthogonal to [Wl(i, j)]i (call it ) such that [Gl(i, j)]i = r + ; 12 l(j) = (1 )l(j) + ||||22; cl(j) = (1 )cl(j) + ; 13 max = max(max, l(j)); cmax = max(cmax, cl(j)) ; 14 adj = max cmax l(j) cl(j) + ; 15 [Wl(i, j)]i = [Wl(i, j)]i rr ; 16 rotate [Wl(i, j)]i by angle ||adj||2 in direction adj||adj||2 ; 17 shrink([Wl(i, j)]i, r |Dt| |D| ); 18 if l < L and [Wl(i, j)]i is a zero vector then 19 remove column j from Wl; remove row j from Wl+1; remove element j from l and cl; decrement dl; 20 end 21 end 22 if t = 0 mod freq then 23 = ; // if 6 Z, we can set e.g. = Poisson() 24 add randomly initialized columns to Wl; add zero rows to Wl+1; add zero elements to l and cl; dl = dl + ; 25 end 26 end 27 end 28 return W; Algorithm 1: AdaRad with `2 fan-in regularizer and the unit addition / removal scheme used in this paper in its most instructive (bot not fastest) order of computation. Note that []i notation is used to indicate a vector over index i. 3.2 CAPPED BATCH NORMALIZATION (CapNorm) Recently, Ioffe & Szegedy (2015) proposed a strategy called batch normalization that quickly became the standard for keeping feed-forward networks well-conditioned during training. In our experiments, nonparametric networks trained without batch normalization could not compete with parametric networks trained with it. Batch normalization cannot be applied directly to nonparametric networks with a fan-in or fan-out regularizer, as it would allow us to shrink the absolute value of individual weights arbitrarily while compensating with the batch normalization layer, thus negating the regularizer. Hence, we make a small adjustment which results in a strategy we term capped batch normalization or CapNorm. We subtract the mean of the pre-activations of each hidden unit, but only scale their standard deviation if that standard deviation is greater than one. If it is less than one, we do not scale it. Also, after the normalization, we do not add or multiply the result with a free parameter. Hence, CapNorm replaces each pre-activation z with zmax(,1) , where is the mean and is the standard deviation of that units pre-activations across the current mini-batch. 3.3 ADAPTIVE RADIAL-ANGULAR GRADIENT DESCENT (AdaRad) The staple method for training neural networks is stochastic gradient descent. Further, there are several popular variants: momentum and Nesterov momentum (Sutskever et al., 2013), AdaGrad (Duchi et al., 2011) and AdaDelta (Zeiler, 2012), RMSprop (Tieleman & Hinton, 2012) and Adam (Kingma & Ba, 2015). All of these methods center around two key principles: (1) averaging the gradient obtained over consecutive iterations to smooth out oscillations and (2) normalizing each component of the gradient so that each weight learns at roughly the same speed. Principle (2) turns out to be especially important for nonparametric neural networks. When a new unit is added, it does not initially contribute to the quality of the output of the network and so does not receive much gradient from the loss term. If the gradient is not normalized, that unit may take a very long time to learn anything useful. However, if we use a fan-in regularizer, we cannot normalize the components of the gradient outright as in e.g. RMSprop, as we would also have to scale the amount of shrinkage induced by the regularizer accordingly. This, in turn, would cause the fan-in of new units to become zero before they can learn anything useful. We resolve this dilemma with a new training algorithm: Adaptive Radial-Angular Gradient Descent (AdaRad), shown in algorithm 1. Like in all the algorithms cited above, we begin each iteration by computing the gradient G of the loss term over the current mini-batch (line 8). Then, for each 1 l L and 1 j dl, we decompose the sub-vector [Gl(1, j), Gl(2, j), .., Gl(dl1, j)] into a component parallel to its corresponding fan-in [Wl(1, j),Wl(2, j), ..,Wl(dl1, j)] and a component orthogonal to it (line 11). Out of the two, we normalize only the orthogonal component (line 14) while the parallel component is left unaltered. Finally, the normalized orthogonal component of each sub-vector is added to its corresponding fan-in in radial-angular coordinates instead of cartesian coordinates (line 16). This ensures that it does not affect the length of the fan-in. Like the parallel component, we leave the induced shrinkage unaltered. Note that `2 shrinkage acts only to shorten the length of each fan-in, but does not alter its direction. Hence, AdaRad with an `2 regularizer applies a normalized shift to each fan-in that alters its direction but not its length (angular shift), as well as an un-normalized shift that includes shrinkage that alters the length of the fan-in but not its direction (radial shift, lines 15 and 17). AdaRad has two step sizes: One for the radial and one for the angular shift, r and respectively. This is desirable as they both control the behavior of the training algorithm in different ways. The radial step size controls how long it takes for the fan-in of a unit to be shrunk to zero, i.e. the time a unit has to learn something useful. On the other hand, the angular step size controls the general speed of learning and is tuned to achieve the quickest possible descent along the error surface. Like RMSprop and unlike Adam, AdaRad does not make use of the principle of momentum. We have developed a variant called AdaRad-M that does. It is described in the appendix in section 7.3. Using AdaRad over SGD incurs additional computational cost. However, that cost scales more gracefully than the cost of, for example, RMSprop. AdaRad normalizes at the granularity of fan-ins instead of the granularity of individual weights, so many of its operations scale only with the number of units and not with the number of weights in the network. In Table 1, we compare the costs of SGD, AdaRad and RMSprop. Further, RMSprop has a larger memory footprint than AdaRad. Compared to SGD, it requires an additional cache of size equal to the number of weights, whereas AdaRad only requires 2 additional caches of size equal to the number of units.",
      "exclude": false
    },
    {
      "heading": "4 EXPERIMENTS",
      "text": "We evaluated our framework using the network architecture shown in Figure 1 with ReLU nonlinearities and CapNorm, and using AdaRad as the training algorithm. We used two hidden layers (L = 3) and started off with ten units in each hidden layer and each fan-in initialized randomly with expected length 1. We add one new unit with random fan-in of expected length 1 and zero fan-out to each layer every epoch. While this does not lead to fast convergence - we have to wait until tens or hundreds of units are added - we believe that growing nets from scratch is a good test case for investigating the robustness of our framework. After the validation error stopped improving, we ceased adding units, allowing all remaining redundant units to be eliminated. We set r = 150 , as this allows each new unit 50 epochs to train before being eliminated by shrinkage, assuming the length of the fan-in is not altered by the gradient of the loss term. When training parametric networks, we replaced CapNorm with batch normalization, either with or without trainable free mean and variance parameters. We trained the network using one of the following algorithms: SGD, momentum, Nesterov momentum, RMSprop or Adam. Further experimental details can be found in the appendix in section 7.4.",
      "exclude": false
    },
    {
      "heading": "4.1 PERFORMANCE",
      "text": "In this section, we investigate our two core questions: (A) Do nonparametric networks converge to a good size? (B) Do nonparametric networks achieve higher accuracy than parametric networks? We evaluated our framework using three standard benchmark datasets - the mnist dataset, the rectangles images dataset and the convex dataset (Bergstra & Bengio, 2012). We started by training nonparametric networks. Through preliminary experiments, we determined a good starting angular step size for all datasets. We chose to start with = 30 and repeatedly divided by 3 when the validation error stopped improving. By varying the random seed, we trained 10 nets each for several values of the regularization parameter per dataset and then chose a typical representative from among those 10 trained nets. Results are shown in black in figure 2. Values of are 3 103, 103 and 3 104 for MNIST, 3 105 and 106 for rectangles images and 105 and 108 for convex. Then, we trained parametric networks of the same size as the chosen representatives. The top performers after an exhaustive grid search are shown in red in figure 2. Finally, we conducted an exhaustive random search where we also varied the size of both hidden layers. The top performers are shown in blue in the same figure. We obtain different results for the three datasets. For mnist, nonparametric networks substantially outperform parametric networks of the same size. The best nonparametric network is close in performance to the best parametric network, while being substantially smaller (144 first layer units versus 694). For rectangles images, nonparametric networks underperform parametric networks of the same size when is large and outperform them when is small. Here, the best nonparametric network has the globally best performance, as measured by the median test error over 10 random reruns, using substantially fewer parameters than the best parametric network. While results for the first two datasets are very promising, nonparametric networks performed badly on the convex dataset. Parametric networks of the same size perform substantially better and also have a smaller range of performance across random reruns. Even if the model found by training nonparametric networks were re-trained as a parametric network, the apparent tendency of nonparametric networks to converge to relatively small sizes hurts us here as we would still miss out on a significant amount of performance. We also conducted experiments with AdaRad-M, but found that performance was very similar to that of AdaRad. Hence, we omit the results. Similarly, we found no significant difference in performance between parametric networks trained with RMSprop and those trained with Adam.",
      "exclude": false
    },
    {
      "heading": "4.2 ANALYSIS OF THE NONPARAMETRIC TRAINING PROCESS",
      "text": "In this section, we analyze in detail a single training run of a nonparametric network. We chose mnist as dataset, set = 3 104 and lowered the angular step size to 10 as we did not use step size annealing. We trained for 1000 epochs while adding one unit to each hidden layer per epoch, then trained another 1000 epochs without adding new units. The final network had 193 units in the first hidden layer and 36 units in the second hidden layer. The results are shown in figure 3. In part (A), we show the validation classification error. As a comparison, we trained two parametric networks with 193 and 36 hidden units for 1000 epochs, once using SGD and the same step size and as the nonparametric network, and once using optimal settings (RMSprop, = 300, = 0). It is not suprising that the parametric networks reach a good accuracy level faster, as the nonparametric network must wait for its units to be added. Also, the parametric network benefits from an increased step size - in this case = 300. This was true throughout our experimental evaluation. In (B), we show the training cross-entropy error for the same training runs. Interestingly, parametric networks reach an error very close to zero. In fact, the unregularized network reaches a value of 106 and the regularized network reaches a value of 104. Both made zero classification mistakes on the training set after training. In contrast, the nonparametric network did not have a near-zero training cross-entropy error. Towards the end of training, it still misclassified around 30 out of 50.000 training examples. However, this did not harm its performance on the validation or test set. In fact, the validation error of nonparametric networks tended to improve slowly for many epochs, whereas unregularized parametric networks (which were the best parametric networks when early stopping is used) tended to have a slightly increasing validation error in the long run. In (C), we show the size of the two hidden layers during training. These curves are very typical of all training runs we examined. For the first 50 epochs, no units are eliminated. This is because we chose r = 150 , which guarantees that units that are added with a fan-in of length 1 take 50 epochs to be eliminated, assuming no impact from the gradient of the loss term. If the layer requires a relatively large number of units, it will keep growing linearly for a while and then either plateau or shrink slightly. Once we no longer add units after 1000 epochs, both layers shrink linearly by 50 units over 50 iterations, as the units that were added roughly between epochs 950 and 1000 are eliminated in succession. Overall, this process shows the value of controlling and r independently, as we can manage the overhead of extraneous units present during training while still ensuring an ideal speed of learning. In (D), we show the length of time individual units in the first hidden layer were present during training. On the x axis, we show the epoch during which a given unit was added. On the y axis, we show the number of epochs the unit was present. Green bars represent units that survived until the end, while black bars represent units that did not. As one might expect, units were more likely to survive the earlier they were added. Units that did not survive were eliminated in 50 epochs. The same graph for the second hidden layer is shown in figure 4. In (E) and (F), we show the lengths of fan-ins (blue) and fan-outs (red) of units in the hidden layers. For each layer, we depict the following units in dark colors: three randomly chosen units that were initially present as well as units that were added at epochs 0, 25, 50, 100, 200, 300, .., 1000. In addition, in light colors, we show three units that were added late but not eliminated. We see a consistent pattern for individual units. First, their length decreases linearly as the CapNorm layer filters the component of the gradient parallel to the fan-ins as long as the standard deviation of the pre-activations exceeds 1. During this period, the unit learns something useful and so the fan-out increases in length. When finally < 1, the parallel component of the gradient starts to slow down the decay and, if the unit has become useful enough, reverses it. If the decay is not reversed, the unit is eliminated. If it is reversed, both fan-in and fan-out will attain a length comparable to those of well-established units. From a global perspective, we notice that fan-ins in the first layer have lengths much less than 1. This is because first layer units encode primarily AND functions of highly correlated input features, meaning weights of small magnitude are sufficient to attain = 1. In contrast, lengths of fan-ins in the second layer are more chaotic. We found this is because = 1 is generally NOT attained in the second layer. In fact, the network compensated for lower activation values in the second layer by assigning fan-ins of stable lengths between 3.5 and 4.5 to the 10 output units. The network can assign these lengths dynamically without altering the output of the network because ReLU is self-similar, as described in section 3.1.",
      "exclude": false
    },
    {
      "heading": "4.3 SCALABILITY",
      "text": "Finally, we wanted to verify whether nonparametric networks could be applied to a large dataset. We visited OpenML http://www.openml.org/, a website containing many datasets as well as the performance of various machine learning models applied to those datasets. We applied nonparametric networks to the largest classification dataset 1 on OpenML meeting our standards 2. This was the poker dataset http://www.openml.org/d/354. It is a binary classification dataset with 1.025.010 datapoints and 14 features per datapoint. We had no prior information about this dataset. In general, we think that nonparametric networks are most useful in cases with no prior information and thus no possibility of choosing a good parametric model a priori. We made the following changes to the experimental setup for poker: (i) we used 4 hidden layers instead of 2 (ii) we added a unit every tenth of an epoch instead of every epoch and (iii) we multiplied the radial step size by 10, i.e. r = 15 . The latter two changes were made as poker is approximately one order of magnitude larger than mnist, and we wanted to approximately preserve the rate of unit addition and elimination per mini-batch. Those changes were made a priori and were not based on examining their performance. After some exploration, we set the starting angular step size for nonparametric networks to 10. We trained nonparametric networks for various values of , obtaining nets of different sizes. We then trained parametric networks of those same sizes with RMSprop, where the step size was chosen by validation, independently for each network size. 1in terms of number of datapoints 2our standards were: at least 10 published classification accuracy values; no published classification accu- racy values exceeding 95%; no extreme label imbalance The results are shown in Table 2. Both parametric and nonparametric networks perform very well, achieving less than 1% test error even for small networks. The nonparametric networks had a higher error for larger values of and a slightly lower error for smaller values of . In fact, the best nonparametric network made no mistake on the test set of 100.000 examples. For comparison, we show that linear models perform roughly as well as random guessing on poker. Also, the best result published on OpenML, achieved by a decision tree classifier, vastly underperforms our 4-hidden layer networks. To achieve convergence, networks required many more mini-batches on poker than they did on the smaller datasets used in section 4.1. However, since units were added to the nonparametric networks at roughly the same rate per mini-batch, the time it took those networks to converge to a stable network size (as in Figure 3C) was a much smaller fraction of the overall training time under poker compared to the smaller datasets. Thus, the downside of increased training time as shown in Figure 3A incurred when networks are built gradually was ameliorated.",
      "exclude": false
    },
    {
      "heading": "5 FURTHER BACKGROUND",
      "text": "Several strategies have been introduced to address the drawbacks of black-box model selection. Maclaurin et al. (2015) indeed calculate the gradient of the validation error after training with respect to certain hyperparameters, though their method only applies to specific networks trained with very specific algorithms. (Luketina et al., 2016) and (Larsen et al., 1998) train certain hyperparameters jointly with the network using second order information. Such methods are limited to continuous hyperparameters and are often applied specifically to regularization hyperparameters. Several papers try to speed up the global model search by estimating the validation error of trained networks without fully training them. Saxe et al. (2011) use the validation error with randomly initialized convolutional layers as a proxy. Klein et al. (2017) predict the validation error after training based on the progress made during the first few epochs. Several papers have achieved increased performance by growing networks during training. Our main inspiration was Wei et al. (2016), who utilize a notion similar to our f -equivalence, though they enlarge their network in a somewhat ad-hoc way. The work of Chen et al. (2016) is similar, but focuses on convergence speed. Pandey & Dukkipati (2014) transform a trained small network into a larger network by multiplying weight matrices with large, random matrices. The performance of a network of given size can be improved by injecting knowledge from other nets trained on the same task. Ba & Caruana (2014) use the predictions of a large network on a dataset to train a smaller network on those predictions, achieving an accuracy comparable to the large network. Hinton et al. (2015) compress the information stored in an ensemble of networks into a single network. Simonyan & Zisserman (2015) train very deep convolutional networks by initializing some layers with the trained layers of shallower networks. Romero et al. (2015) train deep, thin networks utilizing hints from wider, shallower networks. Bayesian neural networks (e.g. McKay (1992), De Freitas (2003)) use a probabilistic prior instead of a regularizer to control the complexity of the network. Gaussian processes can been used to mimick infinitely wide neural networks (e.g. Williams (1997), Hazan & Jaakkola (2015)), thus eliminating the need to choose layer width and replacing it with the need to choose a kernel. Compared to these and other Bayesian approaches, we work within the popular feed-forward function optimization paradigm, which has advantages in terms of computational and algorithmic complexity. Adding units to a network one at a time is an idea with a long history. Ash (1989) adds units to a single hidden layer, whereas Gallant (1986) builds up pyramid and tower structures and Fahlman & Lebiere (1990) effectively create a new layer for each new unit. While these papers provided inspiration to us, the methods they present for determining when to add a new unit requires training the network to convergence first, which is impractical in modern settings. We circumvent this problem by adding units agnostically and providing a mechanism for removing unnecessary units.",
      "exclude": false
    },
    {
      "heading": "6 CONCLUSION",
      "text": "We introduced nonparametric neural networks - a simple, general framework for automatically adapting and choosing the size of a neural network during a single training run. We improved the performance of the trained nets beyond what is achieved by regular parametric networks of the same size and obtained results competitive with those of an exhaustive random search, for two of three datasets. While we believe there is room for performance improvement in several areas - e.g. unit initialization, unit addition schedule, additional regularization and starting network size - we see this paper as validation of the basic concept. We also proved the theoretical soundness of the framework. In future work, we plan to extend our framework to include convolutional layers and to automatically choosing the depth of networks, as done by e.g. Wen et al. (2016). Part of our motivation to develop nonparametric networks was to control the layer size via a continuous parameter. We want to make use of this by tuning during training, either by simple annealing or in a comprehensive framework such as the one introduced in Luketina et al. (2016). We want to use nonparametric networks to learn more complicated network topologies for e.g. semi-supervised or multi-task learning. Finally, we plan to investigate the possibility of sampling units with different nonlinearities and training an ever-growing network for lifelong learning.",
      "exclude": true
    },
    {
      "heading": "7 APPENDIX",
      "text": "",
      "exclude": true
    },
    {
      "heading": "7.1 PROOF OF THEOREM 1",
      "text": "First, we restate the theorem formally. Theorem 1. For all L, d0, dL Z+ finite datasets D of points (x, y) with x Rd0 and y Y for some set Y sets of nonlinearities l : R R, 1 l L where each l fulfils the following conditions: There exists a function b1,l : R0 R0 such that for all S R0, S s S, we have |l(s)| b1,l(S) |s|. It is left- and right-differentiable everywhere. There exists a function b2,l : R0 R0 such that for all S R0, S s S, we have |l (s)| b2,l(S) and |l (s)| b2,l(S), where the superscripts indicate directional derivatives. error functions e : (RdL Y ) R that fulfils the following conditions: It is non-negative everywhere. It is differentiable with respect to its first argument everywhere. There exists a function b3 : R0 R0 such that for all S R0, v RdL and y Y , we have e(v, y) S = ||de(v,y)dv || b3(S) > 0 and 1 p B for all W WB . Since Rd is compact and E is continuous, there exists a point Wmin that is a minimum of E inside Rd . Further, Rd contains at least one point, namely 0, for which E B, so a minimum within Rd is indeed a global minimum, the existence of which was required. Now, some definitions: We call a parameter value (d,W) a local minimum of E iff it is a local minimum in its second component, W. We call a local minimum of E B-locally minimal for some B R iff the value of E at that minimum does not exceed B. We call the proper dimensionality of W the dimensionality obtained when eliminating from W all units which have a zero fan-in or a zero fan-out or both. We call a parameter value (d,W) proper if d is the proper dimensionality of W. We also call a local minimum with such a parameter value proper. Denote (d1, .., dl) by dl and (W1, ..,Wl) by Wl. D = (x(0), y(0)), (x(1), y(1)), .., (x(N), y(N)) We denote intermediate computations of the neural network f(W, x) as follows: x0 := x (5) zl := xl1Wl 1 l L (6) xl := l.(zl) 1 l L (7) f(W, x) = xL (8) We denote the gradients of e(f(W, x), y), when they are defined, as follows: gl := de(f(W, x), y) dxl 0 l L (9) hl := de(f(W, x), y) dzl 1 l L (10) Gl := de(f(W, x), y) dWl 1 l L (11) Vector and matrix indeces are written in brackets. For example, the jth component of z(n)l is denoted by z(n)l (j). We denote by square brackets a vector and by its subscript the index the vector is over, e.g. [vi]i is a vector over index i. Lemma 2. Under the conditions of theorem 1 and the additional condition that the l are differentiable everywhere, if is the fan-in regularizer, then for all B, the set of values of d for which there exist proper B-local minima is bounded. Lemma 3. Under the conditions of theorem 1 and the additional condition that the l are differentiable everywhere, if is the fan-out regularizer, then for all B, the set of values of d for which there exist proper B-local minima is bounded. Lemmas 2 and 3 are the core segments of the overall proof. Here we show that that very large nets have no good local minima. Proof of lemma 2. Throughout this proof, we consider B fixed. Claim 1a: There exist constants Bx,l, 0 l L, such that at all proper B-local minima, for all 1 n N , for all 0 l L, we have ||x(n)l ||1 Bx,l. Claim 1b: There exist constants Bz,l, 1 l L, such that at all proper B-local minima, for all 1 n N , for all 1 l L, we have ||z(n)l ||1 Bz,l. Claim 1c: There exist constants Bd,l, 1 l L, such that at all proper B-local minima, for all 1 n N , for all 1 l L, for all 1 j dl, we have |(z(n)l (j))| Bd,l. First, we notice that it is sufficient to prove the bounds exist for a specific datapoint. The uniform bound across all datapoints is then simply the maximum of the individual bounds. Denote by (x, y) an arbitrary fixed datapoint throughout the proof of the above claims. Also, notice that the claims are trivially true if there are no proper B-local minima. Hence, throughout the proof of the claims, we assume there exists at least one such minimum. We will prove the claims jointly by induction. The order of the induction follows the order of computation of the neural network. Our starting case will be x0, followed by z1, f 1 and x1 etc. The starting case is obvious as x0 = x is fixed and does not depend on the parameter (d,W). Hence we can choose Bx,0 = ||x||1. Now assume we have Bx,l1 such that sup(d,W)properB-locally minimal ||xl1||1 Bx,l1. Then: sup (d,W)properB-locally minimal ||zl||1 (12) = sup (d,W)properB-locally minimal ||xl1Wl||1 (13) sup (d,W),||xl1||1Bx,l1, dl j=1 ||[Wl(i,j)]i||pB ||xl1Wl||1 (14) = sup dl ( sup Wl, dl j=1 ||[Wl(i,j)]i||pB ( sup W B. Then there exists some (d,W) with E(d,W) E(d,W) Et C. Contradiction. Therefore, C = B. Now assume that for some t, Wt has a unit that has zero fan-in but not zero fan-out, or vice versa. Then by setting the non-zero fan to zero, the output of f is unchanged for all x Rd0 and the value of is reduced. Therefore, we reduce E, which contradicts the fact that (dt,Wt) is a global minimum of E when d is fixed to dt. Therefore, all units in Wt that have zero fan-in also have zero fan-out, and vice versa. Let dpropert be the proper dimensionality of Wt and W proper t be the result of removing all units with zero fan-in or fan-out from Wt. Indeed, as we have shown, all units removed had both zero fan-in and fan-out. Assume (dpropert ,W proper t ) is not a local minimum of E. Then there exists a W of dimensionality dpropert with E(d proper t ,W ) < E(dpropert ,W proper t ). When we add the zero units that were removed from Wt to obtain W proper t back into W , we obtain another weight parameter value we call W. Since E is invariant under the addition and removal of units with both zero fan-in and zero fan-out, we have both E(dpropert ,W ) = E(dt,W ) and E(dpropert ,W proper t ) = E(dt,Wt). Therefore, we have E(dt,W) < E(dt,Wt), which contradicts that Wt is a global minimum of E when d is fixed to dt. Therefore, (d proper t ,W proper t ) is a local minimum of E. In particular, it is a proper Et-local minimum of E and therefore a proper E0-local minimum of E. From lemma 4, we know that the set of proper E0-local minima is bounded. Hence, the set dpropert , t 0 is bounded, i.e there exists some dmax with dmax d proper t for all t. Hence, if we denote maxl dmaxl by T , we have dT d proper t for all t and therefore ET E(d proper t ,W proper t ). But E(dpropert ,W proper t ) = E(dt,Wt) = Et, and therefore Et ET for all t. But (E)t converges to B from above. Therefore ET = B, therefore E(dT ,WT ) = B and so E attains its greatest lower bound which means it attains a global minimum, as required.",
      "exclude": false
    },
    {
      "heading": "7.2 PROOF OF PROPOSITION 1",
      "text": "Proposition 1. If all nonlinearities in a nonparametric network model except possibly L are selfsimilar, then the objective function 1 using a fan-in or fan-out regularizer with different regularization parameters 1, .., L for each layer is equivalent to the same objective function using the single regularization parameter = ( L l=1 l) 1 L for each layer, up to rescaling of weights. Proof. Choose arbitrary positive 1, .., L and let = ( L l=1 l) 1 L . We have: f(W, x) (72) = L.(L1.(..2.(1.(xW1)W2)..)WL) (73) = L.(L1.(..2.(1.(( L l=1 l )xW1)W2)..)WL) (74) = L.(L1.(..2.(( L l=2 l )1.( 1 xW1)W2)..)WL) (75) = L.( L L1.(..2.( 2 1.( 1 xW1)W2)..)WL) (76) = L.(L1.(..2.(1.(x( 1 W1))( 2 W2))..)( L WL)) (77) The line-by-line explanation is as follows: 73 Insert the definition of f . 74 Insert a multiplicative factor of value 1. 75 Utilize the self-similarity of 1. 76 Repeat the previous step L 2 times. 77 Utilize linearity. Further, assuming we use a fan-in regularizer, we have: L l=1 l dl j=1 ||[Wl(i, j)]i||p (78) = L l=1 l dl j=1 ||[Wl(i, j)]i||p (79) = L l=1 dl j=1 ||[l Wl(i, j)]i||p (80) The argument is equivalent for the fan-out regularizer. We find that the value of the objective is preserved when we replace all regularization parameters with the same value = ( L l=1 l) 1 L and rescale Wl by l . This completes the proof.",
      "exclude": false
    },
    {
      "heading": "7.3 ADARAD-M",
      "text": "1 input: r: radial step size; : angular step size; : regularization hyperparameter; arith: arithmetic mixing rate; quad: quadratic mixing rate; : numerical stabilizer; d0: initial dimensions; W0: initial weights; : unit addition rate; freq: unit addition frequency; T : number of iterations 2 max = 0; cmax = 0; d = d0; W = W0; 3 for l = 1 to L do 4 set l (angular arithmetic running average) to the zero matrix of size d0l1 d0l ; 5 set l (angular quadratic running average), cl (quadratic running average capacity) and al (arithmetic running average capacity) to zero vectors of size d0l ; 6 end 7 for t = 1 to T do 8 set Dt to mini-batch used at iteration t; 9 G = 1|D|W (x,y)Dt e(f(W, x), y); 10 for l = L to 1 do 11 alt = FALSE; 12 for j = dl to 1 do 13 decompose [Gl(i, j)]i into a component parallel to [Wl(i, j)]i (call it r) and a component orthogonal to [Wl(i, j)]i (call it ) such that [Gl(i, j)]i = r + ; 14 l(j) = (1 quad)l(j) + quad||||22; cl(j) = (1 quad)cl(j) + quad; 15 max = max(max, l(j)); cmax = max(cmax, cl(j)) ; 16 [l(i, j)]i = (1 arith)[l(i, j)]i + arith; al(j) = (1 arith)al(j) + arith; 17 adj = max cmax l(j) cl(j) + [l(i,j)]i al(j) ; 18 [Wl(i, j)]i = [Wl(i, j)]i rr; 19 rotate [Wl(i, j)]i by angle ||adj||2 in direction adj||adj||2 ; 20 rotate [l(i, j)]i by angle ||adj||2 in direction [Wl(i,j)]i||[Wl(i,j)]i||2 ; 21 shrink([Wl(i, j)]i, r |Dt| |D| ); 22 if l < L and [Wl(i, j)]i is a zero vector then 23 remove column j from Wl and l; remove row j from Wl+1 and l+1; remove element j from l, cl and al; decrement dl; 24 alt = TRUE; 25 end 26 end 27 if t = 0 mod freq then 28 = ; // if 6 Z, we can set e.g. = Poisson() 29 add randomly initialized columns to Wl; add zero columns to l; add zero rows to Wl+1 and l+1; add zero elements to l, cl and al; dl = dl + ; 30 end 31 if alt then 32 for j = 1 to dl+1 do 33 [l+1(i, j)]i = [l+1(i, j)]i [l+1(i,j)]i.[Wl+1(i,j)]i||[Wl+1(i,j)]i||22 [Wl+1(i, j)]i; 34 end 35 end 36 end 37 end 38 return W; Algorithm 2: AdaRad-M with `2 fan-in regularizer and the unit addition / removal scheme used in this paper in its most instructive (bot not fastest) order of computation. AdaRad-M is shown in algorithm 2. The main difference in comparison to AdaRad (see algorithm 1) is that, for each fan-in, we maintain an exponential running average of the orthogonal component [l(i, j)]i (line 16) which we use to compute the angular shift (line 17). Hence, AdaRad-M, like Adam but unlike RMSprop and AdaRad, makes use of the principle of momentum. One issue of note is that the running average of the orthogonal component is not itself orthogonal to the current value of the fan-in. Hence, if some multiple of it was added to the fan-in in radialangular coordinates, it would change the length of the fan-in. This is undesirable as explained in section 3.3. Therefore, we take steps to the ensure that [l(i, j)]i is kept orthogonal to [Wl(i, j)]i. First, whenever we rotate [Wl(i, j)]i (line 19), we rotate [l(i, j)]i in the same manner (line 20). Second, whenever a unit in layer l and hence rows of Wl+1 and l+1 are deleted, we explicitly re-orthogonalize them (line 33).",
      "exclude": false
    },
    {
      "heading": "7.4 EXPERIMENTAL DETAILS",
      "text": "In table 3, we show all hyperparameter values and related choices that were universal across all training runs and, unless specified otherwise, datasets.",
      "exclude": false
    },
    {
      "heading": "7.4.1 PROTOCOL FOR SECTION 4.1",
      "text": "1. We conducted a grid search over 102, 3 103, 103, 3 104, 104, 3 105, 105, 3 106, 106, 3 107, 107, 3 108, 108 and 1, 3, 10, 30, 100, 300, 1.000, 3.000, 10.000, 30.000, 100.000 for nonparametric (NP) networks using AdaRad and a single random seed, for each of the mnist, rectangles-images and convex datasets. By examining validation classification error (VCE) and other metrics (but not test error), we chose the single value = 30 for all NP experiments from now on. Further, we chose a few interesting values of for each dataset. From now on, all experiments were conducted independently for each dataset. 2. We trained 10 NP networks for each chosen value of , with 10 different random seeds. Out of the 10 nets produced, we manually chose a single net as a typical representative by approximating the median of both network size, measured in number of weight parameters, and the test classification error (TCE) across the 10 runs. This representative, as well as the range of sizes and TCEs are shown in black in figure 2. 3. For each chosen representative, we conducted a grid search for parametric (P) networks by fixing the size of the net to the size of the representative. The grid was over 1, 3, 10, 30, 100, 300, 1.000, 3.000, 10.000, 30.000, 100.000, over training algorithm (one of SGD, momentum, Nesterov momentum, RMSprop, Adam), and over whether batch normalization layers had free trainable mean and variance parameters. We introduced the last choice to more closely mimic CapNorm, which does not include free parameters. We set = 0 as `2 regularization is not compatible with regular (uncapped) batch normalization. In preliminary experiments, networks trained with `2 regularization and no batch normalization were not competitive. We used the same random seed as in step 1. 4. We chose the 10 best performing settings from the grid search by VCE and produced 10 reruns for each setting using the same 10 random seeds as in step 2. Then we chose the best setting out of the 10 by median VCE. We depict the median as well as the range of TCE for that best setting in red in figure 2. Note that the setting that had the lowest median TCE in all cases also had the lowest median VCE. 5. We conducted a random search for P networks with 500 random settings. We chose uniformly from the interval [1, 100.000] in log scale. Training algorithm and type of batch normalization were chosen uniformly at random from the same sets as in step 3. The size of each hidden layer was chosen uniformly at random between the size of the corresponding layer in the largest NP representative, and 5 times that size. We used the same random seed as in step 1. 6. We chose the 10 best settings by VCE and reran them 10 times, using the same 10 random seeds as in step 2. By considering network size and median VCE, we chose 2 or 3 settings to display in blue in figure 2, including the setting with the lowest median VCE. In each case, the setting with the lowest median VCE also had the lowest median TCE. For NP networks, we trained until the VCE had not improved for 100 epochs. Then, we rewound the last 100 epochs and kept training without adding units. After no units had been eliminated and the VCE had not improved for 100 epochs, we set to zero, rewound the last 100 epochs and kept training. After the VCE had not improved for 100 epochs, we rewound again and divided the angular step size by 3. After the VCE had not improved for 5 epochs, we rewound and divided the angular step size by 3 again. We kept doing this until the angular step size was too small to change the VCE. For P networks, we trained until the VCE had not improved for 100 epochs, then rewound and divided the step size by 3. We kept training until the VCE had not improved for 5 epochs, then rewound again and divided the step size by 3. We kept doing this until the step size was too small to change the VCE.",
      "exclude": false
    },
    {
      "heading": "7.4.2 PROTOCOL FOR SECTION 4.3",
      "text": "1. We conducted a grid search over 103, 3 104, 104, 3 105, 105, 3 106, 106, 3 107, 107 and 1, 10, 100, 1.000, 10.000 for NP networks using AdaRad, a single random seed and the poker data set. By examining VCE and other metrics (but not test error), we chose the single value = 10. For this value, we chose several values of . The size and TCE of the nets trained using those values of are shown in table 2. 2. For each trained NP network shown in table 2, we trained P networks of the same size using RMSprop and each of the following step sizes: 1, 3, 10, 30, 100, 300, 1.000, 3.000, 10.000. For each network size, the TCE of the network with the lowest VCE is shown in table 2. For all network sizes, the network with the lowest TCE also had the lowest VCE. For NP networks, we trained until the VCE had not improved for 10 epochs. Then, we rewound the last 10 epochs and kept training without adding units. After no units had been eliminated and the VCE had not improved for 10 epochs, we set to zero, rewound the last 10 epochs and kept training. After the VCE had not improved for 10 epochs, we rewound again and divided the angular step size by 3. After the VCE had not improved for 0.5 epochs, we rewound and divided the angular step size by 3 again. We kept doing this until the angular step size was too small to change the VCE. For P networks, we trained until the VCE had not improved for 10 epochs, then rewound and divided the step size by 3. We kept training until the VCE had not improved for 0.5 epochs, then rewound again and divided the step size by 3. We kept doing this until the step size was too small to change the VCE.",
      "exclude": false
    }
  ]
}