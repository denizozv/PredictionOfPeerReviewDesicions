{
  "id": "568",
  "title": "Character-aware Attention Residual Network for Sentence Representation",
  "abstract": "Text classification in general is a well studied area. However, classifying short and noisy text remains challenging. Feature sparsity is a major issue. The quality of document representation here has a great impact on the classification accuracy. Existing methods represent text using bag-of-word model, with TFIDF or other weighting schemes. Recently word embedding and even document embedding are proposed to represent text. The purpose is to capture features at both word level and sentence level. However, the character level information are usually ignored. In this paper, we take word morphology and word semantic meaning into consideration, which are represented by character-aware embedding and word distributed embedding. By concatenating both character-level and word distributed embedding together and arranging words in order, a sentence representation matrix could be obtained. To overcome data sparsity problem of short text, sentence representation vector is then derived based on different views from sentence representation matrix. The various views contributes to the construction of an enriched sentence embedding. We employ a residual network on the sentence embedding to get a consistent and refined sentence representation. Evaluated on a few short text datasets, our model outperforms state-of-the-art models.",
  "accepted": false,
  "zeroShot": [
    {
      "model": "claude-haiku-4-5",
      "decision": {
        "rejection": true,
        "confidence": 0.85,
        "primary_reason": "Limited novelty and weak experimental validation. The paper combines existing techniques (character-level embeddings, word embeddings, attention, residual networks) without significant innovation. Experimental evaluation is insufficient: only 3 datasets with no statistical significance testing, missing details on dataset sizes, no error bars/confidence intervals, and weak baselines (no comparison with contemporary deep learning methods like RNNs/LSTMs). The writing quality is below conference standards with grammatical errors and unclear technical descriptions."
      },
      "token": {
        "prompt_tokens": 2931,
        "completion_tokens": 138,
        "total_tokens": 3069
      },
      "time": "2026-02-05T23:00:09.906862+00:00"
    },
    {
      "model": "gpt-5.2",
      "decision": {
        "rejection": true,
        "confidence": 0.86,
        "primary_reason": "Incremental/dated architecture with insufficient experimental rigor (unclear dataset sizes/splits, missing statistical significance and ablations, limited baselines vs modern pretrained models), making novelty and evidence inadequate for a top-tier ML conference."
      },
      "token": {
        "prompt_tokens": 2612,
        "completion_tokens": 63,
        "total_tokens": 2675
      },
      "time": "2026-02-09T21:29:46.864426+00:00"
    }
  ],
  "article": [
    {
      "heading": "1 INTRODUCTION",
      "text": "For text classification, a popular feature representation method is bag-of-word. However, this representation has an intrinsic disadvantage that two separate features will be generated for two words with the same root or of different tenses. Lemmatization and stemming could be applied to partially address this problem, but may not always leads to correct results. For example, meaningful and meaningless would both be considered as meaning after applying lemmatization or stemming algorithms, while they are of opposite meanings. Thus, word morphology could also provide useful information in document understanding, particular in short text where the information redundancy is low. For short text, an important issue is data sparsity, particularly when utilizing feature representation method like bag-of-word, regardless the weighting scheme. Therefore, various distributed word representation like Word2Vec (Mikolov et al., 2013) and document representation Doc2Vec (Le & Mikolov, 2014) have been proposed to address the problem. However, this kind of method miss the word morphology information and word combination information. To deal with these issues, we propose a model which could capture various kinds of features that could benefit classification task. In this paper, we look deep into characters. We learn character representation and combine both character-level (Zhang et al., 2015) and word-level embedding to represent a word. Thus both morphology and semantic properties of the word are captured. As we know, not all the words in a sentence contribute the same when predicting the sentences label. Therefore, highlight the relatively pertinent information would give better chance of correct prediction. Attention mechanism (Mnih et al., 2014; Bahdanau et al., 2016) which focuses on specific part of input could help achieve this goal. The applications of attention mechanism are mostly on sequential model, while we employ The two authors contribute the same for the work. the idea of attention on a feed-forward network (Raffel & Ellis, 2015). By multiplying the weight assigned by attention mechanism to its corresponding word vector, a weighted feature matrix could be constructed by concatenating the sequence of word embeddings in a sentence. Short text usually could not provide much useful information for class prediction. We try different views to extract as much information as possible to construct an enriched sentence representation vector. Specifically, to convert a sentence representation matrix to an enriched vector, we draw two types of features. The first one is based on word feature space and the other one is based on n-gram. However, not all the features contribute the same on sentence classification. Attention mechanism is applied to focus on the significant features. Since these features come from different views, we need a method to make the elements consistent. The residual network proposed in (He et al., 2015; 2016) achieve much better results on image classification task. In other words, the residual mechanism could construct better image representation. Therefore, we adopt residual network to refine the sentence representation vector. Once we obtain a good quality representation for the sentence, it will be delivered to a classifier.",
      "exclude": true
    },
    {
      "heading": "2 RELATED WORK",
      "text": "There are many traditional machine learning methods for text classification and most of them could achieve quite good results on formal text datasets. Recently, many deep learning methods are proposed to solve the text classification task (Zhang et al., 2015; dos Santos & Gatti, 2014; Kim, 2014). Deep convolutional neural network suggests benefits in image classification (Krizhevsky et al., 2012; Sermanet et al., 2013). Therefore, many research also try to apply it on text classification problem. Kim (2014) propose a model similar to Collobert et al. (2011) architecture. However, they employ two channels of word vectors. One is static throughout training and the other is fine-tuned via backpropagation. Various size of filters are conducted on both channel, and the results are concatenated together. Then max-pooling over time is taken to select the most significant feature among each filter. The selected features are concatenated as the sentence vector. Similarly, Zhang et al. (2015) also employ the convolutional networks but add character-level information for text classification. They design two networks, one large and one small. Both of them have nine layers including six convolutional layer and three fully-connected layers. Between the three fully connected layers they insert two dropout for regularization. For both convolution and max-pooling layers, they employ 1-D version (Boureau et al., 2010). After convolution, they add the sum over all the results from one filter as the output. Specially, they claim 1-D max-pooling enable them to train a relatively deep network (Boureau et al.). Besides applying models directly on testing datasets, more aspects are considered when extracting features. Character-level feature is adopted in many tasks besides Zhang et al. (2015) and most of them achieve quite good performance. dos Santos & Zadrozny (2014) take word morphology and shape into consideration which have been ignored for part-of-speech tagging task. They suggest the intra-word information is extremely useful when dealing with morphologically rich languages. They adopt neural network model to learn the character-level representation which is further delivered to help word embedding learning. Kim et al. (2016) construct neural language model by analysis of word representation obtained from character composition. Results suggest the model could be encode semantic and orthographic information from character level. Attention model is also utilized in our model, which is used to assign weights for each parts of components. Usually, attention model is used in sequential model (Rocktaschel et al., 2015; Mnih et al., 2014; Bahdanau et al., 2016; Kadlec et al., 2016). The attention mechanism includes sensor, internal state, actions, and reward. At each time, the sensor will capture a glimpse network which only focus on a small part of the network. Internal state will summarize the extracted information. Actions decides the location for the next step and reward suggests the benefit when taking the action. In our condition, we adopt a simplified attention network as (Raffel & Ellis, 2015; 2016). We do not need to guess the next step location and just give a weight on each components which indicates the significance of the element. Residual network (He et al., 2015; 2016; Chen et al., 2016) is known to be able to make neural network deeper and relieve degradation problem at the same time. And residual network in (He et al., 2015) outperforms the state-of-the-art models on image recognition. He et al. (2016) introduces how to make the residual block more efficient on image classification. Similarly, for short text classification problem, the quality of sentence representation is also quite important for the final result. Thus, we try to adopt the residual block as in (He et al., 2015; 2016) to refine the sentence vector.",
      "exclude": true
    },
    {
      "heading": "3 CHARACTER-AWARE ATTENTION RESIDUAL NETWORK",
      "text": "In this paper, we propose a character-aware attention residual network to generate sentence representation. Figure 1 illustrates the model. For each word, the word representation vector is constructed by concatenating both character-level embedding and word semantic embedding. Thus a sentence is represented by a matrix. Then two types of features are extracted from the sentence matrix to construct the enriched sentence representation vector for short text. However, not all the features contribute the same for classification. Attention mechanism is employed to target on pertinent parts. To make features extracted from different views consistent, a residual network is adopt to refine the sentence representation vector. Thus, an enriched sentence vector is obtained to do text classification.",
      "exclude": false
    },
    {
      "heading": "3.1 WORD REPRESENTATION CONSTRUCTION",
      "text": "Let C be the vocabulary of characters, and E Rdc|C| is the character embedding matrix, where dc is the dimensionality of character embedding. Given a word, which is composed of a sequence of characters [c1, c2, ..., cnc ], its corresponding character-level embedding matrix would be E w Rdcnc . Herein, Ewi = E vi (1) where vi is a binary column vector with 1 only at the ci-th place and 0 for other positions. Here, we fix the word length dc and take zero-padding when necessary. For each of such matrix Ew, a convolution operation (Le Cun et al., 1990) with m filters (i.e., kernels) P Rdck is applied on Ew, and a set of feature maps could be obtained. Instead of adopting max-pooling over time (Collobert et al., 2011), we adopt max-pooling over filters operation to capture local information of words as shown in Figure 1. Similar operation is adopted in (Shen et al., 2014). That is we get the max feature value over results of m filters at the same window position, which depicts the most significant feature over the k characters. Thus, a vector qc for the word which captures the character-level information is constructed. Note that embedding vector qc could only capture the word morphological features, while it can not reflect word semantic and syntactic characteristics. Therefore, we concatenate the distributed word representative vector qw (i.e., Word2Vec) (Mikolov et al., 2013) to qc as the words final representation q R(dc+dw), where dw is the dimensionality of Word2Vec. Given a sentence, which consists of a sequence of words [w1, w2, ..., wnw ], its representation matrix is Q R(dc+dw)nw .",
      "exclude": false
    },
    {
      "heading": "3.2 SENTENCE REPRESENTATION VECTOR CONSTRUCTION",
      "text": "To overcome the lack of information issue for short text, we explore various kinds of useful information from limited context. From higher level, we adopt two types of features as shown in Figure 2 (i.e., type 1 feature and type 2 feature). They capture different views of information for the short text, which could be considered as results from horizontal view and vertical view on sentence representation matrix Q separately. Type 1 feature takes words feature space (i.e., horizontal view on Q) into consideration. The feature space is the composition of both character-level embedding and word semantic embedding. Each word is a point in the feature space. We formulate the summation over all words appearing in the sentence as the sentences representation, inspired by (Zhang et al., 2015). In fact, not all the words in a sentence contribute the same for prediction. Therefore, we want to highlight the significant words and this is realized by weighting the words representation features. To assign the weights, we employ attention mechanism, and multiply the weight to the word feature vector as Equation 2. Specifically, we follow Raffel & Ellis (2015) and Bahdanau et al. (2014) as shown in Figure 3(a). For each word representation vector qi, we apply a Tanh function on the linear transformation of qi as g(qi) = Tanh(Wqhqi + bqh), where Wth R1(dc+dw), bth R. Then a softmax function on g(qi) is used to assign a weight si for each qi, which indicates the significance of word i in the sentence. si = exp(g(qi))j=1 nw exp(g(qj)) , qi = siqi. (2) As a result, we can get a weighted sentence representation matrix Q R(dc+dw)nw . Then we employ an average over words in the sentence at the same feature position and obtain a sentence representation vector r0. Type 2 feature models the word level features (i.e., vertical view on Q). As we know, sometimes continuous words combination is meaningful and pertinent for sentence classification. To capture n-gram information, we apply convolution operation on Q, which is followed by a max-pooling over time. We adopt several different kernel sizes to model various n-grams. Different n-grams contribute differently. The attention mechanism is utilized again on the vectors of n-gram representations, and the resulting weights indicate their significance. We get the weighted feature vectors r1, r2, r3. Concatenating r0, r1, r2, r3, the complete sentence vector v is constructed.",
      "exclude": false
    },
    {
      "heading": "3.3 RESIDUAL NETWORK FOR REFINING SENTENCE REPRESENTATION",
      "text": "The residual learning (He et al., 2015; 2016) is reported to outperform state-of-the-art models in image classification task and object detection task. This suggests residual learning could help to capture and refine the embedding. To make the features of sentence vector v from different views consistent, we employ residual learning to v. Let the desired mapping as H(v), instead of making each layer directly optimize H(v), residual learning (He et al., 2015) turns to fit the residual function: F(v) := H(v) v. (3) Thus, the original target mapping becomes: y = F(v) + v. (4) Both residual function F(v) and the added input form v are flexible. In our model, we construct the building block by two fully connected layers connected by a ReLU (Nair & Hinton, 2010) operation as shown in Figure 3(b). Meanwhile, the identity mapping is adopted by performing a shortcut connection and element-wise addition: v = F(v, G) + v (5) where v is the refined sentence vector, G is the weight matrix to be learned. After getting the sentence embedding v from the building block, it is further delivered to a softmax classifier for text classification.",
      "exclude": false
    },
    {
      "heading": "4 EXPERIMENT",
      "text": "",
      "exclude": false
    },
    {
      "heading": "4.1 DATASETS",
      "text": "We adopt testing datasets from different sources. There are three datasets, including Tweets, Question, AG news. All of them are relatively short. Tweets are typical short text with only 140 characters limitation. We crawl the tweets from Twitter with a set of keywords, which is specifically about some products. We label them as positive, negative, neutral, question and spam. Question dataset is a small dataset. The content is short questions, and the labels are question types. AG News dataset is from (Zhang et al., 2015). The reason we choose this is because the length of text is much shorter than others. The news here only contains the title and description fields.",
      "exclude": false
    },
    {
      "heading": "4.2 EXPERIMENT SETTINGS",
      "text": "In this paper, we take 128 ASCII characters as character set, by which most of the testing documents are composite. We define word length nc as 20 and character embedding length dc as 100. If a word with characters less than 20, zero padding is taken. If the length is larger than 20, just take the first 20 characters. We train the word distributed embedding using training data and the feature dimension is 300. We take sentence length as 20, which is enough to cover most of crucial words. We add 5 residual blocks to refine the sentence vector.",
      "exclude": false
    },
    {
      "heading": "4.3 BASELINE MODEL",
      "text": "We select both traditional models and deep learning models on classification as baselines. TF-SVM is the bag-of-word feature weighted by counting the term frequency in a sentence. Then deliver the feature matrix to a SVM classifier. TFIDF-SVM is taken as traditional baseline model. Since SVM classifier is robust and state-ofthe-art traditional classifier, and TFIDF usually assign good weights for bag-of-words in documents even for tough inputs. So this is a competitive baseline model. Lg. Conv, Sm. Conv are proposed in (Zhang et al., 2015) which also consider character-level embedding, and they concatenate all the characters embeddings in a sentence in order as sentences representation matrix. For fair comparison, we do not include thesaurus to help clear documents.",
      "exclude": false
    },
    {
      "heading": "4.4 COMPARISON RESULTS",
      "text": "Table 3 shows the comparison results on the testing datasets. As we can see, the proposed model could outperform baseline models on Tweets and Question datasets. For AG news dataset, our method could give comparable results as the best baseline model, TFIDF-SVM. The TFIDF-SVM model can achieve relatively better results than others. However, both Lg. Conv and Sm. Conv do not perform well on Tweets dataset. This may because these two models are relatively deep network with several down sampling operations (i.e., max-pooling) and this dramatically decreases the short text representation. And short text does not contain much information. Thus Lg. Conv and Sm. Conv could not give good results. The TF-SVM model also does not perform well on Tweets dataset. This may because the tweet text is too short and term frequencies are mostly 1 which is not enough to provide information on classification. Similar to result of CAR-1 on Tweets data. When removing type 1 feature, the performance drops dramatically. However, for other datasets, in which the document length is longer and the content is relatively formal, removing type 1 feature does not influence the performance that much. Hence, these results suggest the word character-level feature and semantic feature (i.e., type 1 feature) are rather important for short, free-style text. On the other hand, by adding type 2 features can also improve the performance according to results of CAR2. Consequently, when dealing with short text, either formal or informal, including character-level feature, word-semantic feature and n-gram feature would benefit the performance. Another comparison is adding the residual network or not. As we can see from Table 3, residual network could refine the vector representation. When removing residual block, performances on three datasets all decrease. In particular, the improvement for shorter and noisy text (Tweets dataset) is more than those relatively longer and formatted documents. Thus, for short noisy text classification problem, one adopts residual building block would improve the performance.",
      "exclude": false
    },
    {
      "heading": "5 CONCLUSION",
      "text": "We propose a character-aware attention residual network for short text classification. We construct the sentence representation vector by two kinds of features. The first is focusing on feature space, which include both character-level characteristics and semantic characteristics. The other is n-gram features. To make them consistent, the residual network helps refine the vector representation. Experiment results suggest both extracted features and the residual network helps on short text clas- sification. Our proposed method could outperform the state-of-the-art traditional models and deep learning models.",
      "exclude": true
    }
  ],
  "fewShot": [
    {
      "model": "gpt-5.2",
      "decision": {
        "rejection": true,
        "confidence": 0.86,
        "primary_reason": "Incremental architecture (char-CNN + word embeddings + attention + CNN n-grams + residual MLP) with limited novelty and weak/dated baselines; experiments are under-specified (private tweet crawl, unclear splits/hyperparams) and results are only marginal (AG News merely matches TFIDF-SVM), so the empirical evidence and positioning are below ICML/ICLR/NeurIPS standards."
      },
      "token": {
        "prompt_tokens": 5246,
        "completion_tokens": 110,
        "total_tokens": 5356
      },
      "time": "2026-02-09T20:32:00.302922+00:00"
    },
    {
      "model": "claude-haiku-4-5",
      "decision": {
        "rejection": true,
        "confidence": 0.85,
        "primary_reason": "The paper combines existing techniques (character-level CNNs, word embeddings, attention, residual networks) without sufficient novelty or insight. Empirical validation is weak: only three small datasets with modest improvements, no statistical significance testing, limited baselines (missing recent deep learning methods), and unclear experimental setup (e.g., train/test splits, hyperparameter selection). The writing is poor with grammatical errors and unclear motivation for design choices. The residual network application appears ad-hoc and poorly justified for this task."
      },
      "token": {
        "prompt_tokens": 5827,
        "completion_tokens": 147,
        "total_tokens": 5974
      },
      "time": "2026-02-11T19:01:10.288790+00:00"
    }
  ],
  "test": false
}