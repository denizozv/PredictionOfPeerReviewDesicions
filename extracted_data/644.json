{
  "id": "644",
  "title": "Diverse Beam Search: Decoding Diverse Solutions from Neural Sequence Models",
  "abstract": "Neural sequence models are widely used to model time-series data. Equally ubiquitous is the usage of beam search (BS) as an approximate inference algorithm to decode output sequences from these models. BS explores the search space in a greedy left-right fashion retaining only the top B candidates. This tends to result in sequences that differ only slightly from each other. Producing lists of nearly identical sequences is not only computationally wasteful but also typically fails to capture the inherent ambiguity of complex AI tasks. To overcome this problem, we propose Diverse Beam Search (DBS), an alternative to BS that decodes a list of diverse outputs by optimizing a diversity-augmented objective. We observe that our method not only improved diversity but also finds better top 1 solutions by controlling for the exploration and exploitation of the search space. Moreover, these gains are achieved with minimal computational or memory overhead com- pared to beam search. To demonstrate the broad applicability of our method, we present results on image captioning, machine translation, conversation and visual question generation using both standard quantitative metrics and qualitative human studies. We find that our method consistently outperforms BS and previously proposed techniques for diverse decoding from neural sequence models.",
  "accepted": false,
  "zeroShot": [
    {
      "model": "claude-haiku-4-5",
      "decision": {
        "rejection": false,
        "confidence": 0.92,
        "primary_reason": "Novel decoding algorithm with strong empirical validation across multiple tasks (image captioning, MT, dialog, VQG). Well-motivated approach to a real problem in sequence generation. Comprehensive experiments with human studies and sensitivity analyses. Minor concerns: greedy approximation limits optimality, limited theoretical analysis, but practical contributions are significant."
      },
      "token": {
        "prompt_tokens": 5950,
        "completion_tokens": 96,
        "total_tokens": 6046
      },
      "time": "2026-02-05T23:07:22.247483+00:00"
    }
  ],
  "article": [
    {
      "heading": "1 INTRODUCTION",
      "text": "In the last few years, Recurrent Neural Networks (RNNs), Long Short-Term Memory networks (LSTMs) or more generally, neural sequence models have become the standard choice for modeling time-series data for a wide range of applications including speech recognition (Graves et al., 2013), machine translation (Bahdanau et al., 2014), conversation modeling (Vinyals & Le, 2015), image and video captioning (Vinyals et al., 2015; Venugopalan et al., 2015), and visual question answering (Antol et al., 2015). RNN based sequence generation architectures model the conditional probability, Pr(y|x) of an output sequence y = (y1, . . . , yT ) given an input x (possibly also a sequence); where the output tokens yt are from a finite vocabulary, V . Inference in RNNs. Maximum a Posteriori (MAP) inference for RNNs is the task of finding the most likely output sequence given the input. Since the number of possible sequences grows as |V|T , exact inference is NP-hard so, approximate inference algorithms like beam search (BS) are commonly employed. BS is a heuristic graph-search algorithm that maintains the B top-scoring partial sequences expanded in a greedy left-to-right fashion. Fig. 1 shows a sample BS search tree. Lack of Diversity in BS. Despite the widespread usage of BS, it has long been understood that solutions decoded by BS are generic and lacking in diversity (Finkel et al., 2006; Gimpel et al., a train steam black locomotive is traveling on engine train train coming down a the train engine down track train tracks traveling is the with near track down through tracks a with train tracks a in a tracks forest lush a an train steam an the is engine old train a an coming train train steam train black traveling is engine locomotive train and down through train is is white train a is traveling coming on tracks forest down through the a Beam Search Diverse Beam Search A steam engine train travelling down train tracks. A steam engine train travelling down tracks. A steam engine train travelling through a forest. A steam engine train travelling through a lush green forest. A steam engine train travelling through a lush green countryside A train on a train track with a sky background. A steam engine travelling down train tracks. A steam engine train travelling through a forest. An old steam engine train travelling down train tracks. An old steam engine train travelling through a forest. A black train is on the tracks in a wooded area. A black train is on the tracks in a rural area. Single engine train rolling down the tracks. A steam locomotive is blowing steam. A locomotive drives along the tracks amongst trees and bushes. An old fashion train with steam coming out of its pipe. A black and red train moving down a train track. An engine is coming down the train track. Ground Truth Captions Figure 1: Comparing image captioning outputs decoded by BS (top) and our method, Diverse Beam Search (middle) we notice that BS captions are near-duplicates with similar shared paths in the search tree and minor variations in the end. In contrast, DBS captions are significantly diverse and similar to the variability in human-generated ground truth captions (bottom). 2013; Li et al., 2015; Li & Jurafsky, 2016). Comparing the human (bottom) and BS (top) generated captions shown in Fig. 1 demonstrates this deficiency. While this behavior of BS is disadvantageous for many reasons, we highlight the three most crucial ones here: i) The production of near-identical beams make BS a computationally wasteful algorithm, with essentially the same computation being repeated for no significant gain in performance. ii) Due to loss-evaluation mismatch (i.e. improvements in posterior-probabilities not necessarily corresponding to improvements in task-specific metrics), it is common practice to deliberately throttle BS to become a poorer optimization algorithm by using reduced beam widths (Vinyals et al., 2015; Karpathy & Fei-Fei, 2015; Ferraro et al., 2016). This treatment of an optimization algorithm as a hyperparameter is not only intellectually dissatisfying but also has a significant practical side-effect it leads to the decoding of largely bland, generic, and safe outputs, e.g. always saying I dont know in conversation models (Kannan et al., 2016). iii) Most importantly, lack of diversity in the decoded solutions is fundamentally crippling in AI problems with significant ambiguity e.g. there are multiple ways of describing an image or responding in a conversation that are correct and it is important to capture this ambiguity by finding several diverse plausible hypotheses. Overview and Contributions. To address these shortcomings, we propose Diverse Beam Search (DBS) a general framework to decode a set of diverse sequences that can be used as an alternative to BS. At a high level, DBS decodes diverse lists by dividing the given beam budget into groups and enforcing diversity between groups of beams. Drawing from recent work in the probabilistic graphical models literature on Diverse M-Best (DivMBest) MAP inference (Batra et al., 2012; Prasad et al., 2014; Kirillov et al., 2015), we optimize an objective that consists of two terms the sequence likelihood under the model and a dissimilarity term that encourages beams across groups to differ. This diversity-augmented model score is optimized in a doubly greedy manner greedily optimizing along both time (like BS) and groups (like DivMBest). Our primary technical contribution is Diverse Beam Search, a doubly greedy approximate inference algorithm to decode diverse sequences from neural sequence models. We report results on image captioning, machine translation, conversations and visual question generation to demonstrate the broad applicability of DBS. Results show that DBS produces consistent improvements on both task-specific oracle and other diversity-related metrics while maintaining run-time and memory requirements similar to BS. We also evaluate human preferences between image captions generated by BS or DBS. Further experiments show that DBS is robust over a wide range of its parameter values and is capable of encoding various notions of diversity through different forms of the diversty term. Overall, our algorithm is simple to implement and consistently outperforms BS in a wide range of domains without sacrificing efficiency. Our implementation is publicly available at https: //github.com/ashwinkalyan/dbs. Additionally, we provide an interactive demonstration of DBS for image captioning at http://dbs.cloudcv.org.",
      "exclude": true
    },
    {
      "heading": "2 PRELIMINARIES: DECODING RNNS WITH BEAM SEARCH",
      "text": "We begin with a refresher on BS, before describing our generalization, Diverse Beam Search. For notational convenience, let [n] denote the set of natural numbers from 1 to n and let v[n] = [v1, . . . , vn] T index the first n elements of a vector v Rm. The Decoding Problem. RNNs are trained to estimate the likelihood of sequences of tokens from a finite dictionary V given an input x. The RNN updates its internal state and estimates the conditional probability distribution over the next output given the input and all previous output tokens. We denote the logarithm of this conditional probability distribution over all tokens at time t as (yt) = log Pr(yt|yt1, . . . , y1,x). To avoid notational clutter, we index () with a single variable yt, but it should be clear that it depends on all previous outputs, y[t1]. We write the log probability of a partial solution (i.e. the sum of log probabilities of all tokens decoded so far) as (y[t]) = [t] (y ). The decoding problem is then the task of finding a sequence y that maximizes (y). As each output is conditioned on all the previous outputs, decoding the optimal length-T sequence in this setting can be viewed as MAP inference on a T -order Markov chain with nodes corresponding to output tokens at each time step. Not only does the size of the largest factor in such a graph grow as |V|T , but computing these factors also requires repetitively evaluating the sequence model. Thus, approximate algorithms are employed and the most prevalent method is beam search (BS). Beam search is a heuristic search algorithm which stores the topB highest scoring partial candidates at each time step; where B is known as the beam width. Let us denote the set of B solutions held by BS at the start of time t as Y[t1] = y1,[t1], . . . ,yB,[t1]. At each time step, BS considers all possible single token extensions of these beams given by the set Yt = Y[t1] V and retains the B highest scoring extensions. More formally, at each step the beams are updated as Y[t] = argmax y1,[t],...,yB,[t]Yt b[B] (yb,[t]) s.t. yi,[t] 6= yj,[t] i 6= j. (1) The above objective can be trivially maximized by sorting all B |V| members of Yt by their log probabilities and selecting the top B. This process is repeated until time T and the most likely sequence is selected by ranking the B complete beams according to their log probabilities. While this method allows for multiple sequences to be explored in parallel, most completions tend to stem from a single highly valued beam resulting in outputs that are often only minor perturbations of a single sequence (and typically only towards the end of the sequences).",
      "exclude": false
    },
    {
      "heading": "3 DIVERSE BEAM SEARCH: FORMULATION AND ALGORITHM",
      "text": "To overcome this, we augment the objective in Eq. 1 with a dissimilarity term (Y[t]) that measures the diversity between candidate sequences, assigning a penalty (Y[t])[c] to each possible sequence completion c V . Jointly optimizing this augmented objective for allB candidates at each time step is intractable as the number of possible solutions grows with |V|B (easily 1060 for typical language modeling settings). To avoid this, we opt for a greedy procedure that divides the beam budget B into G groups and promotes diversity between these groups. The approximation is doubly greedy across both time and groups so (Y[t]) is constant with respect to other groups and we can sequentially optimize each group using regular BS. We now explain the specifics of our approach. Diverse Beam Search. As joint optimization is intractable, we form G smaller groups of beams and optimize them sequentially. Consider a partition of the set of beams Y[t] into G smaller sets Y g[t], g[G] of B = B/G beams each (we pick G to divide B). In the example shown in Fig. 2, B = 6 beams are divided into G = 3 differently colored groups containing B = 2 beams each. Considering diversity only between groups, reduces the search space at each time step; however, inference remains intractable. To enforce diversity efficiently, we consider a greedy strategy that steps each group forward in time sequentially while considering the others fixed. Each group can then evaluate the diversity term with respect to the fixed extensions of previous groups, returning the search space to B |V|. In the snapshot shown in Fig. 2, the third group is being stepped forward at time step t = 4 and the previous groups have already been completed. With this staggered beamfront, the diversity term of the third group can be computed using these completions. Here we use hamming diversity, which adds diversity penalty -1 for each appearance of a possible extension word at the same time step in a previous group birds, the, and an in the example and 0 to all other possible completions. We discuss other forms for the diversity function in Section 5.1. As we optimize each group with the previous groups fixed, extending group g at time t amounts to a standard BS using dissimilarity augmented log probabilities and can be written as: Y g[t] = argmax yg 1,[t] ,...,yg B,[t]Y g t b[B] ( ygb,[t] ) + ( g1 h=1 Y h[t] ) [ygb,t], (2) s.t. 0, ygi,[t] 6= y g j,[t]i 6= j where is scalar controlling the strength of the diversity term. The full procedure to obtain diverse sequences using our method, Diverse Beam Search (DBS), is presented in Algorithm 1. It consists of two main steps for each group at each time step 1) augmenting the log probabilities of each possible extension with the diversity term computed from previously advanced groups (Algorithm 1, Line 5) and, 2) running one step of a smaller BS with B beams using the augmented log probabilities to extend the current group (Algorithm 1, Line 6). Note that the first group (g = 1) is not conditioned on other groups during optimization, so our method is guaranteed to perform at least as well as a beam search of size B. Algorithm 1: Diverse Beam Search 1 Perform a diverse beam search with G groups using a beam width of B 2 for t = 1, . . . T do // perform one step of beam search for first group without diversity 3 Y 1[t] argmax(y11,[t],...,y1B,[t]) b[B] (y 1 b,[t]) 4 for g = 2, . . . G do // augment log probabilities with diversity penalty 5 (ygb,[t]) (y g b,[t]) + ( g1 h=1 Y h [t])[y g b,t] b [B],y g b,[t] Y g t and > 0 // perform one step of beam search for the group 6 Y g[t] argmax(yg 1,[t] ,...,yg B,[t] ) b[B] (y g b,[t]) s.t. yi,[t] 6= yj,[t] i 6= j 7 Return set of B solutions, Y[T ] = G g=1 Y g [T ]",
      "exclude": false
    },
    {
      "heading": "4 RELATED WORK",
      "text": "Diverse M-Best Lists. The task of generating diverse structured outputs from probabilistic models has been studied extensively (Park & Ramanan, 2011; Batra et al., 2012; Kirillov et al., 2015; Prasad et al., 2014). Batra et al. (2012) formalized this task for Markov Random Fields as the DivMBest problem and presented a greedy approach which solves for outputs iteratively, conditioning on previous solutions to induce diversity. Kirillov et al. (2015) show how these solutions can be found jointly (non-greedily) for certain kinds of energy functions. The techniques developed by Kirillov are not directly applicable to decoding from RNNs, which do not satisfy the assumptions made. Most related to our proposed approach is the work of Gimpel et al. (2013), who applied DivMBest to machine translation using beam search as a black-box inference algorithm. Specifically, in this approach, DivMBest knows nothing about the inner-workings of BS and simply makesB sequential calls to BS to generate B diverse solutions. This approach is extremely wasteful because BS is called B times, run from scratch every time, and even though each call to BS produces B solutions, only one solution is kept by DivMBest. In contrast, DBS avoids these shortcomings by integrating diversity within BS such that no beams are discarded. By running multiple beam searches in parallel and at staggered time offsets, we obtain large time savings making our method comparable to a single run of classical BS. One potential disadvantage of our method w.r.t. Gimpel et al. (2013) is that sentence-level diversity metrics cannot be incorporated in DBS since no group is complete when diversity is encouraged. However, as observed empirically by us and Li et al. (2015), initial words tend to disproportionally impact the diversity of the resultant sequences suggesting that later words may not be important for diverse inference. Diverse Decoding for RNNs. Efforts have been made by Li et al. (2015) and Li & Jurafsky (2016) to produce diverse decodings from recurrent models for conversation modeling and machine translation. Both of these works propose new heuristics for creating diverse M-Best lists and employ mutual information to re-rank lists of sequences. The latter achieves a goal separate from ours, which is simply to re-rank diverse lists. Li & Jurafsky (2016) proposes a BS diversification heuristic that discourages beams from sharing common roots, implicitly resulting in diverse lists. Introducing diversity through a modified objective (as in DBS) rather than via a procedural heuristic provides easier generalization to incorporate different notions of diversity and control the exploration-exploitation trade-off as detailed in Section 5.1. Furthermore, we find that DBS outperforms the method of Li & Jurafsky (2016). Li et al. (2015) introduced a novel decoding objective that maximizes mutual information between inputs and predicted outputs to penalize generic sequences. This operates on a principle orthogonal and complementary to DBS and Li & Jurafsky (2016). It works by penalizing utterances that are generally more frequent (diversity independent of input) rather than penalizing utterances that are similar to other utterances produced for the same input (diversity conditioned on input). Furthermore, the input-independent approach requires training a new language model for the target language while DBS just requires a diversity function . Combination of these complementary techniques is left as interesting future work. In other recent work, Wu et al. (2016) modify the beam search objective by introducing lengthnormalization to favor longer sequences and a coverage penalty that favors sequences that account for the complete input sequence. While the coverage term does not generalize to all neural sequence models, the length-normalization term can be implemented by modifying the joint-log-probability of each sequence. Although the goal of this method is not to produce diverse lists and hence not directly comparable, it is a complementary technique that can be used in conjunction with our diverse decoding method.",
      "exclude": true
    },
    {
      "heading": "5 EXPERIMENTS",
      "text": "In this section, we evaluate our approach on image captioning, machine translation, conversation and visual question generation tasks to demonstrate both its effectiveness against baselines and its general applicability to any inference currently supported by beam search. We also analyze the effects of DBS parameters, explore human preferences for diversity, and discuss diversitys importance in explaining complex images. We first explain the baselines and evaluations used in this paper. Baselines & Metrics. Apart from classical beam search, we compare DBS with the diverse decoding method proposed in Li & Jurafsky (2016). We also compare against two other complementary decoding techniques proposed in Li et al. (2015) and Wu et al. (2016). Note that these two techniques are not directly comparable with DBS since the goal is not to produce diverse lists. We now provide a brief description of the comparisons mentioned: - Li & Jurafsky (2016): modify BS by introducing an intra-sibling rank. For each partial solution, the set of |V| beam extensions are sorted and assigned intra-sibling ranks k [|V|] in order of decreasing log probabilities, t(yt). The log probability of an extension is then reduced in proportion to its rank, and continuations are re-sorted under these modified log probabilities to select the top B diverse beam extensions. - Li et al. (2015): train an additional unconditioned target sequence model U(y) and perform BS decoding on an augmented objective P (y|x) U(y), penalizing input-independent decodings. - Wu et al. (2016) modify the beam-search objective by introducing length-normalization that favors longer sequences. The joint log-probability of completed sequences is divided by a factor, (5 + |y|)/(5 + 1), where [0, 1]. We compare to our own implementations of these methods as none are publicly available. Both Li & Jurafsky (2016) and Li et al. (2015) develop and use re-rankers to pick a single solution from the generated lists. Since we are interested in evaluating the quality of the generated lists and in isolating the gains due to diverse decoding, we do not implement any re-rankers, simply sorting by log-probability. We evaluate the performance of the generated lists using the following two metrics: - Oracle Accuracy: Oracle or top k accuracy w.r.t. some task-specific metric, such as BLEU (Papineni et al., 2002) or SPICE (Anderson et al., 2016), is the maximum value of the metric achieved over a list of k potential solutions. Oracle accuracy is an upper bound on the performance of any re-ranking strategy and thus measures the maximum potential of a set of outputs. - Diversity Statistics: We count the number of distinct n-grams present in the list of generated outputs. Similar to Li et al. (2015), we divide these counts by the total number of words generated to bias against long sentences. Simultaneous improvements in both metrics indicate that output sequences have increased diversity without sacrificing fluency and correctness with respect to target tasks.",
      "exclude": false
    },
    {
      "heading": "5.1 SENSITIVITY ANALYSIS AND EFFECT OF DIVERSITY FUNCTIONS",
      "text": "Here we discuss the impact of the number of groups, strength of diversity , and various forms of diversity for language models. Note that the parameters of DBS (and other baselines) were tuned on a held-out validation set for each experiment. The supplement provides further discussion and experimental details. Number of Groups (G). Setting G=B allows for the maximum exploration of the search space, while settingG=1 reduces DBS to BS, resulting in increased exploitation of the search-space around the 1-best decoding. Empirically, we find that maximum exploration correlates with improved oracle accuracy and hence use G=B to report results unless mentioned otherwise. See the supplement for a comparison and more details. Diversity Strength (). The diversity strength specifies the trade-off between the model score and diversity terms. As expected, we find that a higher value of produces a more diverse list; however, very large values of can overpower model score and result in grammatically incorrect outputs. We set via grid search over a range of values to maximize oracle accuracies achieved on the validation set. We find a wide range of values (0.2 to 0.8) work well for most tasks and datasets. Choice of Diversity Function (). In Section 3, we defined () as a function over a set of partial solutions that outputs a vector of dissimilarity scores for all possible beam completions. Assuming that each of the previous groups influences the completion of the current group independently, we can simplify ( g1 h=1 Y h [t]) as the sum of each groups contributions as g1 h=1 (Y h [t]). In Section 3, we illustrated a simple hamming diversity of this form that penalizes selection of tokens proportionally to the number of time it was used in previous groups. However, this factorized diversity term can take various forms in our model with hamming diversity being the simplest. For language models, we study the effect of using cumulative (i.e. considering all past time steps), n-gram and neural embedding based diversity functions. Each of these forms encode differing notions of diversity and result in DBS outperforming BS. We find simple hamming distance to be effective and report results based on this diversity measure unless otherwise specified. More details about these forms of the diversity term are provided in the supplementary.",
      "exclude": false
    },
    {
      "heading": "5.2 IMAGE CAPTIONING",
      "text": "Dataset and Models. We evaluate on two datasets COCO (Lin et al., 2014) and PASCAL-50S (Vedantam et al., 2015). We use the public splits as in Karpathy & Fei-Fei (2015) for COCO. PASCAL-50S is used only for testing (with 200 held out images used to tune hyperparameters). We train a captioning model (Vinyals et al., 2015) using the neuraltalk21 code repository. Results. Table 1 shows Oracle (top k) SPICE for different values of k. DBS consistently outperforms BS and Li & Jurafsky (2016) on both datasets. We observe that gains on PASCAL-50S are more pronounced (7.14% and 4.65% SPICE@20 improvements over BS and Li & Jurafsky (2016)) than COCO. This suggests diverse predictions are especially advantageous when there is a mismatch between training and testing sets, implying DBS may be better suited for real-world applications. Table 1 also shows the number of distinct n-grams produced by different techniques. Our method produces significantly more distinct n-grams (almost 300% increase in the number of 4-grams produced) as compared to BS. We also note that our method tends to produce slightly longer captions compared on average. Moreover, on the PASCAL-50S test split we observe that DBS finds more likely top-1 solutions on average DBS obtains an average maximum log probability of -6.53 opposed to -6.91 found by BS of the same beam width. This empirical evidence suggests that using DBS as a replacement to BS may lead to lower inference approximation error. Human Studies. To evaluate human preference between captions generated by DBS and BS, we perform a human study via Amazon Mechanical Turk using all 1000 images of PASCAL-50S. For each image, both DBS and standard BS captions are shown to 5 different users. They are then asked Which of the two robots understands the image better? In this forced-choice test, DBS captions were preferred over BS 60% of the time by human annotators. Is diversity always needed? While these results show that diverse outputs are important for systems that interact with users, is diversity always beneficial? While images with many objects (e.g., a park or a living room) can be described in multiple ways, the same is not true when there are few objects (e.g., a close up of a cat or a selfie). This notion is studied by Ionescu et al. (2016), which defines a difficulty score: the human response time for solving a visual search task. On the PASCAL50S dataset, we observe a positive correlation ( = 0.73) between difficulty scores and humans preferring DBS to BS. Moreover, while DBS is generally preferred by humans for difficult images, both are about equally preferred on easier images. Details are provided in the supplement.",
      "exclude": false
    },
    {
      "heading": "5.3 MACHINE TRANSLATION",
      "text": "We use the WMT14 dataset containing 4.5M sentences to train our machine translation models. We train stacking LSTM models as detailed in Luong et al. (2015), consisting of 4 layers and 1024- dimensional hidden states. While decoding sentences, we employ the same strategy to replace UNK tokens. We train our models using the publicly available seq2seq-attn2 code repository. We report results on news-test-2013 and news-test-2014 and use the news-test-2012 to tune the parameters of DBS. We use sentence level BLEU scores to compute oracle metrics and report distinct n-grams 1https://github.com/karpathy/neuraltalk2 2https://github.com/harvardnlp/seq2seq-attn similar to image captioning. Results are shown in Table 2 and we again find that DBS consistently outperforms all baselines.",
      "exclude": false
    },
    {
      "heading": "5.4 DIALOG GENERATION",
      "text": "Dialog generation is a task that is inherently diverse as there are multiple valid ways to respond to a statement. We train a seq2seq model consisting of LSTMs as in Vinyals & Le (2015) on the Cornell Movie Dialogs Corpus (Danescu-Niculescu-Mizil & Lee, 2011) using the neuralconvo3 repository. The training dataset consists of 222,182 conversational exchanges between movie characters. Since automatic evaluation of dialog generation responses is an open research problem with existing metrics being poorly correlated with human judgement (Liu et al., 2016), we show qualitative results to demonstrate the effectiveness of DBS. Table 3 compares BS and DBS at B=3.",
      "exclude": false
    },
    {
      "heading": "5.5 VISUAL QUESTION GENERATION",
      "text": "We also report results on Visual Question Generation (VQG) (Mostafazadeh et al., 2016), where a model is trained to produce questions about an image. Generating visually focused questions is interesting because it requires reasoning about multiple problems that are central to vision e.g., object attributes, relationships between objects, and natural language. Furthermore, many questions could make sense for one image, so it is important that lists of generated questions be diverse. We use the VQA dataset (Antol et al., 2015) to train a model similar to image captioning architectures. Instead of captions, the training set now consists of 3 questions per image. Similar to previous results, using beam search to sample outputs results in similarly worded questions (see Fig. 3) and DBS brings out new details captured by the model. Counting the number of types of questions generated (as defined by Antol et al. (2015)) allows us to measure this diversity. We observe that the number of question types generated per image increases from 2.3 for BS to 3.7 for DBS (at B = 6).",
      "exclude": false
    },
    {
      "heading": "6 CONCLUSION",
      "text": "Beam search is widely a used approximate inference algorithm for decoding sequences from neural sequence models; however, it suffers from a lack of diversity. Producing multiple highly similar and generic outputs is not only wasteful in terms of computation but also detrimental for tasks with 3https://github.com/macournoyer/neuralconvo inherent ambiguity like many involving language. In this work, we modify Beam Search with a diversity-augmented sequence decoding objective to produce Diverse Beam Search. We develop a doubly greedy approximate algorithm to minimize this objective and produce diverse sequence decodings. Our method consistently outperforms beam search and other baselines across all our experiments without extra computation or task-specific overhead. DBS is task-agnostic and can be applied to any case where BS is used, which we demonstrate in multiple domains. Our implementation available at https://github.com/ashwinkalyan/dbs.",
      "exclude": true
    },
    {
      "heading": "APPENDIX",
      "text": "",
      "exclude": true
    },
    {
      "heading": "SENSIVITY STUDIES",
      "text": "Number of Groups. Fig. 4 presents snapshots of the transition from BS to DBS at B = 6 and G = 1, 3, 6. As beam width moves from 1 to G, the exploration of the method increases resulting in more diverse lists. Diversity Strength. As noted in Section 5.1, our method is robust to a wide range of values of the diversity strength (). Fig. 5a shows a grid search of for image-captioning on the PASCAL-50S dataset. Choice of Diversity Function. The diversity function can take various forms ranging from simple hamming diversity to neural embedding based diversity. We discuss some forms for language modelling below: - Hamming Diversity. This form penalizes the selection of tokens used in previous groups proportional to the number of times it was selected before. - Cumulative Diversity. Once two sequences have diverged sufficiently, it seems unnecessary and perhaps harmful to restrict that they cannot use the same words at the same time. To encode this backing-off of the diversity penalty we introduce cumulative diversity which keeps a count of identical words used at every time step, indicative of overall dissimilarity. Specifically, (Y h[t])[y g [t]] = exp( t bB I[y h b, 6=y g b, ])/ where is a temperature parameter control- ling the strength of the cumulative diversity term and I[] is the indicator function. - n-gram Diversity. The current group is penalized for producing the same n-grams as previous groups, regardless of alignment in time similar to Gimpel et al. (2013). This is proportional to the number of times each n-gram in a candidate occurred in previous groups. Unlike hamming diversity, n-grams capture higher order structures in the sequences. - Neural-embedding Diversity. While all the previous diversity functions discussed above perform exact matches, neural embeddings such as word2vec (Mikolov et al., 2013) can penalize semantically similar words like synonyms. This is incorporated in each of the previous diversity functions by replacing the hamming similarity with a soft version obtained by computing the cosine similarity between word2vec representations. When using with n-gram diversity, the representation of the n-gram is obtained by summing the vectors of the constituent words. Each of these various forms encode different notions of diversity. Hamming diversity ensures different words are used at different times, but can be circumvented by small changes in sequence alignment. While n-gram diversity captures higher order statistics, it ignores sentence alignment. Neural-embedding based encodings can be seen as a semantic blurring of either the hamming or n-gram metrics, with word2vec representation similarity propagating diversity penalties not only to exact matches but also to close synonyms. Fig. 5b shows the oracle performace of various forms of the diversity function described in Section 5.1. We find that using any of the above functions help outperform BS in the tasks we examine; hamming diversity achieves the best oracle performance despite its simplicity.",
      "exclude": false
    },
    {
      "heading": "IMAGE CAPTIONING EVALUATION",
      "text": "While we report oracle SPICE values in the paper, our method consistently outperforms baselines and classical BS on other standard metrics such as CIDEr (Table 4), METEOR (Table 5) and ROUGE (Table 6). We provide these additional results in this section.",
      "exclude": false
    },
    {
      "heading": "Dataset Method Oracle Accuracy (CIDEr)",
      "text": "Modified SPICE evaluation. To measure both the quality and the diversity of the generated captions, we compute SPICE-score by comparing the graph union of all the generated hypotheses with the ground truth scene graph. This measure rewards all the relevant relations decoded as against oracle accuracy that compares to relevant relations present only in the top-scoring caption. We observe that DBS outperforms both baselines under this measure with a score of 18.345 as against a score of 16.988 (beam search) and 17.452 (Li & Jurafsky, 2016).",
      "exclude": false
    },
    {
      "heading": "HUMAN STUDIES",
      "text": "For image-captioning, we conduct a human preference study between BS and DBS captions as explained in Section 5. A screen shot of the interface used to collect human preferences for captions generated using DBS and BS is presented in Fig. 6. The lists were shuffled to guard the task from being gamed by a turker. As mentioned in Section 5, we observe that difficulty score of an image and human preference for DBS captions are positively correlated. The dataset contains more images that are less difficulty and so, we analyze the correlation by dividing the data into three bins. For each bin, we report the % of images for which DBS captions were preferred after a majority vote (i.e. at least 3/5 turkers voted in favor of DBS) in Table 7. At low difficulty scores consisting mostly of iconic images one might expect that BS would be preferred more often than chance. However, mismatch between the statistics of the training and testing data results in a better performance of DBS. Some examples for this case are provided in Fig. 7. More general qualitative examples are provided in Fig. 8.",
      "exclude": false
    },
    {
      "heading": "DISCUSSION",
      "text": "Are longer sentences better? Many recent works propose a scoring or a ranking objective that depends on the sequence length. These favor longer sequences, reasoning that they tend to have more details and resulting in improved accuracies. We measure the correlation between length of a sequence and its accuracy (here, SPICE) and observe insignificant correlation between SPICE and sequence length. On the PASCAL-50S dataset, we find that BS and DBS have are negatively correlated ( = 0.003 and = 0.015 respectively), while (Li & Jurafsky, 2016) is correlated positively ( = 0.002). Length is not correlated with performance in this case. Efficient utilization of beam budget. In this experiment, we emperically show that DBS makes efficient use of the beam budget in exploring the search space for better solutions. Fig. 9 shows the variation of oracle SPICE (@B) with the beam size. At really high beam widths, all decoding techniques achieve similar oracle accuracies. However, diverse decoding techniques like DBS achieve the same oracle at much lower beam widths. Hence, DBS not only produces sequence lists that are significantly different but also efficiently utilizes the beam budget to decode better solutions.",
      "exclude": true
    }
  ]
}