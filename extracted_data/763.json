{
  "id": "763",
  "title": "Modelling Relational Time Series using Gaussian Embeddings",
  "abstract": "We address the problem of modeling multiple simultaneous time series where the observations are correlated not only inside each series, but among the different series. This problem happens in many domains such as ecology, meteorology, etc.  We propose a new dynamical state space model, based on representation learning, for modeling the evolution of such series. The joint relational and temporal dynamics of the series are modeled as Gaussian distributions in a latent space. A decoder maps the latent representations to the observations. The two components (dynamic model and decoder) are jointly trained. Using stochastic representations allows us to model the uncertainty inherent to observations and to predict unobserved values together with a confidence in the prediction.",
  "accepted": false,
  "zeroShot": [
    {
      "model": "claude-haiku-4-5",
      "decision": {
        "rejection": false,
        "confidence": 0.72,
        "primary_reason": "Novel probabilistic state-space model for relational time series with solid methodology, though experimental validation is limited and results are mixed compared to baselines"
      },
      "token": {
        "prompt_tokens": 4857,
        "completion_tokens": 57,
        "total_tokens": 4914
      },
      "time": "2026-02-05T23:18:05.993225+00:00"
    },
    {
      "model": "gpt-5.2",
      "decision": {
        "rejection": true,
        "confidence": 0.78,
        "primary_reason": "Insufficient methodological rigor and experimental evidence for a top-tier ML venue (unclear inference/training as a probabilistic model, limited/dated baselines, and weak empirical validation)."
      },
      "token": {
        "prompt_tokens": 4388,
        "completion_tokens": 55,
        "total_tokens": 4443
      },
      "time": "2026-02-09T21:46:33.402726+00:00"
    }
  ],
  "article": [
    {
      "heading": "1 INTRODUCTION",
      "text": "Relational time series, i.e. multiple time series where the observations are correlated both inside each series and between series occur in many domains such as ecology, medicine, biology, earth observation by satellite imagery or local measurements, multimedia or even social data analysis. The correlations between the different observed series can come from a proximity (e.g. earth observation or epidemic diffusion) or from a similarity of behavior (e.g. user traces in social data). In the statistical literature, the modeling of relational time series has been the topic of a dedicated field: spatio-temporal statistics (Cressie & Wikle (2011); Wikle & Hooten (2010)). Different methodologies have been developed for handling a large variety of spatio-temporal phenomena, with an emphasis on the analysis of natural observations like weather prediction, ecology or remote sensing. In the machine learning domain, there exists a vast literature dedicated to sequence or time series prediction. Recently, deep recurrent neural networks have witnessed notable successes in different sequence and time series modeling tasks leading to an increasing number of publications, e.g. (Barbounis et al. (2006); Hsieh et al. (2011); Cao et al. (2012); Hermans & Schrauwen (2013)). Despite a large number of recent developments, the modeling and analysis of relational time series has only attracted a few attention in the field of representation learning. In addition, most of the models are deterministic in the sense that they are trained to learn a fixed mapping for modeling the dynamics of the series. We propose a new state space model for relational time series able to model the uncertainty at the observation and at the modeling levels. The principle of this approach is to associate each point of a time series to a Gaussian distribution in a latent space, the distribution over the observed values being directly computed from these latent distributions. The model has two main components. One is responsible for the dynamics in the latent space. This component is thus modeling the evolution of the Gaussian distribution considering both the temporal intra-series and the relational inter-series Both authors contributed equally to this work dependencies. A second component acts as a decoder and maps the latent representations associated with each series to the corresponding observations in the output space. The contributions of the paper are thus: (i) a new dynamical model for relational time series inspired by representation learning; (ii) a stochastic component for modeling the uncertainties at the observation and dynamic levels The paper is organized as follows. In Section 2 we introduce some related work on forecasting in time series, representation learning for time series, and recent deep learning works focusing on modeling uncertainty. The model is presented in Section 3 together with four different variants. Section 4 presents experimental results on four datasets, and section 5 concludes this work and gives some perspectives.",
      "exclude": true
    },
    {
      "heading": "2 RELATED WORK",
      "text": "The classical topic of time series modeling and forecasting has given rise to an extensive literature. In statistics, classical linear models include many variations around auto-regressive and moving average models (De Gooijer & Hyndman (2006)). In machine learning, non linear extensions of these models based on neural networks have been proposed as early as the 90s, opening the way to many other non linear models including kernel methods (Muller et al. (99)). Relational time series have mainly been studied in the field of spatio-temporal statistics (Cressie & Wikle (2011); Wikle & Hooten (2010)). The traditional method first relied on a descriptive approach using the first and second-order moments of the process for modeling the spatio-temporal dependencies. More recently, dynamical state models, where the current state is conditioned on the past have been explored (Wikle (2015)). These models have been considered both for continuous/discrete space and time components. However, the most common way is to consider discrete time, leading to the modeling of time series of spatial processes as we do here. When space is discrete, the model comes down to a general vectorial autoregressive formulation. These models face a curse of dimensionality in the case of a large number of sources. Different strategies have been adopted to solve this problem such as embedding the spatio-temporal process in a low-dimensional manifold or parameter reduction (Wikle (2015)), leading to model families quite similar to the ones used in machine learning for modeling dynamical phenomena. Also, for complex underlying processes, observations only provide an incomplete description of the process dynamics so that modeling uncertainty at the data and model levels is an important topic. In the last 10 years, there has been a growing interest in learning latent representations for example through neural networks and deep learning. Dynamical state space models such as recurrent neural networks (RNN), which have been used for time series forecasting in different contexts since the early nineties (Connor et al. (1994)), have recently witnessed important successes in different areas for general sequence modeling problems, leading to breakthroughs in domains like speech (Graves et al. (2013)), language generation (Sutskever et al. (2011)), translation (Cho et al. (2014)), and many others. Among this family, the model closest to ours is the dynamic factor graph model of (Mirowski & LeCun (2009)) designed for multiple series modeling for the tasks of forecasting and imputation. However this model does not consider relational dependencies which is the focus of our approach. Most of the above models make use of pointwise representations and do not model explicitly the uncertainties present in the process and/or in the observations. Recently, in the learning representation community, there has been a growing interest in using distributions as latent representations instead of points. (Vilnis & McCallum (2015); He et al. (2015); Dos Santos et al. (2016)) all make use of Gaussian distributions for representing different items like words (Vilnis & McCallum (2015)), nodes in knowledge graphs (He et al. (2015)) or nodes in graphs for transductive classification (Dos Santos et al. (2016)). Note that Gaussian processes have also been used for time series prediction, but they have mainly been considered for univariate time series prediction (Hachino & Kadirkamanathan (2011); Brahim-Belhouari & Bermak (2004)) and they do not use a state space formulation. Recent techniques in variational inference (Kingma & Welling (2014); Rezende et al. (2014)) deal with uncertainty by modeling distributions in the observation space, mapping random variables within a latent space to observations with a deep neural network. Extension of the variational in- ference method to time series has been proposed (Fraccaro et al. (2016); Krishnan et al. (2015)) but contrarily to those works, we take into account relationships (both temporal and relational). Furthermore, in our model, we work directly with random variables to predict observations from time series. This gives us direct access to the output distribution with no need to sample or work with intractable distributions. Our model is built on top of the model in (Ziat et al. (2016)) which proposes a deterministic dynamical process model but does not consider any explicit modeling of uncertainty. In this paper, we propose a model that uses Gaussian embeddings, and extend the dynamics and loss functions of the model in (Ziat et al. (2016)).",
      "exclude": true
    },
    {
      "heading": "3 FORECASTING OF RELATIONAL TIME SERIES",
      "text": "",
      "exclude": false
    },
    {
      "heading": "3.1 NOTATIONS AND TASKS",
      "text": "Let us consider a set of n temporal sequences1 x1, ..,xn such that x ptq i P R is the value of the ith sequence at time t defined by xi pxp1qi , .., x pT q i q, T being the number of observed time steps. For simplification, we consider that all the series have the same length, but this is not restrictive. We model the dependencies between the different series through a graph, the different series sources being the graph vertices and the links modeling explicit dependencies between the sources. These links can reflect a spatial proximity between the sources of the series, a similarity of behavior between users or any other predefined relation. These explicit relations will be modeled in the latent space. Our hypothesis is that they will constrain the representation of linked sources to be similar one to another in the latent space, this similarity being controlled by the strength of the link between the two time series, denoted ei,j . We assume that the graph structure is static in time and is provided as a prior information. The model can be extended to learn these static dependencies but this is not considered here. Let us denote the size of the prediction horizon. The forecasting problem considered here is to compute for all series i the values xpT`kqi for all k in r1; s. Note that the model can be straightforwardly extended to the imputation problem that aims at predicting missing values.",
      "exclude": false
    },
    {
      "heading": "3.2 INFORMAL DESCRIPTION",
      "text": "The proposed model is a dynamic state space model: the dynamics is modeled in a continuous latent state space and the observations are generated from states in this latent space. State space models have already been considered for multiple time series (e.g. Mirowski & LeCun (2009)) and for spatio-temporal processes (e.g. Wikle & Hooten (2010)). Both the observations and the dynamics are subject to uncertainties. Usually, the observations correspond to a partial view of the underlying generating process and the dynamics being hidden is not directly accessible and should be modeled as a stochastic process. To handle this uncertainty, we propose a model, namely Relational Dynamic model with Gaussian representations (RDG), that represents latent factors as distributions in a latent space and learns the series dynamics in this latent space. The distributions themselves are estimated using observations like for any other representation learning model. Besides being more adapted to handling the noise inherent to the process and to the observations, the model can be used to predict the posterior distribution of the variables associated to the series and in particular the confidence or variance associated to the predictions. The model is an extension of the deterministic model of (Ziat et al. (2016)) and has two main components: (i) Decoding component: we consider that each series corresponds to a particular trajectory in an unknown latent space. Each series xp1qi , ...., x pT q i is thus associated to a series of random variables in Rd denoted Zp1qi , ...., Z pT q i , Z ptq i being the latent factor explaining the observed value of the series i at time t and d the size of the latent space. We model each Zptqi as a multivariate 1For simplicity, we consider univariate time series, but the model can be trivially extended to multivariate time series. normal variable N pptqi , ptq i q. The observation can be computed from this latent distribution by using a decoding function mapping Zptqi to X ptq i fpZ ptq i q. (ii) Dynamic component: The second component models the series dynamics in the latent space. We suppose that dynamics can be captured for all series through a function h that maps the latent random variable Zptqi to the next latent variable Zpt`1qi hpZ ptq i q. The function h is thus modeling the time dynamics. In addition, constraints are introduced to reflect prior knowledge about the relational dependency structure of the series. For any couple of series i and j with a known dependency, i.e. such that ei,j a 0 we add a corresponding constraint on Zptqi and Z ptq j as explained in Section 3.3.3. In the following, we explain how the distributions corresponding to the random variables Zptqi are learned, jointly to the functions f (decoder component) and h (dynamic component).",
      "exclude": false
    },
    {
      "heading": "3.3 MODEL DEFINITION",
      "text": "We suppose that the random variables Zptqi follow a Gaussian distribution. Let us denote Z ptq i N pptqi , ptq i q a distribution where ptq i and ptq i have to be estimated from known observations. For simplicity, we consider in the following that ptqi is a diagonal matrix, with ptq i,j denoting the jth value of the diagonal of ptqi . We define a global loss function Lp,, f, hqwhere and are the means and covariance matrices for all the series and for all the time steps between 1 and T . The loss is a sum of three terms: (i) a decoding loss De, (ii) a dynamical loss Dy and (iii) a structural loss R: Lp,, f, hq n y i1 T y t1 DepfpZptqi q, x ptq i q ` Dy n y i1 T 1 y t1 DypZpt`1qi , hpZ ptq i qq ` R n y j1 T y t1 ei,jRpZptqi , Z ptq j q (1) where Dy and R are hyperparameters weighting the importance of the different elements in the loss function. The first term corresponds to the decoding component, and forces both f and the learned distributions of variables Z to explain the observations, the second term, the dynamic component, encourages h to model the time dynamics in the latent space, while the third term captures the relations between the pairs of series. In the following, we use for f a linear function and h will be either a linear or non-linear function (see Section 3.3.2). Learning: Learning the model is performed through the minimization of the loss function Lp,, f, hq with respect to , , f and h. To simplify the notations, the parameters of f and h are not made explicit in the notations f and h are supposed to be differentiable. At the end of the learning process, all the latent distributions for each of the time steps are known for the training data, as well as the decoding function f and the dynamical one h. We used ADAM (Kingma & Ba (2015)) as a stochastic gradient descent technique. This optimization can be easily made on a large scale dataset, and/or by using GPUs.",
      "exclude": false
    },
    {
      "heading": "3.3.1 FROM LATENT SPACE TO OBSERVATIONS",
      "text": "The mapping onto the latent space is learned so that the values xptqi of each series can be predicted from their respective Gaussian embedding Zptqi through the f function. We define below two alternative decoding loss functions De, used in the experiments for measuring the error between the predicted distribution fpZptqi q and the observation x ptq i . Other losses could be used with the same model. The first loss measures the difference between the expected value of f and the observation using a mean-square error: De1pfpZ ptq i q, x ptq i q def E fpZptqi q xptqi 2 (2) When considering a linear decoding function such as fp q a , a , being the set of parameters of f , De1 can be rewritten as as: De1pfpZ ptq i q, x ptq i q pa , ptq i a x ptq i q 2 (3) The second loss aims at measuring the distance between the random variable modeling the predicted observations and the observations. This is the expectation of the mean squared error between the predictions and the observations: De2pfpZ ptq i q, x ptq i q def E pfpZptqi q x ptq i q 2 (4) When f is a linear function, this loss can be written as: De2pfpZ ptq i q, x ptq i q d y k1 2k ptq i,k ` a , ptqi a x ptq i 2 (5) Minimizing De1 only updates the mean of the distributions, whereas minimizing De2 updates both the mean and the variance. More specifically, an observed value with De2 will pull the variances ptq i down. This is an interesting property since observing values should reduce the variance of the representation. Moreover, this effect will be higher for the dimensions of the latent space where the value of is higher. This is sensible since variance is reduced for the dimensions that are important for the prediction.",
      "exclude": false
    },
    {
      "heading": "3.3.2 MODELING DYNAMICS",
      "text": "The loss function Dy aims at finding values Z p.q i and a dynamic model h, that will be used to predict the representation of the next state of time series i, Zpt`1qi . The function h maps a distribution N pptqi , ptq i q to N p pt`1q i , pt`1q i q. Based on (Vilnis & McCallum (2015); Dos Santos et al. (2016)), we use a Kullback-Leibler divergence (noted DKLp || q) to compare the distribution at pt` 1q to the distribution predicted by h. We propose in the following two alternative functions for h. For the first one, we consider that the latent representation at time pt` 1q is a linear transformation of the latent distribution at time t. The transformed variable is also a Gaussian and its parameters can be easily computed. In this case, h is a linear function from Rd to Rd which is represented by a matrix PMd,dpRq: Dy1pZ pt`1q i , hpZ ptq i qq def DKLpZpt`1qi ||Z ptq i q DKLpZ pt`1q i ||N p ptq i , ptq i T qq (6) Linear transformations of random vectors might be too restrictive to model complex processes. As an alternative transformation, we used two non linear multilayer perceptrons (MLP), one hm for predicting the means and one for hc for predicting the variance: the next mean is given by pt`1q i hmp ptq i , ptq i q, and the next variance by pt`1q i hcp ptq i , ptq i q. This gives: Dy2pZ pt`1q i , hpZ ptq i qq def DKLpZpt`1qi ||N ph mpptqi , ptq i q, h cpptqi , ptq i qqq (7) Note hat in the second case, we also make the hypothesis that the resulting distribution (for Zpt`1qi ) is Gaussian. In the two cases, the KL divergence between the two Gaussian distributions has a simple analytic form from which the gradient can be easily computed2.",
      "exclude": false
    },
    {
      "heading": "3.3.3 STRUCTURAL REGULARIZATION TERM",
      "text": "At last, R corresponds to a structural regularization over the graph structure that encourages the model to learn similar representations for time series that are interdependent. This forces the model to learn representations that reflect the structure dependencies between the series. Recall that these 2DKLpZptqi ||Z ptq j q 12 ptrp ptq j 1 ptq i q ` p ptq j ptq i qT ptq j 1 pptqj ptq i q d logp |ptqi | |ptqj | qq dependencies are supposed to be provided as priors for this model. We define this regularization loss as: RpZptqi ||Z ptq j q DKLpZ ptq i ||Z ptq j q (8) which again has, for Gaussian random variables, a simple analytical form that can be used for learning. Minimizing the regularization term R has a direct impact on the distributions of the predicted observations for connected times series. More precisely, we have the following inequality: dTV fpZptqi q, fpZ ptq j q d d d DKLpZptqi ||Z ptq j q 2 (9) with dTV being the total variation distance of probability measures, i.e.: dTV pX,Y q sup APBorel p|DXpAq DY pAq|q (10) with X and Y being to random variables of density distribution respectively DX and DY , and Borel being the Borel set of Rn (roughly, cuboids in Rn). This means that having relatively similar representations (regarding the KL-divergence) constrains the predicted values to be similar. For more details see Appendix A.",
      "exclude": false
    },
    {
      "heading": "3.4 INFERENCE",
      "text": "During inference when forecasting values, the latent distributions at pT ` 1q are deduced from the ones at time T and follow N phppT qi , pT q i qq, distributions at pT ` 2q follow N ph hp pT q i , pT q i qq, and so on...",
      "exclude": false
    },
    {
      "heading": "4 EXPERIMENTS",
      "text": "",
      "exclude": false
    },
    {
      "heading": "4.1 DATASETS AND BASELINES",
      "text": "Experiments have been performed on four datasets respectively extracted from Google Flu Trends3, WHO4 and from two datasets from Grand Lyon5 (GL) (respectively data from traffic conditions and from car parks occupancy). All the series are normalized. For all datasets, we used binary dependency relations indicating whether two series are related or not. The Google Flu Trend (GFT) dataset is composed of an aggregation of weekly Google search queries related to the flu in 29 countries. This dataset spans about ten years of time. The binary relations between series are defined a priori so that the series of two countries i and j are linked, i.e. ei,j 1 in Equation (1), only if the countries have a common frontier. There are 96 relations in all. The GL Traffic (GL-T) dataset corresponds to the traffic conditions of the 50 busiest roads of the city of Lyon (France). Data is aggregated on 20 minutes windows spanning 15 days. The binary relations between series are based on the geographical proximity of roads. There are 130 relations in total. The GL Park (GL-P) dataset represents the occupancy of public car parks in Lyon. The series correspond to the occupancy of the 30 busiest car parks. It has the same window and period of time as the previous dataset, and the binary relations between series are based on the geographical proximity of car parks. There are 74 relations in total. The WHO dataset provides the number of deaths caused by diphtheria over 91 different countries, giving rise to 91 time series. The binary relations between series are defined so that two series are linked if the corresponding countries share a common frontier. There are 228 links in total. We compare our approach with five baselines : Auto-Regressive (AR), a monovariate linear auto-regressive model. It computes its predictions based on a learned linear function of a fixed number p of past values of the series. The order p of the model is a hyperparameter of the model selected by a grid search. Feed Forward Neural Network (FFNN), representative of non-linear 3http://www.google.org/flutrends 4http://www.who.int 5http://data.grandlyon.com auto-regressive models of order p where the non-linear function is modeled as a feed-forward neural network with one hidden layer of size s. In this case, p and s are hyperparameters selected by grid search. RNN, a recurrent neural network with one hidden layer of size s of recurrent units and tanh non-linearities. The RNN model is a state space non-linear auto-regressive model with exogenous inputs (the past values of the series). Note that this model should in principle be able to learn the inter-series dependencies, but the dependencies are not modeled explicitly as they are in our model. Also the RNN does not introduce explicit modeling of uncertainties. KF (Kalman (1960)), is a classic Kalman Filter with linear transformations from one state to another. DFG (Mirowski & LeCun (2009)), a state of the art model that learns continuous deterministic latent variables by modeling the dynamics and the joint probabilities between series. All the hyperparameters of the baselines have been set using a validation set by grid search, including the best architectures for the dynamic model hwhen it is a multi-layer perceptron with one hidden layer or a linear model. . For the evaluation we have considered a root-mean-square error (RMSE) criterion. Regarding the experimental protocol, models are evaluated using cross-validation with rolling origin.",
      "exclude": false
    },
    {
      "heading": "4.2 RESULTS",
      "text": "Let us first present the performance of our model w.r.t. the baselines for prediction at horizon 1 in Figure 1b We have tested the four variants of our approach i.e combinations of De1 or De2 with Dy1 or Dy2 . The proposed model obtains the best results on all the datasets except GFT where KF performs better. Otherwise it outperforms the baselines on two datasets (GL-P -Grand Lyon Parksand GFT -Google Flu Trends- on the table) and gets results similar to the RNN on the two others (GL-T -Grand yon Traffic- and WHO). The non linear dynamical model used for Dy2 usually gets better results than other models, the best combination being the use of the MSE expectation error for the decoder and the non-linear model for the dynamics (denoted RDG2,2 on the figure). The non linear dynamical model used for Dy2 usually gets better results than other models, the best combination being the use of the MSE expectation error for the decoder and the non-linear model for the dynamics (denoted RDG2,2 on the figure). Figure 1a shows the prediction quality (RMSE) at pT `1q, pT `2q, pT `3q, pT `4q and pT `5q and illustrates the ability of RDG to predict correctly at different horizons. Here again, the performance of RDG is very close to the performance of the Recurrent Neural Network. One can remark that at pT ` 5q KF does not goes the distance since it performs well at pT ` 1q but quite badly at pT ` 5q in comparison to other baselines. RDG has the additional property of modeling the uncertainty associated to its predictions, which is not the case for a RNN. Let us consider the curves presented in Figure 2. They illustrate, the predictions made by our model together with their associated variance computed through the Gaussian embeddings. First, one can see that the ground truth values are always within the confidence interval provided by our model, which means that RDG computes relevant minimum and maximum possible values. Another aspect is that the size of the interval increases with the prediction horizon, which is what is expected from such a model. The latter is then able to predict relevant confidence values for its predictions. Comparison between RDG with/without structural regularization or uncertainty We compare in Table 1 the results between our model when taking into account the neighborhood graph (R 0) or not (R 0): forecasts are uniformly worse for all datasets when we do not take into account the neighborhood graph, it suggests that the regularizer improves the model when the input graph is relevant. Furthermore, we give the results obtained without uncertainty, which corresponds to the model described in (Ziat et al. (2016)) (denoted Rainstorm): here again, our model outperforms the previous one for all the datasets.",
      "exclude": false
    },
    {
      "heading": "5 CONCLUSION AND FUTURE WORK",
      "text": "We have proposed a model for relational time series forecasting. Our model (RDG) is based on latent Gaussian embeddings, and has shown competitive performance on four different datasets compared to state-of-the-art models. Moreover, RDG allows us to model the uncertainty of predictions, providing for example confidence intervals for each prediction. Future work will investigate more complex dynamic and prediction functions, as well as observing the behavior of the model for imputation tasks.",
      "exclude": true
    }
  ],
  "fewShot": [
    {
      "model": "claude-haiku-4-5",
      "decision": {
        "rejection": false,
        "confidence": 0.72,
        "primary_reason": "Novel probabilistic state-space model with principled uncertainty quantification for relational time series. Strong theoretical grounding (KL divergence, Gaussian embeddings), well-motivated structural regularization, and comprehensive experimental validation across multiple domains. Solid contribution despite incremental improvements over baselines."
      },
      "token": {
        "prompt_tokens": 5019,
        "completion_tokens": 96,
        "total_tokens": 5115
      },
      "time": "2026-02-08T22:48:20.547108+00:00"
    },
    {
      "model": "gpt-5.2",
      "decision": {
        "rejection": true,
        "confidence": 0.78,
        "primary_reason": "Incremental latent Gaussian state-space model with graph KL regularization; empirical evaluation is dated/limited (small set of baselines, mostly RMSE, no strong modern spatiotemporal graph forecasting comparisons or likelihood/uncertainty calibration), and the method appears close to prior work (graph-regularized latent SSMs / variational SSMs) without a clearly compelling new technical insight or rigorous analysis."
      },
      "token": {
        "prompt_tokens": 7022,
        "completion_tokens": 110,
        "total_tokens": 7132
      },
      "time": "2026-02-09T20:51:46.556849+00:00"
    }
  ],
  "test": false
}