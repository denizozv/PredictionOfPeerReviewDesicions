{
  "id": "512",
  "title": "Nonparametrically Learning Activation Functions in Deep Neural Nets",
  "abstract": "We provide a principled framework for nonparametrically learning activation functions in deep neural networks. Currently, state-of-the-art deep networks treat choice of activation function as a hyper-parameter before training. By allowing activation functions to be estimated as part of the training procedure, we expand the class of functions that each node in the network can learn. We also provide a theoretical justification for our choice of nonparametric activation functions and demonstrate that networks with our nonparametric activation functions generalize well. To demonstrate the power of our novel techniques, we test them on image recognition datasets and achieve up to a 15% relative increase in test performance compared to the baseline.",
  "accepted": false,
  "zeroShot": [
    {
      "model": "claude-haiku-4-5",
      "decision": {
        "rejection": false,
        "confidence": 0.72,
        "primary_reason": "Novel approach to learning activation functions with theoretical generalization bounds and consistent empirical improvements, though limited experimental scope and modest gains on standard benchmarks"
      },
      "token": {
        "prompt_tokens": 8362,
        "completion_tokens": 56,
        "total_tokens": 8418
      },
      "time": "2026-02-05T22:54:21.315217+00:00"
    }
  ],
  "article": [
    {
      "heading": "1 INTRODUCTION",
      "text": "Deep learning techniques have proven particularly useful in the classification setting, recently surpassing even human performance on some image-classification tasks. We seek to advance the state of the art by learning not only weights between neurons in the network but also part of the network structure itself the activation functions. Current deep learning literature largely focuses on improving architectures and adding regularization to the training process. By contrast, the realm of learning the network structure itself is relatively unexplored. Current best practice often treats network size, shape and choice of activation function as a hyper-parameter to be chosen empirically. Instead, we propose to learn the activation functions through nonparametric estimation. We introduce a class of nonparametric models for activation functions. Importantly, we ensure that our technique can be incorporated into the back-propagation framework. This is crucial because it means our method can be easily added to current practice. To this end, we propose learning functions via basis expansion. In particular, we find that using a Fourier basis works well in practice and offers improved performance over the baseline on several benchmark datasets. We see relative improvements in test error rates of up to 15%. Nonparametric activation functions in dropout nets are especially successful. We also introduce a two-stage training process. First, a network without nonparametric activation functions is trained. Then, the learned network is used as an initialization for an identical network with nonparametric activation functions. This method of initializing the network yields considerable improvements in performance for convolution neural networks on image classification tasks. Lastly, we consider the algorithmic stability approach to accessing generalization bounds. We use this to demonstrate that feed-forward networks with our method of nonparametrically estimating activation functions generalize well. To summarize, our contributions are the following: A theoretically justified framework for learning activation functions in a neural network, Provable bounds on the generalization error of networks where the activation functions are learned, and An optional two-stage training process that can greatly improve results for certain network architectures, specifically those with convolution layers.",
      "exclude": true
    },
    {
      "heading": "2 RELATED WORKS",
      "text": "Recent work from Agostinelli et al. (2015) describes piecewise linear activation functions. Theoretically speaking, our approach improves upon this by fully exploring the space of possible activation functions not merely considering those which are piecewise linear. Learning activation functions, in conjunction with the weights in the layer transformations, allows for the fitting of an arbitrary function of the output of the previous layer. A similar idea is Network in Network, due to Lin et al. (2013). There the authors propose to replace standard convolution layers with what they call mlpconv layers. Traditional convolution layers, if using a sigmoid activation, are essentially learning a generalized linear model of the previous layers output. Their innovation is to use a more general nonlinear function estimator as the filter a multilayer perceptron. Thus, their scheme fits in the back-propagation framework and so is easy to implement. Our approach is different because though we are learning arbitrary functions of the previous layer, our activation function approach can be used in any layer of the network and in networks without convolution layers, which theirs cannot. Maclaurin et al. (2015) propose to learn the hyper-parameters of a neural net through reverse gradient descent. This is similar to the work we present here in that they also provide a method for learning parameters of the network.",
      "exclude": true
    },
    {
      "heading": "3 NONPARAMETRIC ACTIVATION FUNCTIONS",
      "text": "Here we describe our main contribution: the nonparametric activation function model. We also provide justification for a weight tying scheme to use nonparametric activations in conjunction with convolution layers, a regularization which proves powerful empirically (see Section 5).",
      "exclude": false
    },
    {
      "heading": "3.1 FOURIER BASIS EXPANSION FOR ACTIVATION FUNCTIONS",
      "text": "For the nonparametric estimation we use a Fourier series basis expansion. One concern, however, with Fourier basis expansion is that function approximation over a fixed interval often exhibits poor tail behavior. To combat that, we fit the approximation over an interval [L,L] and then use it over truncation of the interval, [L+T, L+T ]. The designation NPF stands for Nonparametric Fourier Basis Expansion for Activation Functions. Definition 3.1. Given some sample size n, an interval [L,L] and tail truncation T where 0 T L, the activation function NPF(L, T ) is parametrized by (a0, . . . , ak) and (b1, . . . , bk) given by f(x) = a0 + k i=1 ai cos((L+ T )ix/L) + bi sin((L+ T )ix/L) x L T k grows with sample size n and is given by k = dn1/7e. We choose to present basis expansion of arbitrary functions on the interval [L,L] because it is thought to be advantageous in practice to train neural networks with activation functions that are antisymmetric through the origin (LeCun et al., 1998). Though the nonparametric activation functions learned may not be symmetric about the origin, this approach will hopefully result in them being as close to symmetric as possible. Each node in the fully connected layers estimates a potentially different activation function. However, in the convolution layers one activation function is learned per filter. The power of a convolution net is the weight tying scheme in the convolution layer, and we preserve that by only estimating one activation function per filter. In Section 5 the effectiveness of this approach can be seen. NPFC(L, T ) is the notation for the nonparametrically estimated activation functions in convolution layers. Figure 1 shows some examples of activation functions learned on the CIFAR-10 dataset.",
      "exclude": false
    },
    {
      "heading": "3.2 TRAINING WITH LEARNED ACTIVATION UNITS",
      "text": "A practical difficulty is that training nonparametric deep nets often proved unstable, specifically for networks with convolution layers. To remedy this our two-stage procedure is given as a generic description in Algorithm 1. Algorithm 1 Generic Two Stage Training for Deep Convolutional Neural Networks Input: A network architecture, hyper parameters T ,L 1: Instantiate a network with ReLU activations in convolution layers and NPF(T, L) in all others. 2: Run training algorithm of choice. 3: Instantiate a new network with NPFC(T, L) activations in convolution layers and NPF(T, L) in all others. Instantiate all weights from the trained network. Instantiate NPFC(T, L) as desired. 4: Run training algorithm of choice. 5: return Network weights resulting from the second training. One point that is not immediately clear is how to initialize NPF(L, T ) activation functions. A good initialization, of any parameter in the network, is crucial to the success of fitting the model. We choose to initialize them to the Fourier series approximation of the tanh function. This works quite well as will be seen in Section 5. Also left to choice is how to perform both stages of training. Of particular interest is the training method used for the second stage. We explored the following two approaches: In stage 2, train only the nonparametric activation functions, holding the remainder of the network constant. In stage 2, train all weights in the network together. We discovered that both methods are successful, but allowing for the activations and weights in the network to be trained together gave slightly better results empirically, so it is those that are reported in Section 5. Figure 2a shows the training path for a convolution network on the CIFAR-10 dataset. The network has three convolution layers followed by two fully connected layers. It is initially trained with rectified linear units as activations in the convolution layers and with NPF(4, 1) activations in the fully connected layers. The stage two network, instead, has NPFC(4, 1) activations in the convolution layer. 0 20 40 60 80 100 Epochs Trained 0% 10% 20% 30% 40% 50% 60% 70% 80% E rr o r R a te Begin Stage Two Test and Training Error Rate on CIFAR-10 for Two Stage Training Training Error Test Error (a) The training and testing error paths during training. Notice the jump in error rate after stage two begins, but that ultimately it settles into better generalization performance. This is for a convolutional neural net with three convolution and two fully connected layers trained on CIFAR-10. 0 5000 10000 15000 20000 Number of Weight Updates 0% 10% 20% 30% 40% 50% 60% 70% 80% 90% E rr o r R a te Test Error Rate on CIFAR-10 Single-Stage Training with NPF(4, 1) Activations Baseline CIFAR-10 (b) Test errors for a convolution neural net trained on CIFAR-10. Each line represents one network trained with random initialization. Five random initializations and training paths for each network type are shown.",
      "exclude": false
    },
    {
      "heading": "4 ALGORITHMIC STABILITY APPROACH TO GENERALIZATION BOUNDS",
      "text": "For our model to be theoretically sound, its generalization error should vanish with a sufficiently large sample size. This is true for standard neural networks, and such generalization bounds are well known (for example via VC bounds (Bartlett & Anthony, 1999)). However, because we expand the function class, we can not naively apply the traditional results. Instead of the VC bound route, we take the algorithmic stability approach. In recent work from Hardt et al. (2015) the authors explore a class of bounds for stochastic gradient methods. They build on the definition of stability given in Bousquet & Elisseeff (2002). Denote by f(w, z) some loss function of the output of a model, where w indicates the parameters of the learned model and z is an example from XY , the space of data and labels. A is an algorithm and D is the training data. Denote w = A(D) as the parameters of the model learned by the algorithm A. We take the following definition from Hardt et al. (2015). Definition 4.1. An algorithm is -uniformly stable if for all data sets D,D (X Y )n such that D,D differ in at most one example, we have sup z EA[f(A(D), z) f(A(D), z)] . The expectation is taken with respect to any randomness present in the algorithm itself. By stab(A,n) be the infimum over all for which the statement holds. Both Bousquet & Elisseeff (2002); Hardt et al. (2015) present Theorem 4.2 regarding the generalization ability of -uniformly stable algorithms is given Theorem 4.2. Take some algorithm -uniformly stable algorithm A. Then |ED,A[RD[A(D)]R[A(D)]]| . The notation RD[w] signifies the empirical risk when the loss function depends on w in addition to z. Likewise for the notation RD[A(D)]. Lastly, we have Theorem 4.5 also from Hardt et al. (2015) that will allow us to derive a generalization bound for our model trained with stochastic gradient descent. Definition 4.3. A function f is L-Lipschitz if for all points u, v dom(f), |f(u) f(v)| L||u v||. Definition 4.4. A function f : R is -smooth if for all u, v , ||f(u)f(v)|| ||u v||. Theorem 4.5. Assume that f(,z) [0, 1] is an LLipschitz and -smooth loss function for every z. Suppose that we run SGM for T steps with monotonically non increasing step sizes t c/t. Then SGM has uniform stability with stab 1 + 1/c n 1 (2cL2) 1 c+1T c c+1 . In particular, up to some constant factors, the following relationship approximately holds stab T 11/(c+1) n . (Hardt et al., 2015) In order to apply the theorem, we demonstrate our network architecture and choice of loss function meets this requirement. Once -stability is established, generalization follows by Theorem 4.2. 4.1 FEED-FORWARD NETWORKS WITH NPF(T, L) ACTIVATIONS For the full proofs of results in this section, see Appendix A. Assume a network architecture of Q fully connected hidden layers (affine transformation layers) fed into a softmax layer which classifies into one of c classes. The weight vector for the network w should be viewed as the all parameters of the affine transformations and activation functions. The loss function is the negative log-likelihood of the network output, which is a softmax over c classes. We treat the softmax layer as two layers for the purposes of demonstrating the generalization bound. One layer is an affine transformation of the output of the lower layers in the network to c nodes. We will denote the result of this computation at each node i in the affine transformation layer within the softmax layer as gQ+1(w, x)i. The second layer maps the outputs at these c nodes to c nodes this will be the network output. It computes at the the ith node oi(w, x) = exp gQ+1(w, x)ic i=1 exp gQ+1(w, x)i . Thus, on some training example (x, y), the negative log-likelihood of the network output is f(w, (x, y)) = log(oi(w, x)) = gQ+1(w, x)y + log ( c i=1 exp gQ+1(w, x)c ) . If we can demonstrate that this f has the desired properties, then the generalization bound given in Section 4 will apply. One point to note is that in order to control the gradients of the various transformations in the layers in the network, we need to control the norm of the weight matrices W and biases b in the affine layers and the coefficients of the activations NPF(L, T ). This is a common technique and is compatible with the framework in Hardt et al. (2015), as it only makes updates less expansive. Note that though f does not have range [0, 1], it is bounded due to the max-norm constraints and can be rescaled. Some smoothing is assumed in order achieve the bound, but in practice the smoothing is not a concern as it can be assumed to take place in an -neighborhood of the transition from one piece of the activation function to another. The smoothed nonparametric activation functions are denoted NPF (L, T ). Recall that k = dn1/7e. Theorem 4.6. Consider a fully connected network with a softmax output layer and NPF (L, T ) activation functions. Assume we use SGD with a max-norm constraint on the weight matrix and biases in the affine layers and on the coefficients of the activation functions to train the network. Then there exists some K such that the loss function as given in Section 4.1 is K-Lipschitz. Specifically, for any such network where the number of hidden layers Q 1 and there are p nodes per layer, K = max O ( Q(pQ+1k3Q+p(2Q+1)/2k3Q1L) Q ) ,O ( Q(pQ+1k3Q+p(2Q+1)/2k3Q1L) QLQ ) . Proof. The full proof is in Appendix A. The proof idea is to bound the first partial derivatives of the loss function f with respect to each parameter in the network. Then we can proceed recursively down through the network to obtain a bound. From there the gradient can be bounded, implying the loss function f is Lipschitz. Theorem 4.7. Consider a fully connected network with a softmax output layer and NPF (L, T ) activation functions. Assume we use SGD with a max-norm constraint on the weight matrix and biases in the affine layers and on the coefficients of the activation functions to train the network. Then there exists some K such that the loss function as given in Section 4.1 is -smooth. Specifically, for any such network where the number of hidden layers Q 1 and there are p nodes per layer, = max O ( Q p3Q+5/2k9Q2 L2Q 3Q1 ) ,O ( Q p2 Q+Q+2k52 Q+3QL2 Q1+1 2Q+Q ) . Proof. The full proof is in Appendix A. The proof idea is to bound the second partial derivatives of f with respect to each of the parameters in the network. Then we can bound |max(2f)| and thus the largest singular value. This gives an upper bound of on ||2f ||2, implying f is -smooth. Theorem 4.8. Consider a fully connected network with a softmax output layer and NPF (L, T ) activation functions. Assume we use SGD with a max-norm constraint on the weight matrix and biases in the affine layers and on the coefficients of the activation functions to train the network. Then, for all practical values of L,T , ,Q and p, the resulting Lipschitz constant K and smoothness constant are sufficiently large that stab T n 1 . Thus if T = O( n), stab 0 as sample size n. Proof. By Theorems 4.5, 4.6, and 4.7 the result is immediate.",
      "exclude": false
    },
    {
      "heading": "5 EXPERIMENTAL RESULTS",
      "text": "In this section we provide empirical results for our novel nonparametric deep learning model. The goal of these experiments is to demonstrate its efficacy when used with several different architectures and on several different datasets, not to beat the current best result on either dataset. Crucially, our experiments provide an apples to apples comparison between networks with identical architectures and employing the same regularization techniques where the only difference is choice of activation function. The generalization results from the Section 4 are directly relevant to the experimental results here. In all experiments listed in this section, the reported testing error is an estimation of the generalization error because the final training error was zero in all cases.",
      "exclude": false
    },
    {
      "heading": "5.1 IMPLEMENTATION, REGULARIZATION AND ARCHITECTURES",
      "text": "To implement the various deep architectures, we use the Python CPU and GPU computing library Theano (Bergstra et al., 2010). We use Theano as an interface to the powerful CUDA and CuDNN libraries (John Nickolls, 2008; Chetlur et al., 2014) enabling fast training for our neural networks. We ran our simulations on Princetons SMILE server and TIGER computing cluster. The SMILE server was equipped with a single Tesla K40c GPU, while the TIGER cluster has 200 K20 GPUs. We use early stopping and dropout as regularization techniques (Srivastava et al., 2014; Hardt et al., 2015). The error rate reported for each experiment is the lowest testing error seen during training. A common deep network architecture consists 3 stacked convolutional layers with 2 fully connected layers on top (Srivastava et al., 2014; Agostinelli et al., 2015). The convolutional layers have 96, 128, and 256 filters each and each layer has a 5 5 filter size that is applied with stride 1 in both directions. The max pooling layers pool 3 3 fields and are applied with a stride of 2 in both directions. We use this architecture for the more challenging CIFAR-10 dataset.",
      "exclude": false
    },
    {
      "heading": "5.2 MNIST",
      "text": "This dataset consists of 60,000 training and 10,000 testing images. Images are 28 28 rasterized grayscale images that is they have one channel. All data points are normalized before training by taking the entire combined dataset, finding the maximum and minimum entry, then centering and rescaling. The dataset is from Lecun & Cortes (1999). The experiments investigate the effects of our techniques on standard multilayer neural networks as well as convolutional neural networks. Specifically, for the standard neural networks we use networks with three fully connected hidden layers with 2048 units in each layer fed into a 10-way softmax. The convolutional neural networks in these experiments have two convolutional layers with 32 and 64 filters respectively. Filters are 5 5 and these layers are followed by a max pooling layer with 22 pool size. The feed-forward networks consist of 3 fully connected layers with 2048 units each. A mini-batch size of 250 was used for all experiments. Table 1 contains the results from the MNIST baseline experiments. For the experiments with NPF(L, T ) activations, we use the same fully connected and convolutional architectures as in the MNIST baseline experiments. Table 2 contains the results of the experiments. Learning activation functions via Fourier basis expansion offers sizable improvements on the MNIST dataset as can be seen in Table 2. No experiments were done using our two-stage tech- nique, as it proved unnecessary on this comparatively easy to learn dataset. Notice that the weight tying in the NPFC activation makes a considerable difference for the convolution nets, especially when dropout is used in conjunction with learning the activation functions. The biggest improvement is in learning activations for both convolution and fully connected layers and using dropout a relative 15% improvement over the baseline. Using ReLU activations in the convolution layers with nonparametric activations in the fully connected layers also offered sizable performance improvements over the baseline. In feedforward networks with three fully connected layers there also is an improvement in the test error rate. Here again the improvement is more pronounced with dropout, from 1.35% to 1.10% versus 1.66% to 1.60%.",
      "exclude": false
    },
    {
      "heading": "5.3 CIFAR-10",
      "text": "The CIFAR-10 dataset is due to Krizhevsky (2009). This dataset consists of 50,000 training and 10,000 test images. The images are three channel color images with dimension 32 32. Thus each image can be viewed either as a 3 32 32 tensor or a vector of length 3072. These images belong to one of ten classes. We apply ZCA whitening and global contrast normalization to the dataset as in Srivastava et al. (2014). The architecture described at the beginning of Section 5 is used for all experiments. For dropout nets we follow Srivastava et al. (2014) and use a dropout of 0.9 on the input, 0.75 in the convolution layers and 0.5 in the fully connected layers. The baseline results can be found in Table 3. Mini-batch sizes between 125 and 250 are used, depending upon memory constraints. In Table 4, one-stage training corresponds to standard back-propagation as in the baseline experiments. The two-stage procedure is the one we introduce to train convolution nets with learned activation functions described in Section 3.2. As can be seen in Table 4, one stage training does not work when learning nonparametric activations in convolution layers test error is worse than the baseline result. However, by using the two-stage process we see a relative 5% boost in performance without dropout and a relative 9% boost with dropout! In absolute terms we achieve up to a 1.3% improvement in generalization performance using learned activations. Also in Table 4, we can see that we achieved improved performance both with and without dropout when using ReLU activations for convolution layers and NPF(4, 1) for fully connected layers. Furthermore, Figure 2b shows that in terms of number of weight updates required, adding nonparametric activations did not slow training.",
      "exclude": false
    },
    {
      "heading": "6 DISCUSSION AND CONCLUSIONS",
      "text": "As can be seen in Section 5, nonparametric activation functions offer a meaningful improvement in the generalization performance of deep neural nets in practice. We achieve relative improvements in performance of up to 15% and absolute improvements of up to 1.3% on two benchmark datasets. Equally importantly, networks with the activation functions NPF(L, T ) and NPFC(L, T ) can be trained as fast as their baseline comparison. To realize these improvements on more challenging datasets such as CIFAR-10 we introduced a two-stage training procedure. Our nonparametric activation functions are principled in that they have the necessary properties to guarantee vanishing generalization error in the sense of Bousquet & Elisseeff (2002). Specifically, it was shown in Theorem 4.8, from Section 4.1, that for any feed-forward network architecture using NPF(L, T ) activation functions, we achieve vanishing generalization error as sample size increases. One interesting direction for future work is to investigate why Fourier basis expansion is successful where other methods, such as polynomial basis expansion (which we also explored), were not. Both can be theoretically justified, yet only one works well in practice. Further study of how to eliminate the two-stage process is needed. Ultimately, we improve upon other approaches of expanding the function class which can be learned by deep neural networks. An important previous work on learning more general function approximations at each node is that of Lin et al. (2013). Their result offers a new type of convolution filter that can learn a broader function class, and in practice their technique also works well. It is limited however in that its use is restricted to convolution layers in convolution neural networks. Ours is applicable to any network architecture and, in Section 5, we demonstrate its success on multiple datasets.",
      "exclude": true
    },
    {
      "heading": "A GENERALIZATION BOUND FOR FULLY CONNECTED NETWORKS WITH FOURIER BASIS ACTIVATION FUNCTIONS",
      "text": "This is an extension of Section 4.1. For the purposes of this section assume that all data x are vectors in [1, 1]m, which is permissible because we classify input that is bounded. Furthermore, we assume that the network architecture is a series of Q fully connected hidden layers (affine transformation layers) fed into a softmax layer which classifies into one of c classes. The weight vector for the network w should be viewed as the all parameters of the affine transformations and activation functions. The loss function is the negative log-likelihood of the network output, which is a softmax over c classes. The softmax layer in our framework should be treated as two layers for the purposes of demonstrating the generalization bound. One layer is an affine transformation of the output of the lower layers in the network to c nodes. We will denote the result of this computation at each node i in the affine transformation layer within the softmax layer as gQ+1(w, x)i. The second layer maps the outputs at these c nodes to c nodes this will be the network output. It computes at the the ith node oi(w, x) = exp gQ+1(w, x)ic i=1 exp gQ+1(w, x)i . Thus, on some training example (x, y), the negative log-likelihood of the network output is f(w, (x, y)) = log(oi(w, x)) = gQ+1(w, x)y + log ( c i=1 exp gQ+1(w, x)c ) . If we can demonstrate that this f has the desired properties, then the generalization bound given in Section 4 will apply. For clarity, we refer to network of stacked affine transformation layers as a fully connected network. These layers compute Wx + b, where x is either the input layer (so x Rm) or it is the output of the previous layer in the network (in which case x Rp, p being the number of nodes in that layer). Regarding notation: w is a vector denoting all the parameters in the network, so it consists of all parameters noted below. Wi denotes a portion of the parameter vector, corresponding to the affine transformation of the pi1-dimensional output of the (i 1)th layer to the pi-dimensional input to the ith layer in the network. Wi,j denotes the jth row of the matrix Wi. Wi,j,k refers to the weight of the edge from node j in the (i 1)th layer to edge c in the ith layer. bi denotes the constant term in the affine transformation from the (i 1)th layer to the ith layer. bi,j denotes the jth component of the vector bi. ai,j0 , (a i,j 1 , . . . , a i,j k ) and (b i,j 1 , . . . , b i,j k ) denote the parameters of activation function of the jth node in the ith layer. The 0th layer is the input layer. Let Q be the number of layers in the network; if i = Q+ 1, it refers to weights on edges in the affine transformation contained within the softmax layer. Intuitively, our choice of network topology and use of NPF(L, T ) activation function should have the necessary properties to apply Theorem 4.5. The definition of NPF(L, T ) is repeated below for clarity. Definition A.1. Given some sample size n, an interval [L,L] and tail truncation T where 0 T L, the activation function NPF(L, T ) is parametrized by (a0, . . . , ak) and (b1, . . . , bk) given by f(x) = a0 + k i=1 ai cos((L+ T )i/L) + bi sin((L+ T )i/L) x L T k grows with sample size n and is given by k = dn1/7e. We need to control the norm of the weight matrices W and biases b in the affine transformations. In addition, we also need to control the magnitude of the coefficients in the basis expansion. This does not affect the applicability of the results from Hardt et al. (2015), as it at most reduces the expansivity of the updates. Furthermore, for the analysis of their properties we assume a slightly modified version of the activation functions in an epsilon neighborhood about the points where the pieces of the function connect we smooth their joint by taking a convex combination of the two pieces to smoothly transition from one to the other. To this end define the following functions L(x) = a0 + k i=1 ai cos((L+ T )i/L) + bi sin((L+ T )i/L), M(x) = a0 + k i=1 ai cos(ix/L) + bi sin(ix/L), U(x) = a0 + k i=1 ai cos((L T )i/L) + bi sin((L T )i/L). for given L, T, k, (a0, . . . , ak), and (b1, . . . , bk). Then the -neighborhood smoothed nonparametric activation functions are given by Definition A.2. Definition A.2. Given some sample size n, an interval [L,L] and tail truncation T where 0 T L, the activation function NPF (L, T ) is parametrized by (a0, . . . , ak) and (b1, . . . , bk) given by f(x) = L(x) x L T + [Case 5] k grows with sample size n and is given by k = dn1/7e. Since we are concerned with behavior as is made small because we are approximating NPF(T, L) with NPF (T, L) assume without loss of generality that 1. A.1 LIPSCHITZ CONSTANT We proceed layer by layer to obtain bounds on the first partial derivatives of the loss function which will allow us to bound the gradient. Lemma A.3 gives bounds for the first layer. The notation a(w, x) denotes the output of a layer-wide activation transformation. g(w, x) denotes the output of a layer-wide affine transformation. Lemma A.3. Assume x [a, a]m. Now, consider the affine transformation g1(w) = W1x + b1 from Rm Rp, thus W1 Rmp and b Rp. Denoting by g1(w, x)i = W1,ix + b1,i, it follows that g1 has first partial derivatives bounded by max1, a. Proof. Clearly,g1(w, x)i = [1, . . . , 1;x; 0, . . . , 0]. Thus it has first partial derivatives bounded by max1, a. What if we have instead as input to the affine transformation a function of x rather than x itself? Then we have the Lemma A.4. Lemma A.4. Let ai1(w, x) [a, a]pi1 , the output of the (i 1)th layer in the network. Now, consider the affine transformation gi(w, x) = Wiai1(w, x) + bi. from Rpi1 Rpi . bi Rd. If we have the following assumptions: ai(w, x)q has first partial derivatives bounded by B, and ||Wi,j ||2 N . Denoting by gi(w, x)j = Wi,jai1(w, x) + bi,j , it follows that gi(w, x)j has first partial derivatives bounded by max1, a, nNL. Proof. We need to first examine gi(w, x)j bi,j = bi,j bi,j + bi,j ( n q=1 Wi,j,qai1(w, x)q ) = 1 + 0 = 1. Next we have for any m, gi(w, x)j Wi,j,m = bi,j Wi,j,m + Wi,j,m ( n q=1 Wi,j,qai1(w, x)q ) = 0 + ai1(w, x)m + q 6=m Wi,j,q ai1(w, x)q Wi,j,m = ai1(w, x)m. Partial derivatives with respect to all other parameters from the ith layer are clearly 0. If l > i, partial derivatives with respect to all parameters from the lth layer are clearly 0. If l 1, and then the maximum is taken over both cases. By a simple inductive argument for i 2, gi B = max O ( p2i3/2k6i8 L2i2 2i3 ) ,O ( p2 i11/2k52 i1 L2 i2 2i1 ) ai B = max O ( p2i2k6i2 L2i 2i1 ) ,O ( p2 i1k52 i L2 i1 2i ) Using the bounds derived above, the bounds from Theorem A.9 and Lemma A.10 gives = max O ( Q p3Q+5/2k9Q2 L2Q 3Q1 ) ,O ( Q p2 Q+Q+2k52 Q+3QL2 Q1+1 2Q+Q ) . A.3 GENERALIZABILITY With the necessary properties demonstrated, we arrive at Theorem A.16. By p denote the number of nodes per layer and Q the number of hidden layers. Theorem A.16. Consider a fully connected network with a softmax output layer and NPF (L, T ) activation functions. Assume we use SGD with a max-norm constraint on the weight matrix and biases in the affine layers and on the coefficients of the activation functions to train the network. Then, for all practical values of L,T , ,Q and p, the resulting Lipschitz constant K and smoothness constant are sufficiently large that stab T n 1 . Thus if T = O( n), stab 0 as sample size n. Proof. By Theorems 4.5, A.9, and A.15 the result is immediate. B RESULTS FOR FUNCTIONS OVER Rn Without proof we give the mean value inequality for vector valued function in Theorem B.1. Theorem B.1. Let f be a differentiable vector valued function, that is f : A Rn Rm. Then for x, y A, we have that ||f(x) f(y)|| ||Df(z)(x y)||, where z is some point on the line segment connecting x, y. HereDf represents the Frechet derivative of f , which can be represented in matrix form by the Jacobian.",
      "exclude": false
    },
    {
      "heading": "C RESULTS FOR MATRICES",
      "text": "When we want to bound the largest eigenvalue of the matrix, we can get a loose bound by the sum of the entries of the matrix as in Theorems C.1 and C.2. Theorem C.1. For some matrix norm || ||, the spectral radius of the matrix A is upper bounded by ||A||. Proof. Take the eigenvector corresponding to the largest eigenvalue in magnitude of A. Denote this pair by x, . Next define the matrix X = [x| |x]. It follows then that AX = X . Therefore ||||X|| = ||X|| = ||AX|| ||A||||X|| because matrix norms are sub-multiplicative, demonstrating || ||A||. Theorem C.2. Let A be a symmetric n n matrix. Let the entries of the matrix be bounded above by some constant k. It follows then that maxk |k(A)| nk. Proof. By Theorem C.1 we can see that max k |k(A)| i,j (Aij)2 n2k2 = nk.",
      "exclude": false
    },
    {
      "heading": "D RESULTS FOR LIPSCHITZ AND SMOOTH FUNCTIONS",
      "text": "First note that if f(x) is well defined everywhere and if ||f(x)|| L over the entire domain, it is L-Lipschitz by the mean value theorem and an application of Cauchy-Schwartz. In the convex case, the reverse is also true. Lemma D.1. Take g to be a function from Rn Rd. If g is L-Lipschitz in each of its components, g is nL-Lipschitz. Proof. Consider any x, y Rn. By the assumption we know that for i = 1, . . . , n |gi(x)gi(y)|2 L2||x y||22. Therefore n i=1 |gi(x) gi(y)|2 nL2||x y||22, and, thus, the desired result that ||g(x) g(y)||2 = n i=1 |gi(x) gi(y)|2 nL||x y||2. Directly from the definition it is difficult to prove a gradient to be Lipschitz continuous. However we can utilize Lemma D.2. Lemma D.2. For some twice differentiable function f : A Rn R, f is -smooth if its Hessian is bounded by with respect to the euclidean norm. Proof. Take the gradient f which is a mapping A Rn Rn. We can apply the mean value inequality for vector valued functions, Theorem B.1, which gives us ||f(x)f(y)||2 ||2f(z)(x y)||2, where z is on the line segment connecting x and y. Because of how the induced euclidean norm on a matrix is defined it follows that ||2f(z)(x y)||2 ||2f(z)||2||x y||2 ||x y||2. Combining the two equations above gives the desired result, ||f(x)f(y)||2 ||x y||2. The insight to take this approach to demonstrating -smoothness is from Bach & Moulines (2011).",
      "exclude": false
    }
  ],
  "fewShot": [
    {
      "model": "claude-haiku-4-5",
      "decision": {
        "rejection": false,
        "confidence": 0.72,
        "primary_reason": "Novel nonparametric activation function learning via Fourier basis with theoretical generalization bounds and consistent empirical improvements across datasets. Solid contribution despite modest experimental gains."
      },
      "token": {
        "prompt_tokens": 8524,
        "completion_tokens": 70,
        "total_tokens": 8594
      },
      "time": "2026-02-08T22:24:15.390003+00:00"
    }
  ]
}