{
  "id": "474",
  "title": "Towards Deep Interpretability (MUS-ROVER II): Learning Hierarchical Representations of Tonal Music",
  "abstract": "Music theory studies the regularity of patterns in music to capture concepts underlying music styles and composers' decisions. This paper continues the study of building \\emph{automatic theorists} (rovers) to learn and represent music concepts that lead to human interpretable knowledge and further lead to materials for educating people. Our previous work took a first step in algorithmic concept learning of tonal music, studying high-level representations (concepts) of symbolic music (scores) and extracting interpretable rules for composition. This paper further studies the representation \\emph{hierarchy} through the learning process, and supports \\emph{adaptive} 2D memory selection in the resulting language model. This leads to a deeper-level interpretability that expands from individual rules to a dynamic system of rules, making the entire rule learning process more cognitive. The outcome is a new rover, MUS-ROVER \\RN{2}, trained on Bach's chorales, which outputs customizable syllabi for learning compositional rules. We demonstrate comparable results to our music pedagogy, while also presenting the differences and variations. In addition, we point out the rover's potential usages in style recognition and synthesis, as well as applications beyond music.",
  "accepted": true,
  "zeroShot": [
    {
      "model": "claude-haiku-4-5",
      "decision": {
        "rejection": true,
        "confidence": 0.85,
        "primary_reason": "Limited novelty and experimental validation. While the hierarchical representation framework is interesting, the paper lacks rigorous quantitative evaluation, comparative baselines, and statistical significance testing. The work is primarily a music education tool rather than a machine learning contribution suitable for top-tier ML conferences. The experimental section relies heavily on qualitative examples and manual inspection rather than systematic evaluation metrics. No comparison with other interpretability methods or music analysis approaches. The dataset is small (370 Bach chorales) and domain-specific, limiting generalizability claims."
      },
      "token": {
        "prompt_tokens": 6589,
        "completion_tokens": 136,
        "total_tokens": 6725
      },
      "time": "2026-02-05T22:51:06.573946+00:00"
    },
    {
      "model": "gpt-5.2",
      "decision": {
        "rejection": true,
        "confidence": 0.78,
        "primary_reason": "Insufficient empirical validation and unclear methodological rigor for the proposed hierarchical/adaptive rule-learning (limited dataset, mostly qualitative results, missing baselines/ablation and quantitative evaluation of interpretability/generalization)."
      },
      "token": {
        "prompt_tokens": 5993,
        "completion_tokens": 58,
        "total_tokens": 6051
      },
      "time": "2026-02-09T21:21:56.592601+00:00"
    }
  ],
  "article": [
    {
      "heading": "1 INTRODUCTION",
      "text": "Forming hierarchical concepts from low-level observations is key to knowledge discovery. In the field of artificial neural networks, deep architectures are employed for machine learning tasks, with the awareness that hierarchical representations are important (Bengio et al., 2013). Rapid progress in deep learning has shown that mapping and representing topical domains through increasingly abstract layers of feature representation is extremely effective. Unfortunately, this layered representation is difficult to interpret or use for teaching people. Consequently, deep learning models are widely used as algorithmic task performers (e.g. AlphaGo), but few act as theorists or pedagogues. In contrast, our goal is to achieve a deeper-level interpretability that explains not just what has been learned (the end results), but also what is being learned at every single stage (the process). On the other hand, music theory studies underlying patterns beneath the music surface. It objectively reveals higher-level invariances that are hidden from the low-level variations. In practice, the development of music theory is an empirical process. Through manual inspection of large corpora of music works, theorists have summarized compositional rules and guidelines (e.g. J. J. Fux, author of Gradus ad Parnassum, the most influential book on Renaissance polyphony), and have devised multi-level analytical methods (e.g. H. Schenker, inventor of Schenkerian analysis) to emphasize the hierarchical structure of music, both of which have become the standard materials taught in todays music theory classes. The objective and empirical nature of music theory suggests the possibility of an automatic theorist statistical techniques that perform hierarchical concept learning while its pedagogical purpose requires human interpretability throughout the entire learning process. The book title Gradus ad Parnassum, means the path towards Mount Parnassus, the home of poetry, music, and learning. This paper presents MUS-ROVER II, an extension of our prior work (Yu et al., 2016a;b), to independently retake the path towards Parnassus. The rover acts more as a pathfinder than a generative model (e.g. LSTM), emphasizing the path more than the destination. We compare the paths taken by this improved automatic theorist to paths taken by human theorists (say Fux), studying similarities as well as pros and cons of each. So advantages from both can be jointly taken to maximize the utility in music education and research. In this paper in particular, we highlight the concept hierarchy that one would not get from our prior work, as well as enhanced syllabus personalization that one would not typically get from traditional pedagogy.",
      "exclude": true
    },
    {
      "heading": "2 MUS-ROVER OVERVIEW",
      "text": "As the first algorithmic pathfinder in music, MUS-ROVER I introduced a teacher student model to extract compositional rules for writing 4-part chorales (Yu et al., 2016a;b). The model is implemented by a self-learning loop between a generative component (student) and a discriminative component (teacher), where both entities cooperate to iterate through the rule-learning process (Figure 1). The student starts as a tabula rasa that picks pitches uniformly at random to form sonorities (a generic term for chord) and sonority progressions. The teacher compares the students writing style (represented by a probabilistic model) with the input style (represented by empirical statistics), identifying one feature per iteration that best reveals the gap between the two styles, and making it a rule for the student to update its probabilistic model. As a result, the student becomes less and less random by obeying more and more rules, and thus, approaches the input style. Collecting from its rule-learning traces, MUS-ROVER I successfully recovered many known rules, such as Parallel perfect octaves/fifths are rare and Tritons are often resolved either inwardly or outwardly. What is Inherited from MUS-ROVER I MUS-ROVER II targets the same goal of learning interpretable music concepts. It inherits the self-learning loop, as well as the following design choices. (Dataset and Data Representation) We use the same dataset that comprises 370 C scores of Bachs 4-part chorales. We include only pitches and their durations in a pieces raw representation, notated as a MIDI matrix whose elements are MIDI numbers for pitches. The matrix preserves the twodimensional chorale texture, with rows corresponding to melodies, and columns to harmonies. (Rule Representation) We use the same representation for high-level concepts in terms of rules, unrelated to rules in propositional logic. A (compositional) rule is represented by a feature and its distribution: r = (, p), which describes likelihoods of feature values. It can also be transformed to a linear equality constraint (Apstu = p) in the students optimization problem (s in Figure 1). (Students Probabilistic Model) We still use n-gram models to represent the students style/belief, with words being sonority features, and keep the students optimization problem as it was. To reiterate the distinctions to many music n-grams, we never run n-grams in the raw feature space, but only collectively in the high-level feature spaces to prevent overfitting. So, rules are expressed as probabilistic laws that describe either (vertical) sonority features or their (horizontal) progressions. What is New in MUS-ROVER II We study hierarchies on features, so rules are later presented not just as a linear list, but as hierarchical families and sub-families. In particular, we introduce conceptual hierarchy that is pre-determined by feature maps, and infer informational hierarchy that is post-implied from an information-theoretic perspective. We upgrade the self-learning loop to adaptively select memories in a multi-feature multi-n-gram language model. This is realized by constructing hierarchical filters to filter out conceptual duplicates and informational implications. By further following the information scent spilled by Bayesian surprise (Varshney, 2013), the rover can effectively localize the desired features in the feature universe.",
      "exclude": false
    },
    {
      "heading": "3 RELATED WORK",
      "text": "Adversarial or Collaborative MUS-ROVERs self-learning loop between the teacher (a discriminator) and student (a generator) shares great structural similarity to generative adversarial nets (Goodfellow et al., 2014) and their derivatives (Denton et al., 2015; Makhzani et al., 2015). However, the working mode between the discriminator and generator is different. In current GAN algorithms, the adversarial components are black-boxes to each other, since both are different neural networks that are coupled only end to end. The learned intermediate representation from one model, no matter how expressive or interpretable, is not directly shared with the other. Contrarily in MUSROVER, both models are transparent to each other (also to us): the student directly leverages the rules from the teacher to update its probabilistic model. In this sense, the learning pair in MUSROVER is more collaborative rather than adversarial. Consequently, not only the learned concepts have interpretations individually, but the entire learning trace is an interpretable, cognitive process. Furthermore, MUS-ROVER and GAN contrast in the goal of learning and the resulting evaluations. The rover is neither a classifier nor a density estimator, but rather a pure representation learner that outputs high-level concepts and their hierarchies. Training this type of learner in general is challenging due to the lack of a clear objective or target (Bengio et al., 2013), which drives people to consider some end task like classification and use performance on the task to indirectly assess the learned representations. In MUS-ROVER, we introduce information-theoretic criteria to guide the training of the automatic theorist, and in the context of music concept learning, we directly evaluate machine generated rules and hierarchies by comparison to those in existing music theory. Interpretable Feature Learning In the neural network community, much has been done to first recover disentangled representations, and then post-hoc interpret the semantics of the learned features. This line of work includes denoising autoencoders (Vincent et al., 2008) and restricted Boltzmann machines (Hinton et al., 2006; Desjardins et al., 2012), ladder network algorithms (Rasmus et al., 2015), as well as more recent GAN models (Radford et al., 2015). In particular, InfoGAN also introduces information-theoretic criteria to augment the standard GAN cost function, and to some extent achieves interpretability for both discrete and continuous latent factors (Chen et al., 2016). However, beyond the end results, the overall learning process of these neural networks are still far away from human-level concept learning (Lake et al., 2015), so not directly instructional to people. Automatic Musicians Music theory and composition form a reciprocal pair, often realized as the complementary cycle of reduction and elaboration (Laitz, 2016) as walks up and down the multilevel music hierarchy. Accordingly, various models have been introduced to automate this up/down walk, including music generation (Cope & Mayer, 1996; Biles, 1994; Simon et al., 2008), analysis (Taube, 1999), or theory evaluation (Rohrmeier & Cross, 2008). In terms of methodologies, we have rule-based systems (Cope, 1987), language models (Google Brain, 2016; Simon et al., 2008), and information-theoretic approaches (Jacoby et al., 2015; Dubnov & Assayag, 2002). However, all of these models leverage domain knowledge (e.g. human-defined chord types, functions, rules) as part of the model inputs. MUS-ROVER takes as input only the raw notations (pitches and durations), and outputs concepts that are comparable to (but also different from) our domain knowledge.",
      "exclude": true
    },
    {
      "heading": "4 HIERARCHICAL RULE LEARNING",
      "text": "MUS-ROVER II emphasizes hierarchy induction in learning music representations, and divides the induction process into two stages. In the first stage, we impose conceptual hierarchy as pre-defined structures among candidate features before the self-learning loop. In the second stage, we infer informational hierarchy as post-implied structures through the rule learning loops. Interpretable Features A feature is a function that computes a distributed representation of the building blocks that constitute data samples. For Bachs 4-part chorales, we model every piece (4-row matrix) as a sequence of sonorities (columns). So every sonority is the building block of its composing piece (like a word in a sentence). Then a feature maps a sonority onto some feature space, summarizing an attribute. To formalize, let = R, p1, . . . , pn be an alphabet that comprises a rest symbol R, and n pitch symbols pi. In addition, the alphabet symbols analogous to image pixels are manipulable by arithmetic operations, such as plus/minus, modulo, and sort. More precisely, every pi is an integer-valued MIDI number (60 for middle C, granularity 1 for semi-tone), and R is a special character which behaves like a python nan variable. The four coordinates of every sonority p 4 denote soprano, alto, tenor, and bass, respectively. We define a feature as a surjective function : 4 7 (4), and the corresponding feature space by its range. As a first and brutal categorization, we say a feature (space) is raw (or lowest-level) if |(4)| = |4|, and high-level if |(4)| 0, if gap ( p k ,stu p ) := D ( p k ,stu p ) < , and gap ( p k ,stu p ) ,k < k, where D() is the KL divergence used to characterize the gap of the students style (probabilistic model) against Bachs style (input). One trivial case happens when is extracted as the kth rule, i.e. = k, then gap(p k ,stu p) = 0 < , | P P, meaning that feature , once learned as a rule, informationally implies itself and all its descendants in the conceptual hierarchy. However, what is more interesting is the informational implication from other rules outside the conceptual hierarchy, which is typically hard for humans to eyeball. One might question the necessity of conceptual hierarchy since it can be implied in the informational hierarchy. The answer is yes in principle, but no in practice. The main difference is that conceptual hierarchy is pre-computed over the entire feature universe before the loop, which is global, precise, and trace independent. On the contrary, informational hierarchy is trace specific and loose, due to tolerance and the precision of the optimization solver. As a result, informational hierarchy alone tends to lose the big picture and require more post-hoc interpretations, and is unstable in practice. Hierarchical Filters Beyond their benefits in revealing inter-relational insights among distributed representations, we build hierarchical filters from both conceptual and informational hierarchies, for the purpose of pruning hierarchically entangled features and speeding up feature selection. This upgrades MUS-ROVER II into a more efficient, robust, and cognitive theorist. Recall the skeleton of the teachers optimization problem in Figure 1, we flesh it out as follows: maximize gap ( p k1 ,stu p ) (3) subject to H(p) (Regularity Condition) / Ck1 := P P , k1 (Conceptual-Hierarchy Filter) / Ik1 := gap ( p k1 ,stu p ) < (Informational-Hierarchy Filter) In the above optimization problem, is the feature universe defined in (1) and is the optimization variable whose optimal value is used to form the kth rule: k = ?, rk = (?, p?). We decouple the regularity condition from the objective function in our previous work (which was the generalized cultural hole function), and state it separately as the first constraint that requires the Shannon entropy of the feature distribution to be no larger than a given threshold (Pape et al., 2015). The second constraint encodes the filter from conceptual hierarchy, which prunes coarser partitions of the learned features k1 := 1, . . . , k1. The third constraint encodes the filter from informational hierarchy, which prunes informationally implied features. There are two hyper-parameters and in the optimization problem (3), whose detailed usage in syllabus customization will be discussed later in Sec. 6. At a high level, we often pre-select before the loop to express a users satisfaction level: a smaller signifies a meticulous user who is harder to satisfy; the threshold upper bounds the entropic difficulty of the rules, and is adaptively adjusted through the loop: it starts from a small value (easy rules first), and auto-increases whenever the feasible set of (3) is empty (gradually increases the difficulty when mastering the current level).",
      "exclude": false
    },
    {
      "heading": "5 ADAPTIVE MEMORY SELECTION",
      "text": "MUS-ROVER II considers a continuous range of higher order n-grams (variable memory), and adaptively picks the optimal n based on a balance among multiple criteria. The fact that every ngram is also on multiple high-level feature spaces opens the opportunities for long-term memories without exhausting machine memory, while effectively avoiding overfitting. Two-Dimensional Memory In light of a continuous range of n-grams, say n N = 2, 3, . . . , the feature universe adds another dimension, forming a two-dimensional memory (N) length versus depth for the language model (Figure 2: left). The length axis enumerates n-gram orders, with a longer memory corresponding to a larger n; the depth axis enumerates features, with a deeper memory corresponding to a higher-level feature. Every cell in the memory is indexed by two coordinates (n, ), referring to the feature under the n-gram, and stores the corresponding feature distribution. As a consequence, the rule extraction task involves picking the right feature under the right n-gram, which extends the space of the optimization problem (3) from to N . Accordingly, the constraints of (3) jointly forge a mask on top of the 2D memory (Figure 2: right). Criteria and Balance We propose three criteria to extract rules from the 2D memory: confidence, regularity, and efficacy. Confidence is quantified by empirical counts: the more relevant examples one sees in Bachs chorales, the more confident. Regularity is quantified by Shannon entropy of the rules feature distribution: a rule is easier to memorize if it is less entropic (Pape et al., 2015). Efficacy is inversely quantified by the gap between the students probabilistic model and the rules feature distribution: a rule is more effective if it reveals a larger gap. There are tradeoffs among these criteria. For instance, a lower-level feature is usually more effective since it normally reflects larger variations in the gap, but is also unlikely to be regular, thus harder to memorize and generalize. Also a feature under a higher-order n-gram may be both regular and effective, but the number of examples that match the long-term conditionals is likely to be small, reducing confidence. Adaptive Selection: Follow the (Bayesian) Surprise The teachers optimization problem (3) explicitly expresses the efficacy factor in the objective, and the regularity condition as the first constraint. To further incorporate confidence, we cast the rules feature distribution p in a Bayesian framework rather than a purely empirical framework as in our previous work. We assume the students belief with respect to a feature follows a Dirichlet distribution whose expectation is the students probabilistic model. In the kth iteration of the self-learning loop, we set the students prior belief as the Dirichlet distribution parameterized by the students latest probabilistic model: prior,stu Dir ( c pk1,stu ) , where c > 0 denotes the strength of the prior. From Bachs chorales, the teacher inspects the empirical counts q associated with the feature and the relevant n-gram, and computes the students posterior belief if were selected as the rule: posterior,stu Dir ( q + c pk1,stu ) . The concentration parameters of the Dirichlet posterior show the balance between empirical counts and the prior. If the total number of empirical counts is small (less confident), the posterior will be smoothed more by the prior, de-emphasizing the empirical distribution from q. If we compute p ( q + c pk1,stu ) in the objective of (3), then gap ( p k1 ,stu p ) = D ( E [ prior,stu ] E [ posterior,stu ]) . (4) The right side of (4) is closely related to Bayesian surprise (Varshney, 2013), which takes the form of KL divergence from the prior to posterior. If we remove the expectations and switch the roles between the prior and posterior, we get the exact formula for Bayesian surprise. Both functionals capture the idea of comparing the gap between the prior and posterior. Therefore, the efficacy of concept learning is analogous to seeking (informational) surprise in the learning process. The subtlety in (4) where we exchange the prior and posterior, makes a distinction from Bayesian surprise due to the asymmetry of KL divergence. As a brief explanation, adopting (4) as the objective tends to produce rules about what Bach hated to do, while the other way produces what Bach liked to do. So we treat it as a design choice and adopt (4), given that rules are often taught as prohibitions (e.g. parallel fifths/octaves are bad, never double the tendency tones). There are more in-depth and information-theoretic discussions on this point (Huszar, 2015; Palomar & Verdu, 2008).",
      "exclude": false
    },
    {
      "heading": "6 EXPERIMENTS",
      "text": "MUS-ROVER IIs main use case is to produce personalized syllabi that are roadmaps to learning the input style (customized paths to Mount Parnassus). By substituting the student module, users can join the learning cycle, in which they make hands-on compositions and get iterative feedback from the teacher. Alternatively, for faster experimentation, users make the student their learning puppet, which is personalized by its external parameters. This paper discusses the latter case in detail. Math-to-Music Dictionary MUS-ROVER II conceptualizes every rule feature as a partition of the raw space, and uses the inducing function as its mathematical name. To get the meanings of the features, one can simply work out the math, but some of them already have their counterparts as music terminologies. We include a short dictionary of those correspondences in Appendix A.1. Pace Control and Syllabus Customization We present a simple yet flexible pace control panel to the users of MUS-ROVER II, enabling personalized set-up of their learning puppet. The control panel exposes four knobs: the lower bound, upper bound, and stride of the rules entropic difficulty (min, max, stride), as well as the satisfactory gap (). These four hyper-parameters together allow the user to personalize the pace and capacity of her learning experience. The entropic difficulty caps the Shannon entropy of a rules feature distribution in (3), a surrogate for the complexity (or memorability) of the rule (Pape et al., 2015). It is discretized into a progression staircase from min up to max, with incremental stride. The resulting syllabus starts with = min, the entry level difficulty; and ends whenever max, the maximum difficulty that the user can handle. Anywhere in between, the loop deactivates all rules whose difficulties are beyond current , and moves onto the next difficulty level + stride if the students probabilistic model is -close to the input under all currently active rule features. To showcase syllabus customization, we introduce an ambitious user who demands a faster pace and a patient user who prefers a slower one. In practice, one can collectively tune the stride parameter stride and the gap parameter , with a faster pace corresponding to a larger stride (lets jump directly to the junior year from freshman) and a larger (having an A- is good enough to move onto the next level, why bother having A+). Here we simply fix stride, and let control the pace. We illustrate two syllabi in Table 1, which compares the first ten (1-gram) rules in a faster ( = 0.5) syllabus and a slower one ( = 0.1). Notice the faster syllabus gives the fundamentals that a music student will typically learn in her first-year music theory class, including rules on voice crossing, pitch class set (scale), intervals, and so on (triads and seventh chords will appear later). It effectively skips the nitty-gritty rules (marked by an asterisk) that are learned in the slower setting. Most of these skipped rules do not have direct counterparts in music theory (such as taking the diff operator twice) and are not important, although occasionally the faster syllabus will skip some rules worth mentioning (such as the second rule in the slower pace, which talks about spacing among soprano, alto, and bass). Setting an appropriate pace for a user is important: a pace that is too fast will miss the whole point of knowledge discovery (jump to the low-level details too fast); a pace that is too slow will bury the important points among unimportant ones (hence, lose the big picture). Fundamentals: Hierarchical 1-gram Similar to our teaching of music theory, MUS-ROVER IIs proposed syllabus divides into two stages: fundamentals and part writing. The former is under the 1-gram setting, involving knowledge independent of the context; the latter provides online tutoring under multi-n-grams. We begin our experiments with fundamentals, and use them to illustrate the two types of feature hierarchies. Lets take a closer look at the two syllabi in Table 1. The specifications (left) and hierarchies (right) of the four common rules are illustrated in Table 2. The rules translations are below the corresponding bar charts, all of which are consistent with our music theory. Extracted from the conceptual hierarchy, the right column lists the partition sub-family sourced at each rule, which is pictorially simplified as a tree by hiding implied edges from its corresponding DAG. Every coarser partition in a sub-family is indeed a higher-level representation, but has not accumulated sufficient significance to make itself a rule. A partition will never be learned if one of its finer ancestors has been made a rule. Observe that all of the coarser partitions are not typically taught in theory classes. MUS-ROVER II measures the students progress from many different angles in terms of features. With respect to a feature, the gap between the student and Bach is iteratively recorded to form a trajectory when cycling the loop. Studying the vanishing point of the trajectory reveals the (local) informational hierarchy around the corresponding feature. Taking the second and seventh rule in the slower syllabus for example, we plot their trajectories in Figure 3. Both illustrate a decreasing trend1 for gaps in the corresponding feature spaces. The left figure shows that the second rule is largely but not entirely implied by the first, pointing out the hierarchical structure between the two: the first rule may be considered as the dominant ancestor of the second, which is not conceptually apparent, but informationally implied. On the contrary, the right figure shows that the seventh rule is not predominantly implied by the first, which instead is informationally connected to many other rules. However, one could say that it is probably safe to skip both rules in light of a faster pace, since they will eventually be learned fairly effectively (with small gaps) but indirectly. Part Writing: Adaptive n-grams Unlike fundamentals which studies sonority independently along the vertical direction of the chorale texture, rules on part writing (e.g. melodic motion, chord progression) are horizontal, and context-dependent. This naturally results in an online learning framework, in which rule extractions are coupled in the writing process, specific to the realization of a composition (context). Context dependence is captured by the multi-n-gram language model, which further leads to the 2D memory pool of features for rule extraction (Sec. 5). Consider an example of online learning and adaptive memory selection, where we have the beginning of a chorale: s (60, 55, 52, 36) (60, 55, 52, 36) (62, 59, 55, 43) (62, 59, 55, 43) (62, 59, 55, 43), and want to learn the probabilistic model for the next sonority. Instead of starting from scratch, MUS-ROVER II launches the self-learning loop with the ruleset initialized by the fundamentals (incremental learning), and considers the 2D memoryN, forN = 2, 3, 4, 5. The first extracted rule is featured by order sort mod12 w3,4. The rule is chosen because its corresponding feature has a large confidence level (validated by the large number of matched examples), a small entropy after being smoothed by Bayesian surprise, and reveals a large gap against the Bachs style. Figure 4 shows the relative performance of this rule (in terms of confidence, regularity, and style gap) to other candidate cells in the 2D memory. Among the top 20 rules for this sonority, 12 are 5-gram, 5 are 4-gram, 3 are 2-gram, showing a long and adaptive dependence to preceding context. Visualizing Bachs Mind With the hierarchical representations in MUS-ROVER II, we are now able to visualize Bachs music mind step by step via activating nodes in the DAG of rule features 1Fluctuations on the trajectory are largely incurred by the imperfect solver of the optimization problem. (similar to neuron activations in a brain). The hierarchical structure, as well as the additive activation process, is in stark contrast with the linear sequence of rules extracted from our prior work (Appendix A.5). Figure 5 shows a snapshot of the rule-learning status after ten loops, while the student is writing a sonority in the middle of a piece. The visualization makes it clear how earlier independent rules are now self-organized into sub-families, as well as how rules from a new context overwrite those from an old context, emphasizing that music is highly context-dependent.",
      "exclude": false
    },
    {
      "heading": "7 CONCLUSIONS AND DISCUSSIONS",
      "text": "Learning hierarchical rules as distributed representations of tonal music has played a central role in music pedagogy for centuries. While our previous work achieved the automation of rule extraction, and to certain level, the interpretability of the rules, this paper yields deeper interpretability that extends to a system of rules and the overall learning process. In summary, it highlights the importance of disentangling the rule features, sorting out their interconnections, and making the concept learning process more dynamic, hierarchical, and cognitive. MUS-ROVER is targeted to complement music teaching and learning. For instance, to many music students, learning and applying rules in part-writing is like learning to solve a puzzle (like Sudoku). Rules themselves are quite flexible as opposed to 0-1 derivatives, and may sometimes be contradictory. In addition, due to the limitation of human short-term memory and the difficulty of foreseeing implications, one has to handle a small set of rules at a time in a greedy manner, make some trials, and undo a few steps if no luck. Hence, solving this music puzzle could become a struggle (or maybe interesting): according to personal preferences, one typically begins with a small set of important rules, and via several steps of trial and error, tries ones best to make the part-writing satisfy a majority of rules, with occasional violations on unimportant ones. On the other hand, a machine is often good at solving and learning from puzzles due to its algorithmic nature. For instance, MUSROVERs student can take all rules into consideration: load them all at a time as constraints and figure out the global optimum of the optimization problem in only a few hours. The same level of efficiency might take a human student years to achieve. We envision the future of MUS-ROVER as a partner to humans in both music teaching and research, which includes but is not limited to, personalizing the learning experience of a student, as well as suggesting new methodologies to music theorists in analyzing and developing new genres. It also has practical applications: as by-products from the self-learning loop, the teacher can be made into a genre classifier, while the student can be cast into a style synthesizer. We are also eager to study the rovers partnership beyond the domain of music.",
      "exclude": true
    },
    {
      "heading": "ACKNOWLEDGMENTS",
      "text": "We thank Professor Heinrich Taube, President of Illiac Software, Inc., for providing Harmonias MusicXML corpus of Bachs chorales (https://harmonia.illiacsoftware.com/), as well as his helpful comments and suggestions. This work was supported by the IBM-Illinois Center for Cognitive Computing Systems Research (C3SR), a research collaboration as part of the IBM Cognitive Horizons Network.",
      "exclude": true
    },
    {
      "heading": "A APPENDIX",
      "text": "",
      "exclude": true
    },
    {
      "heading": "A.1 MATH-TO-MUSIC DICTIONARY",
      "text": "",
      "exclude": false
    },
    {
      "heading": "A.2 ATOMIC ARITHMETIC OPERATORS",
      "text": "In MUS-ROVER II, we set B = order, diff, sort, mod12, where diff(x) = (x2 x1, x3 x2, ), x 2 3 4; sort(x) = (x(1), x(2), ), x 2 3 4; mod12(x) = (mod(x1, 12),mod(x2, 12), ), x 2 3 4; and order(x), similar to argsort, maps x 2 3 4 to a string that specifies the ordering of its elements, e.g. order((60, 55, 52, 52)) = 4=3<2<1. The numbers in an order string denote the indices of the input vector x.",
      "exclude": false
    },
    {
      "heading": "A.3 ALGORITHM FOR CONCEPTUAL HIERARCHY",
      "text": "Input: A family of distinct partitions, represented by a sorted list P = [p1, . . . , pn]: pi 6= pj, for all i 6= j, and |p1| . . . |pn|; Output: The conceptual hierarchy as a DAG, represented by the n by n adjacency matrix T: T[i, j] = 1 if there is an edge from node i to node j in the DAG; initialize T[i, j] = 0 for all i, j; for i = n : 1 do for j = (i + 1) : n do if T[i, j] == 0 then if is coarser(pi, pj) then for k in k | pj pk j do T[i, k] = 1; end end end end end T = Transpose(T); Algorithm 1: Algorithm for computing the conceptual hierarchy",
      "exclude": false
    },
    {
      "heading": "A.4 HEURISTICS FOR COMPARING TWO PARTITIONS",
      "text": "Given two partitions P,Q from the partition family, the function is coarser(P,Q) in Algorithm 1 returns True if P Q. A brute-force implementation of this function involves studying all (unordered) pairs of elements in the input domain (Hubert & Arabie, 1985), which incurs computational burdens if the size of the input domain is large. Therefore, we try to get around this brute-force routine whenever certain heuristic can be used to infer the output of is coarser directly. We propose a few of these heuristics as follows. Transitivity Heuristic If P P and P P , then P P . Window Heuristic Let P and P be two partitions induced by features and , respectively. In addition, and are generated from the same descriptor that preserves the orders of the inputs coordinates, e.g. diff, mod12: = d wI , = d wI . We claim that P P , if I I and |(4)| > |(4)|. To see why this is the case, pick any x, y 4 from the same cluster in P, then (x) = (y). Since d preserves the orders of the inputs coordinates, and I extracts coordinates from I , then (x) = (y), i.e. x, y are in the same cluster in P . So, by definition, P P . Since |(4)| > |(4)|, P P . Descriptor Heuristic Let P and P be two partitions induced by features and , respectively. In addition, and are generated from the same window: = d wI ; = d wI . We claim that P P , if d = b d for some function b and |(4)| > |(4)|. To see why this is the case, pick any x, y 4 from the same cluster in P, then (x) = (y). Since d = b d for some b, then = b d wI = b , thus, (x) = (y), i.e. x, y are in the same cluster in P . So, by definition, P P . Since |(4)| > |(4)|, P P . Combined Heuristic Combining the above heuristics, one can show that for P and P where = d wI ; = d wI , we have P P , if the following conditions are satisfied: 1) d, d both preserve the orders of the inputs coordinates,",
      "exclude": false
    },
    {
      "heading": "2) d′ = b ◦ d for some b,",
      "text": "3) I I , 4) |(4)| > |(4)|.",
      "exclude": false
    },
    {
      "heading": "A.5 SAMPLE RULE TRACES FROM MUS-ROVER I",
      "text": "Table 4 is essentially the same as Table 2 in our previous publication (Yu et al., 2016a), with feature notations following the current fashion. is the pace control parameter that we used in our previous system. No hierarchy was present in any of the three rule traces. For instance, the ordering features were learned as independent rules in a trace, even if they are apparently correlated, e.g. the ordering of w1,2,3,4 (S,A,T,B) implies the ordering of w1,4 (S,B).",
      "exclude": false
    }
  ],
  "fewShot": [
    {
      "model": "claude-haiku-4-5",
      "decision": {
        "rejection": false,
        "confidence": 0.72,
        "primary_reason": "Novel hierarchical representation learning for interpretable music concept extraction with principled information-theoretic framework (Bayesian surprise, KL divergence). Strong technical depth in feature hierarchy induction and adaptive memory selection. Clear pedagogical application. However, limited experimental validation (single dataset, no quantitative comparisons with baselines, no user studies despite pedagogical claims)."
      },
      "token": {
        "prompt_tokens": 6751,
        "completion_tokens": 111,
        "total_tokens": 6862
      },
      "time": "2026-02-08T22:20:26.658695+00:00"
    },
    {
      "model": "gpt-5.2",
      "decision": {
        "rejection": true,
        "confidence": 0.78,
        "primary_reason": "Interesting interpretability-motivated system for symbolic music, but the core ML contribution appears incremental/heuristic (hierarchy construction + filtered feature selection + Dirichlet-smoothed KL objective) with limited rigorous evaluation: narrow dataset (Bach chorales only), unclear strength/fairness of baselines, and mostly qualitative/illustrative claims about “deeper interpretability” and pedagogy without strong quantitative validation (e.g., predictive performance, generalization, user studies, or ablations isolating each proposed component)."
      },
      "token": {
        "prompt_tokens": 8627,
        "completion_tokens": 132,
        "total_tokens": 8759
      },
      "time": "2026-02-09T20:23:07.395461+00:00"
    }
  ],
  "test": false
}